[["index.html", "Kvantitatiivne andmeanalüüs Peatükk 1 Teadmiseks", " Kvantitatiivne andmeanalüüs Marko Sõmer 2022-04-21 Peatükk 1 Teadmiseks Siia on koondatud õppematerjalid kursusele Kvantitatiivne andmeanalüüs II. "],["sissejuhatus-r-i.html", "Peatükk 2 Sissejuhatus R-i 2.1 Päris algus 2.2 Andmetega töötamine 2.3 R markdown 2.4 Andmegraafika 2.5 Edasiseks lugemiseks", " Peatükk 2 Sissejuhatus R-i 2.1 Päris algus Mis see R on? Analüüsikeskkond Programmeerimiskeel Vabavara Põhineb kasutajate kirjutatud pakkettidel, ehk siis pidevalt arenev Andmeanalüüsi seisukohalt praktiliselt piiramatute võimalustega Järjest enam kasutatav Publikatsioonitaseme andmegraafika Suhteliselt järsu õppimiskõveraga, aga kui see ületatud, siis läheb lihtsamaks :) Miks R? Kui on vajadus natukenegi tõsisemalt andmeanalüüsi või andmegraafikaga tegeleda, siis tuleb mingi hetk nii ehk naa R ära õppida (või siis näiteks Python). Kui vahest harva on vaja mõni analüüs teha, siis ei ole ju mingit mõtet selle jaoks kallist (ja ebaefektiivset) kommertstarkvara soetada. R see-eest on tasuta. Seega küsimus võiks pigem kõlada, et miks SPSS, Stata, SAS või Mplus? Mida Ri kasutamiseks vaja on? Baasprogramm https://cran.r-project.org/ RStudio kasutajaliides https://www.rstudio.com/ Saab tegelikult hakkama ka ainult baasprogrammiga. Kuid RStudio teeb töö päris palju mugavamaks ja efektiivsemaks ning lisab võimalusi (näiteks R Markdown analüüside kommunikeerimiseks, projektihaldus jne). On ka teisi kasutajaliideseid ja scriptide kirjutamise abivahendeid, kuid RStudio on neist kahtlemata kõige populaarsem, funktsionaalsem ja mugavam. Kuidas Ri ja RStudiot kasutada? Ris ei ole rippmenüüsid, OK nuppe ega avanevaid aknaid. Kõik käsud tuleb käsureale sisse trükkida. Ühest küljest nõuab see kasutajalt mõnevõrra põhjalikumat arusaamist oma tegevusest, teisalt võimaldab tegevusi kombineerida, neist head ülevaadet saada ning tehtud analüüse lihtsalt korrata (reproducible research) Projektide haldamine RStudio võimaldab erinevaid töid hallata projektidena. Igal projektil on projektikaust (nn juurkataloog), kuhu saab salvestada kõik antud projektiga seonduvad scriptid, andmed jne. Uue projekti loomiseks klikkige File -&gt; New Project, misjärel saab valida kataloogi, kuhu projekt (ja seega kõik projektiga seotud matejalid) salvestub. Skriptide loomine Kõik, mida me andmetega teeme, tuleks salvestada skripti (tegelikult on tegemist kõige tavalisema tekstifailiga). Uue skripti saab teha valides RStudios File -&gt; New File -&gt; R Script Skriptide kaudu on meil ülevaade kõigest, mida me andmetega teinud oleme (andmeteisendused, analüüsikäik jne) ning samas saame iga hetk oma analüüsi korrata. Skriptis olevat koodi saame jooksutada kui märgime vajaliku koodi ära ja vajutame Cntr+Enter või kasutame scriptiakna üleval paremas servas asuvat nuppu Run. R kui kalkulaator 2 + 3 ## [1] 5 (4 - 2) / 2 # sulud toimivad nii nagu nad toimima peavad ## [1] 1 10 * 10 # korrutamine ## [1] 100 10 / 5 # jagamine ## [1] 2 3^2 #astendamine ## [1] 9 3**2 # ka nii saab astendada ## [1] 9 Loogilised tehted 2 == 2 # võrdub ## [1] TRUE 1 != 2 # ei võrdu ## [1] TRUE 2 &gt;= 2 # suurem kui või võrdne ## [1] TRUE 1 &lt; 2 # väiksem kui ## [1] TRUE (3&lt;6) | (6&lt;3) # loogiline või ## [1] TRUE (3&lt;6) &amp; (6&lt;3) # loogiline ja ## [1] FALSE Andmeobjektid R töötab andmeobjektidega ehk andmetega, mis on salvestatud mingisse objekti. Andmeobjekt võib sisaldada üksikut numbrilist väärtust (või ka näiteks sõna), aga ka mitut üksiväärtus koondavat vektorit (seega tunnus) või hoopis mitut andmevektorit koondavat andmestikku. Andmeobjektile väärtuse omistamine toimub &lt;- märgiga. x &lt;- 1 Omistasime objektile x väärtuse 1. Nüüd käsitleb R xi kui 1te ja me saame sellega näiteks tehteid teha. x + 1 ## [1] 2 a &lt;- 10 a ## [1] 10 a &lt;- a + 10 # kirjutame algse a üle ja omistame talle uue väärtuse a ## [1] 20 Andmeobjektide nimed ei tohi sisaldada tehtemärke või tühikut ega alata numbriga. Sõnu võib vajadusel eristada näiteks punkti või alakriipsuga: a.1, a_1 . Ei ole ka soovitatav kasutada ka täpitähti (kuigi üldjuhul R neid tunnistab). R eristab suuri ja väikeseid tähti. R ei võrdu riga. Andmeobjektid võivad sisaldada ka sõnu, lauseid või terveid lõike. linn &lt;- &quot;Tallinn&quot; linn ## [1] &quot;Tallinn&quot; kool &lt;- &quot;Tallinna Ülikool&quot; kool ## [1] &quot;Tallinna Ülikool&quot; Jutumärgid annavad Rile teada, et tegemist on tekstiga ja mitte teise andmeobjekti või funktsiooniga. Loogilised andmed (TRUE või FALSE) a &lt;- TRUE a ## [1] TRUE b &lt;- FALSE b ## [1] FALSE Loogilisi väärtused on tulemuseks loogilistele tehetele. c &lt;- 3 &gt; 2 c ## [1] TRUE R käsitleb loogilisi väärtusi sisemiselt 1 ja 0ina, seega saame ka nendega tehteid teha: a ## [1] TRUE b ## [1] FALSE a + b ## [1] 1 Puuduvad väärtused Puuduvate väärtuste jaoks on tähis NA. c &lt;- NA c ## [1] NA a ## [1] TRUE c + a ## [1] NA Miks on tulemuseks NA? Kui me liidame mingi arvu millelegi, mida me ei tea, siis me ju ei tea ka vastust. Ülesanne! tehke andmeobjekt synniaasta, mille väärtuseks on Teie sünniaasta tehke andmeobjekt aasta, mille väärtuseks on 2017 arvutage kui vana te olete? kontrollige looglise tehtega, kas Teie sünniaasta ikka on väiksem kui 2019 tehke andmeobjekt nimi, mille väärtuseks on Teie nimi Funktsioonid Enamik toimingutest toimub Ris funktsioonide abil. sqrt(4) # ruutjuure funktsioon ## [1] 2 Funktsioonile järgnevad alati sulud, milles tuleb määrata funktsiooni argument (antud juhul 4, ehk number millest tahame ruutjuurt võtta). Argumente võib olla ka rohkem kui üks (ja üldjuhul ongi). Sellisel juhul on nad eraldatud komaga. Funktsioonil log() on kaks argumenti: x, ehk arv millest me tahame logaritmi võtta ja base ehk logaritmi alus. log(x = 100, base = 10) ## [1] 2 Kui me teame argumentide järjekorda, siis me ei pea nende tähiseid eksplitsiitselt välja kirjutama. log(100, 10) ## [1] 2 Osadel argumentidel on vaikeväärtused ( näit log() funktsiooni puhul base argumendi vaikeväärtuseks e ehk 2.7) Kui me vaikeväärtusega argumenti välja ei kirjuta, kasutatakse vaikeväärtust. log(x=100) ## [1] 4.60517 See kehtib muidugi ainult sellisel juhul kui teisel argumendil on vaikeväärtus. Kuidas me seda aga teadma peaksime? Kõige lihtsam on vaadata funktsiooni abilehte, kus on kõik selle argumendid loetletud. Kuidas abi saada? Iga funktsiooni kohta on Ris abileht millele pääseb ligi kirjutade funktsiooni nime ette ?. ?log help(log) # saab ka nii Kui on spetsiifilisemad probleemid, saab alati googeldada. Erinevaid materjale, tutoriale, foorumeid jne on väga palju. Vektorid Üldiselt ei tööta me üksikväärtustega (skalaaridega) vaid andmejadade ehk vektoritega. Vektori loomine ehk mitmest väärtusest andmeobjekti loomine käib c() funktsiooniga. see &lt;- c(3,6,1,4,10) see ## [1] 3 6 1 4 10 too &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) # vektor võib sisaldada ka teksti too ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; Sarnaselt üksikväärtustega saab ka vektoritega tehteid teha. see ## [1] 3 6 1 4 10 too &lt;- see * 2 too ## [1] 6 12 2 8 20 see + too ## [1] 9 18 3 12 30 Kui üks vektor on teisest lühem, siis R taaskasutab lühema vektori väärtusi. pikk &lt;- c(1,1,2,2,3,3) lyhike &lt;- c(10, 100) pikk * lyhike ## [1] 10 100 20 200 30 300 See juhtub ka siis, kui lühem vektor ei ole pikema täisarvuline jagatis: pikk &lt;- c(1,1,2,2,3,3, 4) lyhike &lt;- c(10, 100) pikk * lyhike ## [1] 10 100 20 200 30 300 40 Üldiselt me sellist asja teha ei taha. Õnneks antakse taolisest olukorrast meile ka hoiatusteatega märku. Aga miks taoline vektoriseeritus üldse vajalik peaks olema? Asi on selles, et R käsitleb kõiki objekte vektoritena. Ka üksik number on vektor, mille pikkus on 1. Ehk siis just vektoriseeritus võimaldab meil teha nii: c(1,2,3,4) * 2 ## [1] 2 4 6 8 Ja midagi taolist tahame me teha päris tihti. Vektori elementidele saab ka nimesi anda (ja üksikväärtustele muidugi ka) see2 &lt;- c(a=1, b=2, c=3) see2 ## a b c ## 1 2 3 Indekseerimine Kuidas üksikuid väärtusi vektorist kätte saada? Neile saab ligi kasutades [ ] funktsiooni koos soovitava väärtuse indeksiga (positsiooninumbriga). see &lt;- c(3,6,1,4,10) see[2] # tahame teada vektori teist väärtust ## [1] 6 a &lt;- see[2] # tahame selle kirjutada uude andmeobjekti a ## [1] 6 see[3:4] # saame välja võtta mitu väärtust. &quot;:&quot; on &quot;kuni&quot; märk (teine kuni kolmas positsioon) ## [1] 1 4 Andmeobjekti elemnte saab ka välja jätta. see[-2] # kõik elemendid välja arvatud teine ## [1] 3 1 4 10 Indekseerida saab ka nimega. see2 &lt;- c(a=1, b=2, c=3) # teeme nimedega vektori see2[&quot;a&quot;] # jällegi peame kasutama jutumärke, kuna ei viita mitte andmeobjektile, ## a ## 1 # vaid selle väärtusele Kui tahame nimega indekseerida mitut väärtust, peame kasutama indeksite vektorit, mille teeme c() funktsiooniga. Viitame indeksite vektoriga andmevektorile. see2 ## a b c ## 1 2 3 see2[c(&quot;a&quot;,&quot;b&quot;)] ## a b ## 1 2 Viidata saab ka loogiliste tehete või loogiliste vektoritega. Saame loogilise tehtega luua loogilise vektori, mida siis saab kasutada väärtuste väljavõtmiseks. see &lt;- c(3,6,1,4,10) see &gt; 5 # loogiline vektor ## [1] FALSE TRUE FALSE FALSE TRUE see[see &gt; 5] # kasutame loogilist vektorit indekseerimiseks ## [1] 6 10 # see[c(F,T,F,F,T)] # kui me kirjutaks loogilise vektori välja Ülesanne! looge vektor a milles sisalduvad numbrid 2 8 3 6 7 looge vektor b milles sisalduvad numbrid 3 4 5 7 2 looge vektor c, mis on kahe eelmise summa looge andmeobjekt d, mis sisaldab c esimest väärtust looge vektor e, kus on c väärtuseid, mis on suuremad kui 10 Maatriksid Andmevektoreid saab omakorda ühendada. see &lt;- c(12,5) too &lt;- c(6,9) loo &lt;- cbind(see, too) # cbind ühendab vektorid veergude kaupa, # ridade kaupa ühendamiseks on funktsioon rbind() loo ## see too ## [1,] 12 6 ## [2,] 5 9 Tulemuseks on uus andmeobjekt, mis kuulub klassi matrix. Saame ka matrix() funktsiooniga maatrikseid teha: loo &lt;- matrix(c(12,5,6,9), nrow = 2, ncol = 2, byrow = T) loo ## [,1] [,2] ## [1,] 12 5 ## [2,] 6 9 Maatriksi veergudele ja ridadele saame nimesid anda: colnames(loo) &lt;- c(&quot;esimene_veerg&quot;, &quot;teine_veerg&quot;) rownames(loo) &lt;- c(&quot;esimene_rida&quot;, &quot;teine_rida&quot;) loo ## esimene_veerg teine_veerg ## esimene_rida 12 5 ## teine_rida 6 9 Andmeobjektide klassid Erinevatel andmeobjektidel on erinevad klassid. Klassid tulenevad sellest, millist tüüpi andmed selles andmeobjektis on (numbrilised, tekstilised, loogilised jne). class(see) ## [1] &quot;numeric&quot; class(loo) ## [1] &quot;matrix&quot; &quot;array&quot; Enamikes andmeobjektides saab olla vaid ühte tüüpi elemente. Kui numbrilises vektoris on näiteks üks tekstiline väärtus, siis arvestab R seda vektorit kui tekstilist (kuna numrit on võimalik tekstiliseks teha, kuid teksti numbriks mitte). Näiteks maatriksis võivad olla vaid numbrilised väärtused. Üks andmeobjekt on siinkohal erandlik. Selleks on list, kus võib korraga olla erinevat tüüpi andmeid. x &lt;- list(1, c(&quot;b&quot;, &quot;d&quot;)) x ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;b&quot; &quot;d&quot; Ülesanne! teil on andmevektorid a, b, c. Ühendage need veergupidi andmestikuks koos mis on selle andmeobjekti klass? Data frame (andmestik) Kui meil on mingi andmestik, siis üldjuhul on seal erinevat liiki tunnuseid, nii arvtunnuseid kui kategoriaalseid ehk faktortunnuseid jne. Sellise andmebaasi jaoks on Ris eraldi andmeobjekti formaat - data.frame. data.frame on iseenesest list, aga omapärane selles mõttes, et tema read peavad olema ühepikkused ja veerud peavad olema ühepikkused. Põhimõtteliselt on data.frame siis selline andmeobjekt, kus ridadeks on vaatlused ja veergudeks tunnused. nimi &lt;- c(&quot;Jaan&quot;, &quot;Mari&quot;, &quot;Kadri&quot;, &quot;Mati&quot;) vanus &lt;- c(29, 42, 35, 52) hinnang &lt;- c(1.438, 2.763, 1.548, 2) see &lt;- data.frame(nimi, vanus, hinnang) # ühendame tunnused andmestikuks see ## nimi vanus hinnang ## 1 Jaan 29 1.438 ## 2 Mari 42 2.763 ## 3 Kadri 35 1.548 ## 4 Mati 52 2.000 # saab ka nii (siin peame kasutama võrdusmärki) see &lt;- data.frame(nimi = c(&quot;Jaan&quot;, &quot;Mari&quot;, &quot;Kadri&quot;, &quot;Mati&quot;), vanus = c(29, 42, 35, 52), hinnang = c(1.438, 2.763, 1.548, 2)) Kui meil juba on mingi andmetabel, näiteks maatriks, saame selle muuta data.frameiks funktsiooniga as.data.frame() loo &lt;- cbind(c(12,5), c(6,9)) as.data.frame(loo) Sarnaselt vektoritele saame indekseerida ka data.framei (maatrikseid samuti). Kuna aga data.frame on kahedimensionaalne, peame kasutama kahte indeksit. Esiteks rea ja teiseks veeru indeks. Tahame teada Kadri vanust, seega 3 rida ja 2 veerg: see[2,3] ## [1] 2.763 Kui jätame veeru koha tühjaks, valitakse kõik veerud. see[2,] ## nimi vanus hinnang ## 2 Mari 42 2.763 Kui jätame rea koha tühjaks, valitakse kõik read see[,3] ## [1] 1.438 2.763 1.548 2.000 Kui tahame valida mingit tunnust (veergu) siis võime kasutada selle numbrilist või nimelist indeksit see[,&quot;nimi&quot;] # tahame nime veeru kõiki ridu, seega jätame indekseerimisel rea koha tühjaks ## [1] &quot;Jaan&quot; &quot;Mari&quot; &quot;Kadri&quot; &quot;Mati&quot; Võime valida ka mitu veergu või rida korraga. see[1:2,1:2] ## nimi vanus ## 1 Jaan 29 ## 2 Mari 42 Seega saame valida ainult mingi, meile vajaliku osa datasetist Teine viis veeru ehk tunnuse valimiseks on $ märk see$nimi ## [1] &quot;Jaan&quot; &quot;Mari&quot; &quot;Kadri&quot; &quot;Mati&quot; Ülesanne! Teil on andmestik koos. Tehke see data.frameiks Võtke sealt välja esimene rida Võtke sealt välja veerg c ja salvestage see eraldi andmeobjektina Kasutades alternatiivset viisi, võtke välja veerg b Data framei modifitseerimine Data.framei väärtuste muutmisel saame jälle indekseid kasutada: see$hinnang[1] &lt;- 1 # muudame hinnangu tunnuse esimese väärtuse 1-ks see[1, 3] &lt;- 1 # sama mis eelmine see ## nimi vanus hinnang ## 1 Jaan 29 1.000 ## 2 Mari 42 2.763 ## 3 Kadri 35 1.548 ## 4 Mati 52 2.000 Data.framei uute tunnuste lisamine: see$rahulolu &lt;- c(2, 4, 3, 5) see$sugu &lt;- c(&quot;m&quot;, &quot;n&quot;, &quot;n&quot;, &quot;m&quot;) see ## nimi vanus hinnang rahulolu sugu ## 1 Jaan 29 1.000 2 m ## 2 Mari 42 2.763 4 n ## 3 Kadri 35 1.548 3 n ## 4 Mati 52 2.000 5 m Data.framei tunnuste kustutamine: see$rahulolu &lt;- NULL see ## nimi vanus hinnang sugu ## 1 Jaan 29 1.000 m ## 2 Mari 42 2.763 n ## 3 Kadri 35 1.548 n ## 4 Mati 52 2.000 m Saame ridade valimiseks (indekseerimiseks) kasutada loogilisi tehteid ja seega välja võtta just need vaatlused mida vajame. see[see$vanus &lt; 40, ] ## nimi vanus hinnang sugu ## 1 Jaan 29 1.000 m ## 3 Kadri 35 1.548 n Samal ajal saame saame võtta ka ainult vajalikud veerud: see[see$vanus &lt; 40, c(&quot;nimi&quot;, &quot;vanus&quot;)] ## nimi vanus ## 1 Jaan 29 ## 3 Kadri 35 Nii saame teha andmestikust alamandmestiku, is vastab konkreetsetele tingimustele (subseti loomine): uus &lt;- see[see$sugu == &quot;n&quot;, c(&quot;nimi&quot;, &quot;hinnang&quot;)] uus ## nimi hinnang ## 2 Mari 2.763 ## 3 Kadri 1.548 Selleks saab kasutada ka subset() funktsiooni: subset(see, sugu == &quot;n&quot;, select = c(&quot;nimi&quot;, &quot;hinnang&quot;)) ## nimi hinnang ## 2 Mari 2.763 ## 3 Kadri 1.548 Ülesanne! Lisage oma andmeobjektile koos uus tunnus f, milles sisalduvad tähed a i a i a Looge uus data.frame, mis sisaldab tunnuseid a, b ja ainult ridu, mille väärtus tunnuses f on a Faktorid Faktorid on Ri kategoriaalsed tunnused. Mõned meetodid vajavad sisendiks faktoreid. Võime tekstilise tunnuse (või ka numbrilise) muuta faktoriks funktsiooniga as.factor(). x1 &lt;- as.factor(c(&quot;punane&quot;, &quot;roheline&quot;, &quot;sinine&quot;, &quot;sinine&quot;)) x1 ## [1] punane roheline sinine sinine ## Levels: punane roheline sinine Aga faktortasemed on järjestatud tähestiku järgi. Üldjuhul on meil ikkagi mingi oma järjekord. Peaksime selle määrama nii: x1 &lt;- c(&quot;punane&quot;, &quot;roheline&quot;, &quot;sinine&quot;, &quot;sinine&quot;) x1 &lt;- factor(x1, levels = c(&quot;sinine&quot;, &quot;roheline&quot;, &quot;punane&quot;)) x1 ## [1] punane roheline sinine sinine ## Levels: sinine roheline punane Mis aga juhtub kui me ühe taseme kogemata ära unustame: x1 &lt;- c(&quot;punane&quot;, &quot;roheline&quot;, &quot;sinine&quot;, &quot;sinine&quot;) x1 &lt;- factor(x1, levels = c(&quot;sinine&quot;, &quot;roheline&quot;)) x1 ## [1] &lt;NA&gt; roheline sinine sinine ## Levels: sinine roheline Seega faktorid võivad teatud kohtades natukene ohtlikud olla ning nende kasutamisel peab tähelepanelik olema. Ülesanne! looge oma andmestikku koos juurde faktortunnus g, milles sisalduvad tähed r t r t r Andmeobjektide kustutamine R jätab kõik konkreetse sessiooni ajal loodud või imporditud andmeobjektid mällu. Andmeobjektide kustutamine käib funktsiooniga rm(). rm(x) Kui tahame kustutada kõik mälus olevad andmeobjektid, siis  rm(list=ls()) Ri paketid Paljud funktsioonid on kaasas baasRiga. Lisaks neile on aga suur hulk funktsioone, mida on võimalik pakettidena juurde installida. Paketid on kasutajate eneste poolt kirjutatud. Mõned neist on väga spetsiifilised, teised jällegi väga laialdaselt kasutatavad. Hetkel on ligi 13 700 paketti (kaks aastat tagasi oli neid veel 10 000). Et paketti kasutada, tuleb see esmalt installida. install.packages(&quot;ggplot2&quot;) # jutumärgid on vajalikud Kui pakett on installitud, tuleb see Ri mällu laadida (igaks sessiooniks uuesti). library(ggplot2) # jutumärgid ei ole vajalikud Miks peab enne igat sessiooni paketi uuesti laadima? Kuna pakette on väga palju ja neis funktsioone veelgi rohkem, siis hakkavad funktsioonide nimed korduma. Et seda vältida, ongi mõistlik laadida vaid need paketid, mida konkreetse sessiooni ajal otseselt vaja on. Funktsioonide nimed võivad kattuda isegi väheste laaditud pakettide korral. Sellisel juhul kasutab R viimati laetud paketi funktsiooni. 2.2 Andmetega töötamine Andmete sisselugemine Andmete sisselugemiseks on mitmeid erinevaid funktsioone, mille valik sõltub sellest mis formaadis meie andmed on. Kõige mõistlikum viis andmeid hoida on .csv fail (comma separated value). Näiteks Excelis saab andmetabeli csvks salvestada (save as). Samuti Statas, SPSSis jne. Olenevalt sellest mida me numbri komakohana kasutame (. või ,), saab csv faili laadida funktsiooniga read.csv() või read.csv2. andmed &lt;- read.csv(&quot;C:/Users/Mina/Kvant analüüsi meetodid II (2019)/Andmed/andmed.csv&quot;) Kindlasti tuleb andmed kuhugi andmeobjekti (data.framei) sisse lugeda, muidu kuvatakse nad lihtsalt konsooli. faili path peab olema jutumärkides. Kaldkriipsud on teistpidi kui folderi käsujoonel. Aegajalt juhtub, et loete täiesti korralikud andmed sisse, kuid kui neid R-is vaatate, siis on ü-de, ä-de ö-de või õ-de asemel mingid imelikud krõnksud. Sellisel puhul on üldjuhul tegemist encodingu probleemiga, st R ei saa aru kuidas arvutikeelt (see kuidas kõik tekstid ja andmed jne arvutisiseselt salvestatud on) inimkeelde tõlkida. Maailmas kasutatakse selleks erinevaid kodeeringuid, mis lähtuvad erinevatest tähestikest ja sümbolitest. Üks univesaalseid kodeeringuid, mis tuleb toime praktiliselt kõikvõimalike sümolite ja tähestikega on UTF-8. Kui nüüd juhtub, et teie sissetõmmatud andmete puhul esineb taoline encodingu probleem, siis read.csv() funktsioonil on vastav parameeter, millega saate sobiliku encodingu määrata. andmed &lt;- read.csv(&quot;C:/Users/Mina/Kvant analüüsi meetodid II (2019)/Andmed/andmed.csv&quot;, fileEncoding = &quot;UTF-8&quot;) On ka üks mugav pakett csv failide sissetõmbamiseks, kus see encoding on juba automaatselt määratud - readr ja selle funktsioon read_csv() (või read_csv2()). install.packages(&quot;readr&quot;) andmed &lt;- read_csv(&quot;C:/Users/Mina/Kvant analüüsi meetodid II (2019)/Andmed/andmed.csv&quot; Kui andmed on näiteks SPSS faili kujul (ja teil ei ole SPSSi, et neid ümber salvestada) aitab pakett haven ja funktsioon read_spss() (Stata faili puhul read_dta()). Exceli faile saab sisse tõmmata paketi readxl funktsiooniga read_excel(). install.packages(&quot;haven&quot;) install.packages(&quot;readxl&quot;) library(haven) andmed &lt;- read_spss(&quot;andmed.sav&quot;) Viimase funktsiooniga ei kasutanud ma faili pathi. Kui me oleme määranud working directoryks ehk Ri konkreetse sessiooni töökataloogiks selle kataloogi, kus andmed parajasti on, siis ei ole seda vaja teha. working directory saab määrata ka funktsiooniga setwd(). setwd(&quot;C:/Users/Mina/Kvant analüüsi meetodid II (2019)/Andmed/andmed.csv&quot;) Working directorysse salvestuvad ka kõik asjad mida me Ris salvestame (graafikud, andmed jne). Kui Kasutame Ri projekti, siis on wd automaatselt projektikataloog. Andmete salvestamine Andmete salvestamine .csv formaati käib write.csv() funktsiooniga (ja üldiselt me tahame neid sellesse formaati salvestada). write.csv(andmed, file=&quot;C:/Users/Mina/Kvant analüüsi meetodid II/Andmed/andmed.csv&quot;) Teine (ja tegelikult eelistatum) salvestamisviis on readr funktsioon write_csv(). Ril on ka oma salvestusformaat. Kui on aga plaanis andmeid pikemalt salvestada, teise arvutiga kasutada või kellegagi jagada, siis ei ole mõistlik Rdata salvestusvisi kasutada, kuna RData fail on konkreetse Ri konfiguratsiooni spetsiifiline. save(see, file = &quot;see.RData&quot;) #salvestamine rm(see) load(&quot;see.RData&quot;) # sisse laadimine Andestikust ülevaate saamine Ris on mitmeid näidisandmestikke. Võtame neist ühe ja salvestame eraldi andmeobjekti. dat &lt;- iris Vaatame andmestiku struktuuri str(dat) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Mitu rida ja mitu veergu andmestikus on (ehk siis dimensioonid)? dim(dat) ## [1] 150 5 Tunnuste nimed names(dat) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; Andmestiku esimesed read head(dat) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Kuna R eristab suuri ja väikesi tähti, siis oleks lihtsam kui kõik tunnuste nimed olekid väikestes tähtedes. Kasutame funktsiooni tolower(): names(dat) &lt;- tolower(names(dat)) Vaatame, mis klassist on tunnus species: class(dat$species) ## [1] &quot;factor&quot; Mhh. Faktor. Mis faktorlevelid on? levels(dat$species) ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; Ülevaade kõikidest tunnustest (kui on suur andmebaas, siis oleks mõistlik valida ainult mõned tunnused (indeksitega siis)): summary(dat) ## sepal.length sepal.width petal.length petal.width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Kirjeldav statistika Mõned olulisemad funktsioonid mean(dat$sepal.length) #aritmeetiline keskmine ## [1] 5.843333 median(dat$sepal.length) # mediaan ## [1] 5.8 sd(dat$sepal.length) # standardhälve ## [1] 0.8280661 var(dat$sepal.length) #dispersioon ## [1] 0.6856935 max(dat$sepal.length) #maksimaalne väärtus ## [1] 7.9 min(dat$sepal.length) #minimaalne väärtus ## [1] 4.3 length(dat$sepal.length) #vaatluste arv, tunnuse pikkus ## [1] 150 sum(dat$sepal.length) #summa ## [1] 876.5 cor(dat$sepal.length, dat$sepal.width) #korrelatsioon ## [1] -0.1175698 Kui tunnuses on puuduvad väärtused, siis paljud funktsioonid ei tööta x &lt;- dat$sepal.length x[3] &lt;- NA # muudame ühe väärtuse puuduolevaks mean(x) ## [1] NA Et puuduvaid väärtusi mitte arvestada, kasutame na.rm argumenti (mõnedel funktsioonidel on teistsugused missingute argumendid, vaadake helpi) mean(x, na.rm=TRUE) ## [1] 5.851007 Ülesanne! Leidke tunnuse sepal.width keskmine ilma funktsiooni mean() kasutamata Tegime tunnusest sepal.length uue andmeobjekti x (kus on üks puuduv väärtus). Leidke xi korrelatsioon sepal.widthiga (vajadusel kasutage helpi) Tabelid Kategoriaalsetele tunnustele tabelid table(dat$species) ## ## setosa versicolor virginica ## 50 50 50 Risttabeli jaoks on meil ka teist faktortunnust (või tekstilist tunnust) vaja. Teeme ise ühe dat$kat &lt;- cut(dat$sepal.length, breaks = 3, labels = c(&quot;L&quot;, &quot;K&quot;, &quot;P&quot;)) # funktsioon cut() lõikab arvtunnuse kategooriateks, breaks argumendiga saab määrata mitmeks kategooriaks (võib ka cut-pointid ette anda, nt breaks=c(2,3,4)) Risttabel table(dat$species, dat$kat) ## ## L K P ## setosa 47 3 0 ## versicolor 11 36 3 ## virginica 1 32 17 Saame juurde panna rea ja veeru summad (selleks peab tabel enne olemas olema) x &lt;- table(dat$species, dat$kat) addmargins(x) ## ## L K P Sum ## setosa 47 3 0 50 ## versicolor 11 36 3 50 ## virginica 1 32 17 50 ## Sum 59 71 20 150 Margineid võib lisada ka ainult veerule või ainult reale, samuti võivad need midagi muud kui summa olla (vaata funktsiooni helpi) Proportsioonide tabel prop.table(x) ## ## L K P ## setosa 0.313333333 0.020000000 0.000000000 ## versicolor 0.073333333 0.240000000 0.020000000 ## virginica 0.006666667 0.213333333 0.113333333 Default on proportsioon kogusummast. Aga võime ka argumendiga 1 määrata rea proportsiooni või argumendiga 2 veeru proportsiooni. prop.table(x, 1) prop.table(x, 2) 2.2.1 dplyr Dplyr on väga funktsionaalne pakett data.frameis olevate andmete töötlemiseks, kirjeldamiseks ja transformeerimiseks. Praktiliselt kogu andmetöötluse saab dplyri abil ära teha. Dplyr põhineb viiel peamisel funktsioonil: - filter() - vaatluste filtreerimine mingite kriteeriumite alusel - select() - tunnuste valimine - arrange() - andmete järjestamine mingi tunnuse põhjal - mutate() - uue tunnuse tegemine - summarise() - tunnuste summeerimine Lisaks veel funktsioon group_by(), millega saab andmestiku mingi tunnuse alusel gruppideks jaotada ja siis igale grupile näiteks summarise() funktsiooni rakendada. Kõigepealt installime ja laadime dplyri ja ühe näidisandmestiku install.packages(&quot;dplyr&quot;) install.packages(&quot;nycflights13&quot;) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(nycflights13) Salvestame paketist nycflights13 andmestiku flights käepärasema nimega : dat &lt;- flights Vaatame, mis andmestikuga tegu on head(dat) ## # A tibble: 6 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## # ... with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; names(dat) ## [1] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;dep_time&quot; ## [5] &quot;sched_dep_time&quot; &quot;dep_delay&quot; &quot;arr_time&quot; &quot;sched_arr_time&quot; ## [9] &quot;arr_delay&quot; &quot;carrier&quot; &quot;flight&quot; &quot;tailnum&quot; ## [13] &quot;origin&quot; &quot;dest&quot; &quot;air_time&quot; &quot;distance&quot; ## [17] &quot;hour&quot; &quot;minute&quot; &quot;time_hour&quot; dplyr: filter Valime ainult need vaatlused, kus lennufirmaks on AA (tunnus carrier) ja mis toimusid jaanuari- või veebruarikuus (dplyri funktsioonides on andmestik alati esimeseks argumeniks) dat.aa &lt;- filter(dat, carrier==&quot;AA&quot; &amp; (month==1 | month==2)) table(dat.aa$month) ## ## 1 2 ## 2794 2517 table(dat.aa$carrier) ## ## AA ## 5311 dplyr: select Valime dat.aa andmestikust tunnused month, arr_delay ja tailnum dat.aa1 &lt;- select(dat.aa, month, arr_delay, tailnum) head(dat.aa1) ## # A tibble: 6 x 3 ## month arr_delay tailnum ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 33 N619AA ## 2 1 8 N3ALAA ## 3 1 31 N3DUAA ## 4 1 -12 N633AA ## 5 1 5 N3EMAA ## 6 1 -3 N3BAAA Küllaltki kasulikud on select()i lisavõimalused, mis lasevad valida tunnuseid vastavalt sellele, mis tähekombinatsiooniga tunnusenimi algab, lõpeb või mida sisaldab (täpsemalt vaata select() helpi). dat.aa2 &lt;- select(dat.aa, contains(&quot;arr&quot;)) dplyr: arrange Järjestame dat.aa1 andmedtiku arr_delay tunnuse järgi dat.aa1 &lt;- arrange(dat.aa1, arr_delay) head(dat.aa1) ## # A tibble: 6 x 3 ## month arr_delay tailnum ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2 -69 N3EAAA ## 2 2 -65 N320AA ## 3 2 -60 N3FAAA ## 4 1 -54 N335AA ## 5 2 -54 N4UBAA ## 6 2 -54 N350AA Suuremast väiksemaks järjestamieks tuleb kasutada desc() lisavõimalust dat.aa1 &lt;- arrange(dat.aa1, desc(arr_delay)) dplyr: mutate Teeme uue tunnuse, kus hilinemise aeg oleks tundides. dat.aa1 &lt;- mutate(dat.aa1, tunnid=arr_delay/60) head(dat.aa1) ## # A tibble: 6 x 4 ## month arr_delay tailnum tunnid ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 -69 N3EAAA -1.15 ## 2 2 -65 N320AA -1.08 ## 3 2 -60 N3FAAA -1 ## 4 1 -54 N335AA -0.9 ## 5 2 -54 N4UBAA -0.9 ## 6 2 -54 N350AA -0.9 Saab luua ka funktsioonide alusel uusi tunnuseid, näiteks kui tahame mingil põhjusel tunnust, kus oleks kõikide hilinemiste keskmine. mutate(dat.aa1, keskmine=mean(arr_delay, na.rm=T)) dplyr: summarise Tahame summeerida hilinemised keskmise, standardhälbe, maksimumi ja miinimumi alusel. summarise(dat.aa1, keskmine=mean(arr_delay, na.rm=T), sdh=sd(arr_delay, na.rm=T), maks=max(arr_delay, na.rm=T), min=min(arr_delay, na.rm=T)) ## # A tibble: 1 x 4 ## keskmine sdh maks min ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.03 34.1 368 -69 dplyr: group_by Tahame teada keskmist hilinemist kuude lõikes. summarise(group_by(dat.aa1, month), keskmine=mean(tunnid, na.rm=T)) ## # A tibble: 2 x 2 ## month keskmine ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.0164 ## 2 2 0.0182 dplyr: piping dplyr toetab nn pipingut. Kombinatsiooniga %&gt;% saab saab ühe funktsiooni tulemuse võtta sisendiks järgmisele funktsioonile. Seega saame kõik vajalikud toimingud ühes reas ära teha. Kõikide dplyri funktsioonide puhul on esimene argument data. Kui kasutame %&gt;%, ei pea me enam datat defineerima ja dplyr saab ise aru, et andmeteks on eelmise funktsiooni tulemused. Nii saame kõik eelnevad käsud panna ühte jadasse: dat %&gt;% #algsed andmed filter(carrier==&quot;AA&quot;, month==1 | month==2) %&gt;% select(month, arr_delay, tailnum) %&gt;% arrange(arr_delay) %&gt;% mutate(tunnid=arr_delay/60) %&gt;% group_by(month) %&gt;% summarise(keskmine=mean(tunnid, na.rm=T)) ## # A tibble: 2 x 2 ## month keskmine ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.0164 ## 2 2 0.0182 Kui me tahame tulemusi kuhugi salvestada, peame uue andmeobjekti alguses määrama. dat1 &lt;- dat %&gt;% select(month, arr_delay, tailnum) Tänu pipingule saab küllaltki keerulisi andmeteisendusi teha väga lihtsalt ja elegantselt (puhta ja arusaadava koodiga). Näiteks tunnus, milles on kõikide lennufirmade keskmine hilinemine kõikide kuude lõikes: dat1 &lt;- dat%&gt;% group_by(carrier, month) %&gt;% # saame grupeerida ka mitme tunnuse lõikes mutate(keskmine = mean(arr_delay, na.rm=T)) Veel mõned kasulikud funktsioonid Kui on vaja välja jätta dubleerivad vaatlused: flights %&gt;% distinct(carrier, flight) ## # A tibble: 5,725 x 2 ## carrier flight ## &lt;chr&gt; &lt;int&gt; ## 1 UA 1545 ## 2 UA 1714 ## 3 AA 1141 ## 4 B6 725 ## 5 DL 461 ## 6 UA 1696 ## 7 B6 507 ## 8 EV 5708 ## 9 B6 79 ## 10 AA 301 ## # ... with 5,715 more rows Kui on vaja vaatluste arvu: flights %&gt;% summarise(n()) ## # A tibble: 1 x 1 ## `n()` ## &lt;int&gt; ## 1 336776 # Või vaatluste arv gruppide lõikes flights %&gt;% group_by(carrier) %&gt;% summarise(kokku = n()) ## # A tibble: 16 x 2 ## carrier kokku ## &lt;chr&gt; &lt;int&gt; ## 1 9E 18460 ## 2 AA 32729 ## 3 AS 714 ## 4 B6 54635 ## 5 DL 48110 ## 6 EV 54173 ## 7 F9 685 ## 8 FL 3260 ## 9 HA 342 ## 10 MQ 26397 ## 11 OO 32 ## 12 UA 58665 ## 13 US 20536 ## 14 VX 5162 ## 15 WN 12275 ## 16 YV 601 Kui tahame välja võtta juhuvalimi: # Võtame välja 10 juhuslikku rida flights %&gt;% sample_n(10) ## # A tibble: 10 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 2 13 1150 1145 5 1457 1454 ## 2 2013 1 29 857 900 -3 1122 1135 ## 3 2013 4 13 727 729 -2 1014 1020 ## 4 2013 5 9 1932 1925 7 2048 2105 ## 5 2013 10 15 1922 1930 -8 2042 2056 ## 6 2013 2 15 1526 1518 8 1648 1642 ## 7 2013 7 23 926 900 26 1038 1022 ## 8 2013 10 8 1707 1707 0 1907 1913 ## 9 2013 6 27 1754 1615 99 2025 1809 ## 10 2013 10 3 1752 1755 -3 1934 1930 ## # ... with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Kui tahame välja võtta konkreetsed read: # Võtame välja esimesed 5 rida flights %&gt;% slice(1:5) ## # A tibble: 5 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## # ... with 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 2.2.2 Andmete ühendamine Andmestike ühendamisel võib olla kaks eesmärki: tahame lisada ridu või tahame lisada tunnuseid (veergusid). Ridade lisamiseks on dplyris funktsioon bind_row(): #Teeme kaks andmestikku dt1 &lt;- data.frame(a = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;), b = 1:5) dt1 ## a b ## 1 a 1 ## 2 b 2 ## 3 c 3 ## 4 d 4 ## 5 e 5 dt2 &lt;- data.frame(a = c(&quot;a&quot;, &quot;b&quot;, &quot;e&quot;, &quot;f&quot;), c = 6:9) dt2 ## a c ## 1 a 6 ## 2 b 7 ## 3 e 8 ## 4 f 9 # Ühendame andmestikud ridadena bind_rows(dt1, dt2) ## a b c ## 1 a 1 NA ## 2 b 2 NA ## 3 c 3 NA ## 4 d 4 NA ## 5 e 5 NA ## 6 a NA 6 ## 7 b NA 7 ## 8 e NA 8 ## 9 f NA 9 Andmestike tunnuste kaupa ühendamiseks on meil vaja ID-tunnust või tunnuseid, mis identifitseeriks unikaalsed vaatlused. Antud juhul on meil selleks tunnus a. # Ühendame teise andmestiku esimese külge # (ehk siis alles jäävad kõik esimese andmestiku vaatlused) left_join(dt1, dt2, by = &quot;a&quot;) ## a b c ## 1 a 1 6 ## 2 b 2 7 ## 3 c 3 NA ## 4 d 4 NA ## 5 e 5 8 # Ühendame esimese andmestiku teise külge # (ehk siis alles jäävad kõik teise andmestiku vaatlused) right_join(dt1, dt2, by = &quot;a&quot;) ## a b c ## 1 a 1 6 ## 2 b 2 7 ## 3 e 5 8 ## 4 f NA 9 # Ühendame andmestikud nii, et alles jäävad kõik vaatlused mõlemast andmestikust full_join(dt1, dt2) ## Joining, by = &quot;a&quot; ## a b c ## 1 a 1 6 ## 2 b 2 7 ## 3 c 3 NA ## 4 d 4 NA ## 5 e 5 8 ## 6 f NA 9 # Ühendame andmestikud nii, et alles jäävad need vaatlused, mis mõlemas andmestikus olemas on inner_join(dt1, dt2) ## Joining, by = &quot;a&quot; ## a b c ## 1 a 1 6 ## 2 b 2 7 ## 3 e 5 8 # Ühendame andmestikud nii, et alles jäävad need vaatlused, mida ei ole kummaski andmestikus anti_join(dt1, dt2) ## Joining, by = &quot;a&quot; ## a b ## 1 c 3 ## 2 d 4 2.2.3 Andmestiku kuju muutmine Andmestik võib olla nn pikal kujul või laial kujul. Pikad andmed on sellised, mille puhul kõik muutujad on kirjeldatud tunnustena. Laial kujul andmed on sellised, mille puhul mõni muutuja on jaotatad erinevateks tunnusteks. tidyr pakett võimaldab mugavalt andmestiku ühelt kujult teise tranformeerimist: library(tidyr) # Teeme &quot;laia&quot; näidisandmestiku lai &lt;- data.frame(nimi = c(&quot;Jüri&quot;, &quot;Mari&quot;, &quot;Jaan&quot;), test_1 = c(3,5,2), test_2 = c(8,4,5), test_3 = c(2,5,4)) lai ## nimi test_1 test_2 test_3 ## 1 Jüri 3 8 2 ## 2 Mari 5 4 5 ## 3 Jaan 2 5 4 # Antud andmestikus on erinevad testid eri tunnustena. # Aga kui me tahaksime, et test oleks tunnus. # Keerame andmestiku pikale kujule pikk &lt;- lai %&gt;% pivot_longer(cols = starts_with(&quot;test_&quot;), names_to = &quot;test&quot;, values_to = &quot;tulemus&quot;, names_prefix = &quot;test_&quot;) pikk ## # A tibble: 9 x 3 ## nimi test tulemus ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Jüri 1 3 ## 2 Jüri 2 8 ## 3 Jüri 3 2 ## 4 Mari 1 5 ## 5 Mari 2 4 ## 6 Mari 3 5 ## 7 Jaan 1 2 ## 8 Jaan 2 5 ## 9 Jaan 3 4 # Keerame tagasi laiale kujule lai &lt;- pikk %&gt;% pivot_wider(names_from = &quot;test&quot;, values_from = &quot;tulemus&quot;, names_prefix = &quot;test_&quot;) lai ## # A tibble: 3 x 4 ## nimi test_1 test_2 test_3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jüri 3 8 2 ## 2 Mari 5 4 5 ## 3 Jaan 2 5 4 2.2.4 Kuupäevad Ris käsitletakse kuupevi ja kellaaegu eraldi Date klassina. See tagab, et kupäevad ja kellaajad on alati ühtses formaadis ning võimaldab nendega tehteid teha. Aja tunnustega tegelemiseks on mugav kasutada paketti lubridate. Numbriliste või tekstiliste tunnuste kuupäevadeks muutmine: library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union # Kui kuupäeva järjekord on kuupäev (d), kuu (m), aasta (y), siis: dmy(&#39;24.03.2017&#39;) ## [1] &quot;2017-03-24&quot; # või dmy(24032017) ## [1] &quot;2017-03-24&quot; # või dmy(&#39;24-03-2017&#39;) ## [1] &quot;2017-03-24&quot; # või dmy(&#39;24/03/2017&#39;) ## [1] &quot;2017-03-24&quot; # Kui järjekord on teine, siis tuleb lihtsalt tähed funktsiooninimes vastavalt vahetada mdy(&#39;03-24-2017&#39;) ## [1] &quot;2017-03-24&quot; ymd(&#39;2017/03/24&#39;) ## [1] &quot;2017-03-24&quot; # jne Kui tahame kuupevast aastat, kuud, päeva vms: kp &lt;- dmy(24032017) year(kp) ## [1] 2017 month(kp) ## [1] 3 week(kp) ## [1] 12 day(kp) ## [1] 24 wday(kp) ## [1] 6 # või wday(kp, label = T) ## [1] Fri ## Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat Praegune aeg: today() ## [1] &quot;2022-04-21&quot; now() ## [1] &quot;2022-04-21 00:46:36 EEST&quot; Kestus: # Mitu sekundit kestab päev duration(day = 1) ## [1] &quot;86400s (~1 days)&quot; # mitu sekundit kestab nädal duration(week = 1) ## [1] &quot;604800s (~1 weeks)&quot; # aasta duration(1, &quot;year&quot;) ## [1] &quot;31557600s (~1 years)&quot; Interval: kp1 &lt;- dmy(24032017) kp2 &lt;- dmy(26062017) interval(kp1, kp2) ## [1] 2017-03-24 UTC--2017-06-26 UTC # või kp1 %--% kp2 ## [1] 2017-03-24 UTC--2017-06-26 UTC # mitu päeva interval kestab kp1 %--% kp2 %/% days(1) ## [1] 94 # Kas mingi kuupäev jääb intervalli sisse dmy(23032017) %within% interval(kp1, kp2) ## [1] FALSE dmy(25032017) %within% interval(kp1, kp2) ## [1] TRUE 2.2.5 Tekstilised tunnused Tekstiliste tunnuste jaoks on pakett stringr library(stringr) # Teeme vektori tekstidega (stringidega) tekst &lt;- c(&quot;Tekstiliste&quot;, &quot;tunnuste&quot;, &quot;jaoks&quot;, &quot;on&quot;, &quot;pakett&quot;, &quot;stringr&quot;) # Mitu tähemärki on igas sõnas str_length(tekst) ## [1] 11 8 5 2 6 7 # Paneme erinevad sõnad kokku str_c(tekst, collapse = &quot; &quot;) ## [1] &quot;Tekstiliste tunnuste jaoks on pakett stringr&quot; # võtame välja iga sõna esimese ja teise tähemärgi str_sub(tekst, start = 1, end = 2) ## [1] &quot;Te&quot; &quot;tu&quot; &quot;ja&quot; &quot;on&quot; &quot;pa&quot; &quot;st&quot; # Võtame välja sõnad, mis sisaldavad &quot;t&quot; tähte str_subset(tekst, &quot;t&quot;) ## [1] &quot;Tekstiliste&quot; &quot;tunnuste&quot; &quot;pakett&quot; &quot;stringr&quot; # Võtame välja sõnad, mis sisaldavad &quot;a&quot; või &quot;o&quot; tähte str_subset(tekst, &quot;[ao]&quot;) ## [1] &quot;jaoks&quot; &quot;on&quot; &quot;pakett&quot; # Kas sõnas on &quot;a&quot; või &quot;o&quot; täht str_detect(tekst, &quot;[ao]&quot;) ## [1] FALSE FALSE TRUE TRUE TRUE FALSE # Võtame sõnadest välja &quot;te&quot; tähekombinatsioonid str_extract(tekst, &quot;te&quot;) ## [1] &quot;te&quot; &quot;te&quot; NA NA NA NA # Mitu &quot;t&quot; tähte igas sõnas on str_count(tekst, &quot;t&quot;) ## [1] 2 2 0 0 2 1 # Asendame kõik &quot;t&quot; tähed &quot;T&quot; tähega str_replace(tekst, &quot;t&quot;, &quot;T&quot;) ## [1] &quot;TeksTiliste&quot; &quot;Tunnuste&quot; &quot;jaoks&quot; &quot;on&quot; &quot;pakeTt&quot; ## [6] &quot;sTringr&quot; # Nagu näha, siis asendati ainult sõna esimene &quot;t&quot; täht # Kui tahame kõik &quot;t&quot; tähed asendada, siis: str_replace_all(tekst, &quot;t&quot;, &quot;T&quot;) ## [1] &quot;TeksTilisTe&quot; &quot;TunnusTe&quot; &quot;jaoks&quot; &quot;on&quot; &quot;pakeTT&quot; ## [6] &quot;sTringr&quot; 2.3 R markdown R markdown teeb tulemuste esitamise (koos koodiga) või raporti tegemise väga lihtsaks. Kõigepealt on vaja installida pakett rmarkdown (RStudioga tuleb see defaultis kaasa). Seejärel saame scriptifaili asemel valida markdowni dokumendi: File &gt; New File &gt; R Markdown. Saab valida formaadi, mida väljundina tahame saada (html, pdf, word). Avaneb markdowni dokument, millesse saab kirjutada nii tavalist teksti kui ka Ri koodi, ning mille väljundis sisalduvad (kui me seda muidugi tahame) ka analüüsitulemused. Väljundi loomiseks tuleb vajutada Knit nuppu. Lisainfo jaoks võite vaadata: Help &gt; Markdown Quick Reference http://rmarkdown.rstudio.com/ 2.4 Andmegraafika Edward Tufte, üks andmegraafika legende, kirjeldab oma raamatutes Beautiful Evidence ja The Visual Display of Quantitative Information peamisi andmegraafika põhialuseid: Graafikul esitatud tunnuste representatsioonid peavad olema proportsionaalsed mõõdetud tunnustega reaalses maailmas Graafikul esitatule peab olema selge, detailne ja läbiv tähistus ning selgitus Esita andmete varieerumist, mitte graafiku disainist tulenevat varieerumist Informatsiooni edastavaid dimensioone ei tohiks esitada rohkem kui andmed seda võimaldavad (3D tulpdiagrammid on saatanast) Graafik peab edastama ainult andmetest lähtuvat informatsiooni (mida saaks graafikul kustudada, ilma et selle infoedastusvõime kannataks?) Võrdlusmoment Mingi kvantiteet (keskmine, sagedus, hajuvusnäitaja jne) omab mõtet vaid suhestudes mingi teise kvantiteedi või referentsiga Sisukas hüpotees võrdluses nullhüpoteesiga Mitmemõõtmelisus Maailma on alati mitmemõõtmeline Näita võimalikult palju andmeid (aga mitte rohkem kui võimalik) Esitatud andmed peavad olema olulised (mõttekad) Millekes üldse graafikud? Andmete mõistmine Mustrite leidmine Vigade leidmine Tulemuste kommunikeerimine 2.4.1 Ri baasgraafikud Baas-Ris on väga võimekas graafikamootor, millega on võimalik väga ilusaid ja sisukaid graafikuid teha. Tänapäeval kasutab aga enamik andmeanalüütikuid baas-Ri asemel paketti ggplot2, kus on jooniste tegemine muudetud mõnevõrra lihtsamaks, loogilisemaks ja võimalusterohkemaks. Kuid, et oleksite vähemalt tuttav ka baas-Ri võimalustega, vaatame kiirelt üle ka selles leiduvad võimalused. dt &lt;- iris #Kasutame Iris&#39;e näidisandmestikku names(dt) &lt;- tolower(names(dt)) Scatterplot plot(x = dt$sepal.length, y = dt$sepal.width) Saab kasutada ka ainult ühte argumenti. plot(x = dt$sepal.length) Argumendiga type= saab määrata graafiku tüübi. Näiteks l joongraafik, b jooned ja sümolid koos jne (vaata ?plot). plot(dt$sepal.length, type= &quot;b&quot;) Histogram hist(x = dt$sepal.length) hist(x = dt$sepal.length, breaks = 20) Boxplot Ühele grupile boxplot(dt$sepal.length) Mitme grupi lõikes peab kasutama formula märki (~) boxplot(dt$sepal.length ~ dt$species) Barplot Barploti jaoks on sisendiks vaja tabelit library(dplyr) d_bar &lt;- dt%&gt;% filter(sepal.length&gt;5.5)%&gt;% select(species)%&gt;% table() d_bar ## . ## setosa versicolor virginica ## 3 39 49 barplot(d_bar) Baasgraafikute parameetrid pch: graafikul esitatv sümbol (vaikimisi ring) lty: joone tüüp (vaikimisi tavaline) ldw: joone laius (numbriline vaikimisi 1) col: värv (colors() funktsiooniga näeb võimalikke värve) xlab ja ylab: telgede nimed (tekstiline väärtus) xlim ja ylim: telgede limiidid (kui on vaja neid suurenda või vähendada) plot(x = dt$sepal.length, y = dt$sepal.width, pch=2, col=&quot;red&quot;, xlab=&quot;Sepal length&quot;, ylab=&quot;Sepal width&quot;) par() funktsiooniga saab seada globaalseid parameetrieid. Näiteks saab panna mitu graafikut üksteise kõrvale: par(mfrow=c(1,2)) Mis värviparameeter on vaikimisi globaalselt määratletud? par(&quot;col&quot;) ## [1] &quot;black&quot; Baasgraafikute ehitamine Graafikutele saab lisada erinevaid komponente või ka teisi graafikuid. nii on võimalik vajalik graafik kokku ehitada. Mõned võimalused: lines() joonte lisamine points() punktide lisamine text() teksti lisamine title() palkirja lisamine legend() legendi lisamine plot(x = dt$sepal.length, y = dt$sepal.width, xlab=&quot;Sepal length&quot;, ylab=&quot;Sepal width&quot;) dt2 &lt;- dt %&gt;% filter(species==&quot;setosa&quot;) points(dt2$sepal.length, dt2$sepal.width, col=&quot;red&quot;) legend(&quot;topright&quot;, pch=1, col=c(&quot;black&quot;, &quot;red&quot;),legend = c(&quot;muu&quot;, &quot;seotsa&quot;)) title(main = &quot;Pealkiri&quot;) 2.4.2 ggplot ggploti lähtekohaks on Leland Wilkinsoni graafika grammatika, mis lähtub põhimõttest, et graafiku võib lahutada eraldiseisvateks komponentideks ja neist komponentidest saab saab uusi tervikuid ehitada. &gt;  the grammar tells us that a statistical graphic is a mapping from data to &gt; aesthetic attributes (colour, shape, size) of geometric objects (points, &gt; lines, bars). The plot may also contain statistical transformations of the data &gt; and is drawn on a specific coordinate system. Facetting can be used to generate &gt; the same plot for diferent subsets of the dataset. It is the combination of these &gt; independent components that make up a graphic. &gt; (Hadley Wickham, ggplot2: Elegant Graphics for Data Analysis) ggploti elemendid data : andmed. Üldiselt peaks olema dataframe kujul geom : geomeetriline objekt, mille läbi me oma anmdeid esitame (punktid, jooned, tulbad jne) aes : aesthetic ehk siis kuidas ja mille läbi me oma andmeid geomeetriliste objektidega suhestame (mis on x ja y telg, värv, kuju, suurus). Need on joonise objektide visuaalsed omadused facet : tahud ehk kuidas joonist alamosadeks (tahkudeks) jagada stat : milliseid statistilisi transformatsioone on vaja kasutada scales : kuidas andmete reaalsed väärtused joonise väärtusteks tõlgendatakse coord : mis koordinaatsüsteemi kasutada. Üldiselt cartesian positsion : andmeobjektide positsioonide nihutamine guides : teljed, legendid jne theme : joonise üldine kujundus (kus asub legend, mis värvi on tagapõhi jne) Installime ggploti (kui me seda jua teinud ei ole) ja laadime selleks sessiooniks. Üritame teha regressioonijoontega scatterploti. #install.packages(&quot;ggplot2&quot;) library(ggplot2) dt &lt;- iris names(dt) &lt;- tolower(names(dt)) Scatterplot Kõigepealt ggploti peafunktsioon, kus märgime andmestiku (tegelikult võime seda teha ka geomi sees). Seejärel lisame geomi kihi. Liidame selle peafunktsioonile otsa (kasutatdes + märki). Tahame punktdiagrammi, seega geom_point (et saada aimu erinevatest võimalikest geomidest, võib uurida ggploti kodulehte https://ggplot2.tidyverse.org/reference/ või cheatsheeti (Help &gt; Cheatsheets &gt; Data visualization with ggplot2)). Defineerime aestheticu ehk siis mapime tunnused x ja y teljele. ggplot(data=dt)+ geom_point(mapping = aes(x=sepal.width, y=sepal.length)) Tegelikult ei pea argumente välja kirjutama, vaid järjekord on tähtis. Saab ka nii: ggplot(dt)+ geom_point(aes(sepal.width, sepal.length)) Tahame erinevad iirise liigid erinevate värvidega grupeerida. Kuna me tahame määrata seda, kuidas andmeid esitatakse (tunnuseid graafikule mapitakse), peame seda tegema aesi argumendi sees. ggplot(data=dt)+ geom_point(mapping = aes(x=sepal.width, y=sepal.length, color=species)) Saaksime neid eristada ka näiteks kuju shape= või suuruse size= või ka läbipaistvuse alpha= järgi. ggplot(data=dt)+ geom_point(mapping = aes(x=sepal.width, y=sepal.length, shape=species)) Kui me tahame lihtsalt punktide värvi muuta (ja mitte lähtuda mingist grupeerivast tunnusest), saame seda teha väljaspool aes() argumenti. ggplot(data=dt)+ geom_point(mapping = aes(x=sepal.width, y=sepal.length), color=&quot;green&quot;) Oleks vaja joonisele ka regressioonijooned saada. Selleks lisame lihtsalt järgmise kihi (kasutades + märki). Regressioonijoone joonistamiseks võime kasutada geom_abline()i, aga sellisel juhul peame regressioonikoefitsiendid enne välja arvutama (geam_abline() vajab sisendiks intercepti ning slopei). Lihtsam on kasutada geom_smooth()i. ggplot(data=dt)+ geom_point(mapping = aes(x=sepal.width, y=sepal.length, color=species))+ geom_smooth(aes(x=sepal.width, y=sepal.length)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Mhh, joon ei meenuta regressioonijoont. Asi on selles, et tegemist on küll regressioonijoonega, kuid mitte harjumuspärase lineaarse regressioonijoonega. geom_smooth kasutab vaikimisi nn Local Polynomial Regression Fittingut, mis üritab predictida y väärtuse sõltuvust xi väärtusest võimalikult täpselt ja lähtudes xi lähiümbrusest. Aga saame tellida ka tavalise lineaarse regressioonijoone, kasutades argumenti method=\"lm\". ggplot(data=dt)+ geom_point(mapping = aes(x=sepal.width, y=sepal.length, color=species))+ geom_smooth(aes(x=sepal.width, y=sepal.length), method=&quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Kõikide iirise liikide kohta eraldi joonte saamiseks tuleb jällegi määrata grupeerimine geom_smooth()i aes()i sees (kuna see on eraldi kiht ja eelmise kihi määrangud siin enam ei kehti). Kui me usaldusintervalle mingil põhjusel ei taha, võime need tühistada argumendiga se=F. ggplot(data=dt)+ geom_point(mapping = aes(x=sepal.width, y=sepal.length, color=species))+ geom_smooth(aes(x=sepal.width, y=sepal.length, color=species), method=&quot;lm&quot;, se=F) ## `geom_smooth()` using formula &#39;y ~ x&#39; Pidime aes() argumendi määrangud kaks korda järjest välja kirjutama, kuigi nad on identsed. Ei tundu väga mõistlik. Õnneks saab ka lihtsamalt. Võime need määrata ka ggplot() funktsiooni enda sees. Sellisel juhul kehtivad nad ka kõikide järgenvate kihtide kohta (välja arvatud juhul kui järgenvates kihtides on teisiti määratud). ggplot(dt, aes(sepal.width, sepal.length, color=species))+ geom_point()+ geom_smooth(method=&quot;lm&quot;, se=F) Facetid Gruppe saab eristada ka neid erinevatele tahkudele pannes, kasutades selleks facet_wrap()i või facet_gridi. facet_wrap() eristab ühe tunnuse lõikes, facet_grid() mitme tunnuse lõikes. Mõlema puhul tuleb kasutada formula määrangut, st. tuleb kasutada ~ märki (tegelikult ggploti viimase versiooni puhul saame kasutada ka argumente rows= ja cols=). ggplot(dt, aes(sepal.width, sepal.length))+ geom_point()+ geom_smooth(method=&quot;lm&quot;, se=F)+ facet_wrap(~species) ## `geom_smooth()` using formula &#39;y ~ x&#39; Kui tahame tahkusid näiteks ainult kahes tulbas, saame kasutada argumenti nrow= või ncol=. ggplot(dt, aes(sepal.width, sepal.length))+ geom_point()+ geom_smooth(method=&quot;lm&quot;, se=F)+ facet_wrap(~species, ncol=2) ## `geom_smooth()` using formula &#39;y ~ x&#39; facet_grid()i ja kahe tunnuse lõikes tahkude illustreerimiseks meil irise andmestikus piisavalt kategoriaalseid tunnuseid ei ole. Aga ggplotiga tuleb kaasa diamonds andmebaas. Vaatame seda: dt1 &lt;- diamonds str(dt1) ## tibble [53,940 x 10] (S3: tbl_df/tbl/data.frame) ## $ carat : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... ## $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... ## $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... ## $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... ## $ depth : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... ## $ table : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ... ## $ price : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ... ## $ x : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... ## $ y : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... ## $ z : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... Kuidas on seotud teemandite karaadid (carat) ja nende hind (price)? ggplot(dt1)+ geom_point(mapping = aes(x=carat, y=price)) Kuidas siia suhestub teemandite selgus (clarity)? ggplot(dt1)+ geom_point(mapping = aes(x=carat, y=price, color=clarity)) Aga nende lõige cut? kasutame selleks facet_grid()i. Kui tahame facet_grid()iga ainult ühe tunnuse lõikes tahke tekitada, tuleb teise tunnuse asemel kasuatada punkti. Seda, kas tahud on tulbas või reas, saab määrata sellega, kuhupoole ~ märki punkt panna. ggplot(dt1)+ geom_point(mapping = aes(x=carat, y=price, color=clarity))+ facet_grid(cut ~ .) Lisame veel phe tunnuse, mille lõikes teemantide erisusi vaadata, värvi (color). ggplot(dt1)+ geom_point(mapping = aes(x=carat, y=price, color=clarity))+ facet_grid(cut ~ color) Geomid ja aestetikud Erinevaid geome on päris palju. Kõik nad on üles loetletud ggploti kodulehel (koos suure hulga muu infoga): https://ggplot2.tidyverse.org/reference/. Aga mõned olulisemad: geom_bar() geom_histogram() ja geom_freqpoly() geom_boxplot() ja geom_violin() geom_line() ja geom_path() geom_density() geom_abline(), geom_hline() ja geom_vline() geom_text() Barplot Tavaline barplot ggplot(dt1)+ geom_bar(aes(clarity)) Kahe tunnuse lõikes tulpdiagrammi jaoks peame kasutama aes() sees argumenti fill=. ggplot(dt1)+ geom_bar(aes(clarity, fill=cut)) Mhh, see vist ei ole päris see mida me silmas pidasime. Pigem tahaksime, et gruppide tulbad asuksid kõrvuti. Selleks peame määratlema geomi positsiooni: ggplot(dt1)+ geom_bar(aes(clarity, fill=cut), position=&quot;dodge&quot;) Või siis kui tahame 100% barplot ggplot(dt1)+ geom_bar(aes(clarity, fill=cut), position = &quot;fill&quot;) Boxplot ja violin plot ggplot(dt1)+ geom_boxplot(aes(x=color, y=carat)) ggplot(dt1)+ geom_violin(aes(x=color, y=carat)) Histogram ja frequency polygon ggplot(dt1)+ geom_histogram(aes(x=carat), bins = 50, color=&quot;white&quot;) ggplot(dt1)+ geom_freqpoly(aes(x=carat)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Keskmised ja usalduspiirid Väga tihti on meil vaja esitada keskmiste või proportsioonide punkthinnanguid mingite gruppide lõikes koos usaldusintervallidega. Selleks on meil kõigepealt vaja keskmisi ja usaldusintervalle. ggplot neid ise ei arvuta. Aga dplyri abil saab need võrdlemisi lihtsalt kätte. t-jaotuse kvartiilid on leitavad qt(p, df) funktsiooniga (p on siis tõenäosus). Leiame teemantite keskmised hinnad koos usalduspiiridega teemadite lõiketi (cut): library(dplyr) keskmised &lt;- dt1 %&gt;% group_by(cut)%&gt;% summarise(keskmine=mean(price), se=sd(price)/sqrt(length(price)), l.ci=keskmine-qt(0.975, length(price)-1)*se, u.ci=keskmine+qt(0.975, length(price)-1)*se) keskmised ## # A tibble: 5 x 5 ## cut keskmine se l.ci u.ci ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fair 4359. 88.7 4185. 4533. ## 2 Good 3929. 52.6 3826. 4032. ## 3 Very Good 3982. 35.8 3912. 4052. ## 4 Premium 4584. 37.0 4512. 4657. ## 5 Ideal 3458. 25.9 3407. 3508. Kasutame geom_point()i keskmiste visualiseerimiseks ja geom_linerange()i usalduspiiride märkimiseks ggplot(keskmised, aes(cut, keskmine))+ geom_point()+ geom_linerange(aes(ymin=l.ci, ymax=u.ci))+ coord_flip() # saame joonise teljed ära vahetada Joonise disain ggplot võimaldab kontrollida praktiliselt kogu joonise väljanägemist. Vaatame mõnda olulisemat võimalust: telgede nimed ja joonise pealkiri ggplot(keskmised, aes(cut, keskmine))+ geom_point()+ geom_linerange(aes(ymin=l.ci, ymax=u.ci))+ coord_flip()+ ylab(&quot;Hind&quot;)+ xlab(&quot;Lõige&quot;)+ ggtitle(&quot;Teemantite hind&quot;) Kui me tahame telgede nimedest lahti saada: ggplot(keskmised, aes(cut, keskmine))+ geom_point()+ geom_linerange(aes(ymin=l.ci, ymax=u.ci))+ coord_flip()+ ylab(NULL)+ xlab(NULL) # või ka nii: # ggplot(keskmised, aes(cut, keskmine))+ # geom_point()+ # geom_linerange(aes(ymin=l.ci, ymax=u.ci))+ # coord_flip()+ # labs(x = NULL, y = NULL) xlab ja ylab on tegelikult mugavdatud variandid scale funktsioonidest. scale funktsioonid kontrollivad seda kuidas andmed mappitakse aes()i. Võetakse andmed ja teakse neist midagi joonisel nähtavat. Igal aestheticul on oma scale: - Kui x telg on pidev: scale_x_continuous - Kui y telg on kategoriaalne: scale_y_discrete - Kui kasutame fill aesi: scale_fill_discrete ggplot(keskmised, aes(cut, keskmine))+ geom_point()+ geom_linerange(aes(ymin=l.ci, ymax=u.ci))+ coord_flip()+ scale_x_discrete(name=&quot;Lõige&quot;)+ scale_y_continuous(name=&quot;Hind&quot;) Saame kontrollida ka tick marke ja labeleid ggplot(keskmised, aes(cut, keskmine))+ geom_point()+ geom_linerange(aes(ymin=l.ci, ymax=u.ci))+ coord_flip()+ scale_x_discrete(name=&quot;Lõige&quot;, labels=c(1:5))+ scale_y_continuous(name=&quot;Hind&quot;, breaks = seq(3000,5000, by=100)) Kõige võimsam joonise visuaali tööriist, millega saab kontrollida pea kõike, on theme(). Täpsemalt saab selle kohta lugeda https://ggplot2.tidyverse.org/reference/theme.html. Vaatame näiteks, kuidas theme() abil muula legendi asukohta ning kustutada x-telje skaala: dt &lt;- iris names(dt) &lt;- tolower(names(dt)) ggplot(dt, aes(sepal.width, sepal.length, color=species))+ geom_point()+ theme(legend.position=&quot;bottom&quot;, axis.text.x = element_blank()) Saame üksikasjalikult muuta praktiliselt kogu joonise väljanägemist. Saame kasutada ka juba mõningaid valmistehtud theme()ide templatee. Näiteks theme_bw(). dt &lt;- iris names(dt) &lt;- tolower(names(dt)) ggplot(dt, aes(sepal.width, sepal.length, color=species))+ geom_point()+ theme_bw() 2.5 Edasiseks lugemiseks Ri baasteadmised Daniel Navarro, Learning statistics with R: A tutorial for psychology students and other beginners, peatükid 3, 4, 5, 7 dplyr https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html ggplot Grolemund, G., Wickham, H., R for Data Science. peatükk 3 Data visualisation http://r4ds.had.co.nz/ "],["lineaarne-regressioon.html", "Peatükk 3 Lineaarne regressioon 3.1 Lihtne lineaarne regressioon 3.2 Regressioon R-is 3.3 Regressiooni jäägid 3.4 Regressioonimudeli sobitumine 3.5 Kategoriaalsed tunnused regressioonis 3.6 Mitmene regressioon 3.7 Koosmõjud 3.8 Mudelite võrdlemine 3.9 Regressioonimudeli eeldused 3.10 Kuidas eelduste täidetust hinnata?", " Peatükk 3 Lineaarne regressioon 3.1 Lihtne lineaarne regressioon Lihtne lineaarne regressioon (simple linear regression) on statistiline meetod mis võimaldab hinnata ja kvantifitseerida kahe arvtunnuse vahelist suhet. Regressioonsuhte puhul eeldatakse, et üks tunnustest oleks nn sõltuv tunnus ja teine sõltumatu1, kus sõltuva tunnuse väärtus on mõjutatud (sõltub) sõltumatu tunnuse väärtusest. Kui sõltumatuid tunnuseid on rohkem kui üks, on tegemist mitmese regressiooniga (sellest hiljem), ühe sõltumatu tunnuse korral nn lihtsa regressiooniga (simple linear regression). Keskendume esialgu lihtsale variandile. Kasutame näitena Piaaci andmestikku. Tõmbame andmestiku sisse ja uurime graafiliselt sissetuleku (sissetulek) ning matemaatilise kirjaokuse (numeracy) vahelist seost. # Loeme kõigepealt sisse vajalikud paketid library(dplyr) library(ggplot2) library(readr) # Tõmbame sisse andmestiku piaac &lt;- read_csv(&quot;https://github.com/mrksom/kvant/raw/master/data/piaac.csv&quot;) piaac %&gt;% ggplot(aes(x = numeracy, y = sissetulek))+ geom_point(size = 0.5, alpha = 0.3)+ theme_minimal() Tundub, et nende kahe tunnuse vahel on seos olemas. Mida kõrgem on matemaatilise kirjaoskuse skoor, seda kõrgem on sissetulek. Me saame selle suhte kokku võtta regressioonisirge abil. ggplotis on olemas vastav funktsioon geom_smooth(), mis selle joone meile graafikule paneb. Kuna me tahame saada lineaarse regressiooni sirget, siis peame geom_smoothis kasutama argumenti method = \"lm\"2 piaac %&gt;% ggplot(aes(x = numeracy, y = sissetulek))+ geom_point(size = 0.3, alpha = 0.2)+ geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;#972D15&quot;)+ theme_minimal() Regressioonisirge on väljendatav tavalise joone võrrandiga: \\[\\begin{equation} Y=a+bX \\end{equation}\\] kus \\(a\\) on vabaliige (intercept) ja \\(b\\) on sirge tõus (slope). Regressiooni kontekstis kutsutakse seda sirge tõusu regressioonikoefitsiendiks või regressioonikordajaks. Vabaliige tähistab \\(Y\\) väärtust juhul kui \\(X\\) on \\(0\\) (sirge lõikumine y-teljega) ja sirge tõus ühikulist muutust \\(Y\\) väärtuses kui \\(X\\) väärtus muutub ühe ühiku võrra. Eelneva näite puhul oleks vabaliige võrdne sissetulekuga (\\(Y\\)) juhul kui matemaatilise kirjaoskuse tase (\\(X\\)) oleks \\(0\\) ja sirge tõus võrdne keskmise sissetuleku muutusega, mis lisandub iga matemaatilise kirjaoskuse punktiga. Kui sirge tõus on positiivne, siis \\(X\\)i väärtuse kasvades \\(Y\\) väärtus suureneb, kui negatiivne, siis kahaneb. Kui sirge tõus on aga \\(0\\), siis seos kahe tunnuse vahel puudub (iga \\(X\\) väärtuse korral on keskmine \\(Y\\) sama). Linaarse regressioonanalüüsi eesmärgiks ongi leida parim võimalik sirge (st leida vabaliige ja regressioonikoefitsient, mis seda sirget määratlevad) tunnustevahelise lineaarse suhte kirjeldamiseks. Parim võimalik tähendab siinjuures seda, et see sirge läheb punktiparvest läbi võimalikult keskelt, st kirjeldab kõiki punkte võimalikult hästi. 3.2 Regressioon R-is R-is käib lihtsa regressioonimudeli tegemine lm() (linear model) funktsiooniga. Loomulikult on ka teisi funktsioone, mis regressiooni jooksutamisega hakkama saavad ja hea tahtmise korral võib vastava funktsiooni ka mõningase vaevaga ise valmis kirjutada. Kuid jätame teised variandid hetkel kõrvale. lm() funktsioonis tuleb defineerida regressioonivõrrand. Selleks peame määratlema sõltuva tunnuse, seejärel kasutama tildet (~) ning seejärel määratlema sõltumatu(d) tunnuse(d): sõltuv_tunnus ~ sõltumatu_tunnus3. Võtame eelpool toodud näite sissetuleku ja matemaatilise kirjaoskuse seosest ning defineerime regressioonimudeli, millega hindame matemaatilise kirjaoskuse mõju sissetulekule4: lm(sissetulek ~ numeracy, data = piaac) ## ## Call: ## lm(formula = sissetulek ~ numeracy, data = piaac) ## ## Coefficients: ## (Intercept) numeracy ## -140.887 3.606 Lihtsalt lm() funktsiooni jookustades saame kaks numbrit - vabaliikme (intercept), mis antud näite puhul on \\(-140\\), ja regressioonikoefitsiendi (regression coefficient), mis antud näite puhul on \\(3.6\\). Mida need meile ütlevad? Nagu eelnevalt juttu oli, siis vabaliige on \\(Y\\) väärtus kui \\(X\\) on \\(0\\), ehk siis inimesel, kelle matemaatilise kirjaoskuse skoor on \\(0\\), peaks meie mudeli kohaselt sissetulek olema \\(-140\\). Regressioonikoefitsient aga annab meile teada kui palju \\(Y\\) muutub, kui \\(X\\) muutub ühe ühiku võrra, ehk siis kui matemaatilise kirjaoskuse skoor tõuseb ühe punkti võrra, tõuseb sissetulek keskmiselt \\(3.6\\) euro võrra. Nüüd, kui teame mudeli parameetreid, saame nende abil regressioonijoone graafikule kanda ka ilma geom_smoothita: piaac %&gt;% ggplot(aes(x = numeracy, y = sissetulek))+ geom_point(size = 0.3, alpha = 0.2)+ geom_abline(slope = 3.6, intercept = -140, color = &quot;#972D15&quot;)+ coord_cartesian(xlim = c(0,450), ylim = c(0,3500))+ theme_minimal() Kui me teame regressioonisirge tõusu ehk regressioonikoefitsienti ja vabaliiget, siis lähtuvalt sõltumatu tunnuse väärtustest saame prognoosida sõltuva tunnuse väärtuse: \\[\\begin{equation} \\hat{y}_i=b_0+b_1x_i \\end{equation}\\] \\(\\hat{y}_i\\) antud võrrandis tähistab hinnatud või prognoositud \\(y\\) väärtust (sellest ka see müts \\(y\\) peal) vaatlusele \\(i\\). Kui meil on regressioonivõrrand \\(\\hat{y}_i=-140+3.6x_i\\) ja meil on mingi vaatlus \\(i\\), kelle \\(x\\) väärtus on näiteks \\(200\\), siis saame sellele vaatlusele prognoosida \\(y\\) väärtuseks \\(-140+3.6\\times200=580\\). Ehk siis inimesel, kelle matemaatilise kirjaoskuse skoor on 200, peaks meie mudeli järgi sissetulek olema ca 580 eurot. Inimesel, kelle matemaatilise kirjaoskuse skoor on 400, peaks sissetulek olema keskmiselt \\(-140+3.6\\times400=1300\\) eurot Joonis 3.1: Prognoosime y väärtust kui x on 200 ja kui x = 400 3.3 Regressiooni jäägid Samas on muidugi võimatu ühe sirgega kõiki punkte ideaalselt kirjeldada. Iga punkti ja sirge vahele jääb alati mingi viga või teisisõnu, kõik punktid (või vähemalt enamus neist) hälbivad suuremal või vähemal määral regressioonisirgest. Mida suuremad need hälbed on, seda vähem suudab on meie mudel (regressioonisirge) kirjeldada sõltuva tunnuse variatsiooni ja seda suurem on vea määr meie mudelis. Neid hälbeid kutusutakse regressiooni jääkideks (regression residuals). Joonis 3.2: Regressiooni jäägid Ehk siis iga kord, kui prognoosime \\(\\hat{y}_i=\\beta_0+\\beta_1x_i\\) abil \\(y_i\\) väärtust, teeme me mingi vea5. Seetõttu tuleb regressioonivõrrandile lisada vea komponent (\\(\\epsilon\\)) ning võrrand ise muutub vastavalt: \\[\\begin{equation} \\hat{y_i}=\\beta_0+\\beta x_i+\\epsilon \\end{equation}\\] Kõige parem regressioonisirge annab joon, mille puhul jäägid on minimaalsed, ehk siis joon, mille puhul kõikide vaatluste jääkide summa oleks võimalikult väike. Kuna me ei saa jääke kokku võtta neid lihtsalt kokku liites (ca pooled jäägid on väiksemad kui regressioonijoon ja ca pooled suuremad, seega nende summa oleks \\(0\\)), siis tuleb nad enne liitmist ruutu panna. Ja meie eesmärgiks on nüüd leida regressioonisirge, mis minimeeriks ruutjääkide summa (residual sum of squares ehk \\(RSS\\)) ehk siis regressioonisirge, mille puhul \\(RSS\\) oleks võimalikult väike6. Eelnevast lähtuvalt on ka küllaltki loogiline, et meetodit, millega \\(RSS\\) minimeeritakse ja regressioonisirge ning vastavad koefitsiendid leitakse, nimetatakse vähimruutude meetodiks. Ülesanne! Kasutades ggploti ja tehke punktdiagramm geom_point() matemaatilise kirjaoskuse (numeracy) ja funktsionaalse lugemisoskuse (literacy) vahelisest seosest. Pange numeracy x-teljele ja literacy y-teljele. Kasutades geom_abline()i, lisage joonisele lineaarne regressioonijoon (seega peate eelnevalt lm() funktsiooniga leidma regressioonijoone vabaliikme ja regressioonikoefitsiendi) 3.4 Regressioonimudeli sobitumine Olles leidnud joone, mis kirjeldab kahe tunnuse vahelist seost kõige paremini, võiks ju eeldada, et ülesanne on täidetud. Aga kas ikka on? Ükskõik, millisest punktiparvest võib regressioonijoone läbi panna. Kuid tulenevalt regressioonijääkide (vaatluste hälbed regressioonijoonest) suurusest saame selle joone kohta teha väga erinevaid järeldusi. Kui jäägid on väikesed, siis võime suhteliselt täpselt prognoosida sõltuva tunnuse väärtust või teha järeldusi seose kohta. Kuid mida suuremad on jäägid, seda ebatäpsem on ka meie prognoos/järeldus. Üldjuhul kasutame regresioonanalüüsi, et teha valimi baasil järeldusi mingi üldkogumi kohta. Meid huvitab, kas see seos, mida näeme oma valimi andmete põhjal, kehtib ka üldkogumis. Saame küll eeldada, et valimipõhiselt leitud regressioonisirge on suhteliselt sarnane üldkogumi sirgele (sirge, mille me saaksime, kui kaasaksime analüüsi kõik üldkogumi liikmed), aga kui sarnane, seda me ei tea. Kui me võtaksime samast üldkogumist teise valimi, siis juhul, kui mõlemad valimid on võetud korrektselt7 ja valimid on piisavalt suured, siis peaksid nende põhjal leitud regressioonisirged olema suhteliselt sarnased, aga identsed ei ole nad praktiliselt kunagi. Kõikide võimalike valimite puhul me mingil määral alahindame või ülehindame tegelikku, populatsiooni regressioonikoefitsienti (ja ka vabaliiget). Seega, et saada aimu valimipõhise hinnangu täpsusest (vastavusest tegelikule tegelikule üldkogumi parameetrile), peaksime kuidagi välja selgitama valimi kasutamisest tuleneva vea võimaliku suuruse. Et hinnata mudeli sobivust andmetega ja sellega leitud hinnagute täpsust, vajame mudeli kohta täiendavat infot. Eelnevalt regressioonimudelit lm() funktsiooniga jooksutades oli väljund väga lakooniline. Saime teada ainult vabaliikme ja regressioonikoefitsinedi väärtused. Tegelikult on lm() tulem muidugi märksa põhjalikum. Muule mudeliga kaasnevale infole saame ligi kui salvestame mudeli esmalt mingisse andmeobjekti ja kasutame selle andmeobjekti peal summary() käsku8. mudel1 &lt;- lm(sissetulek ~ numeracy, data = piaac) summary(mudel1) ## ## Call: ## lm(formula = sissetulek ~ numeracy, data = piaac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1016.7 -351.5 -129.1 179.4 2923.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -140.887 56.510 -2.493 0.0127 * ## numeracy 3.606 0.202 17.849 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 555.8 on 3982 degrees of freedom ## (3648 observations deleted due to missingness) ## Multiple R-squared: 0.07408, Adjusted R-squared: 0.07385 ## F-statistic: 318.6 on 1 and 3982 DF, p-value: &lt; 2.2e-16 # Kui me ei taha mudelit salvestada, siis saab ka nii: summary(lm(numeracy ~ literacy, data = piaac)) Nüüd näeme juba märksa põhjalikumat väljundit. Vaatame mis seal kirjas on ja kuidas seda tõlgendada. Käime väljundi sektsioonide kaupa läbi (v.a. esimene rida, mis on vist niigi suht selge) 3.4.1 Jääkide jaotus ## Residuals: ## Min 1Q Median 3Q Max ## -1016.7 -351.5 -129.1 179.4 2923.4 Väljundis on kirjeldatud regressiooni jääkide (residuals) jaotus. Enne nägime, et regressiooni jäägid on regressioonijoone ja tegelike, vaadeldud väärtuste vahe. Mida väiksemad on jäägid, seda täpsemini kirjeldab regressioonijoon andmete vahelist seost. Nägime ka, et pooled jäägid peaksid ideaalis olema suuremad (positiivse märgiga) kui regressioonisirge ja pooled väiksemad (negatiivse märgiga). Seega peaks jääkide keskmine olema ligikaudu \\(0\\) ning jääkide jaotus normaaljaotuse sarnane, kus esimene ja kolmas kvartiil, aga ka maksimum ja miinimum, on keskväärtusest umbes sama kaugel. Hiljem vaatame jääkide jaotust ka graafiliselt, mis on märksa mõistlikum viis neid uurida, kuid esmase mulje saab ka siit kätte. 3.4.2 Regressioonikoefitsiendid ja nende olulisus ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -140.887 56.510 -2.493 0.0127 * ## numeracy 3.606 0.202 17.849 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Koefitsientide sektsioonis on esitatud mudeli oluliseim info. Estimate on hinnang mudeliga leitud regressioonikoefitsientidele. Lihtsa regressiooni puhul on meil ainult vabaliige ja ühe sõltumatu tunnuse koefitsient. Hiljem, mitmese regressiooni kontekstis, on neid koefitsiente rohkem. Vabaliikmeid on aga mudeli kohta alati üks. Tulbas Std. Error on toodud koefitsientide standardvead. Standardviga kirjeldab meie mudeli hinnangus sisalduvat määramatust. Me kasutame regressioonikoefitsientide leidmiseks üldjuhul valimipõhiseid andmeid, kuigi tegelikult huvitavad meid ju üldkogums esinevad seosed. Valimipõhine hinnang peaks piisavalt suure valimi korral olema tõenäoliselt küllaltki sarnane üldkogumi vastavale parameetrile, kuid väikese valimi korral puhta juhuse läbi sellest arvestatavalt erineda. Standardviga näitabki kui kindlad me oma mudeli hinnangus olla saame. Mida väiksem on standardviga (võrreldes hinnangu endaga), seda kindlamad võime olla ka oma hinnangus. Standardvea suurs sõltub eelkõige jääkide hajuvusest ja valimi suurusest. Mida väiksemad on jäägid ja mida suurem on valim, seda väiksem on ka standardviga. Standardvea abil saame t-testi abil testida, kas regressioonikoefitsient erineb oluliselt nullist (kui koefitsient on null, siis seos tunnuste vahel puudub). t-testi tulemust näitab veerg t value. t-väärtus ütleb meile kui mitme standardvea kaugusel meie regressioonikoefitsient 0-st on. Kui on piisavalt kaugel, siis saame järeldada, et leitud koefitsient on ka üldkogumis 0-st erinev. Kui kaugel on aga piisavalt kaugel? See sõltub sellest, kui suurt vea tõenäosust me oleme valmis tolereerima (mingi vea tõenäosus jääb seejuures alati). Üldjuhul valitakse selleks tõenäosuseks \\(5\\%\\) (ütleme, et regressioonikoefitsient on statistiliselt oluline usaldusnivool \\(95 \\%\\) või olulisusnivool \\(p &lt; 0.05\\)), aga see võib olla ka \\(1\\%\\) või \\(10\\%\\). Siin tegelikult ei ole mingit väga konkreetset piirmäära, millest juhinduda. Kui me aga lepime kokku, et võimaliku vea tõenäosusena aktsepteerime \\(5\\)-te protsenti, siis peab t-väärtus olema suurem kui ca \\(\\pm2\\) (täpne väärtus sõltub vaatluste arvust). Antud näite puhul on t-väärtused \\(-2.5\\) ja \\(17.8\\), ehk siis mõnevõrra suuremad kui \\(\\pm2\\) ja me võime järeldada, et nii vabaliige kui regressioonikoefitsient erinevad olulisusnivool \\(95\\%\\) oluliselt nullist (kuigi jah, vabaliige on suhteliselt piiri peal). Õnneks ei pea me seda täpset t-väärtuse piirmäära ise välja nuputama. R arvutab meile automaatselt võimaliku vea tõenäosuse konkreetse t-väärtuse kohta. See tõenäosus on ära toodud veerus Pr(&gt;|t|) ja seda nimetatakse p-väärtuseks. p-väärtuse tõlgendus on: kui tõenäoline on, et me saaksime niivõrd suure või suurema t-väärtuse nagu me saime, kui regressioonikoefitsient oleks üldkogumis tegelikult \\(0\\). Seega kui p-väärtus on näiteks \\(0.04\\), siis oleks tõenäosus, et me saaksime sellise regressioonikoefitsiendi, juhul kui üldkogumis oleks regressioonikoefitsient tegelikult \\(0\\) (ehk tunnuste vahe seost ei oleks), \\(0.04\\) ehk \\(4\\)% või väiksem. Üldjuhul tahaksime näha p-väärtust, mis on väiksem kui \\(0.05\\). Sellisel juhul oleks koefitsient statistiliselt oluline usaldusnivool \\(95\\%\\). Antud näites on meil regressioonikoefitsiendi puhul tegemist väga väikeste p väärtustega (&lt;2e-16 tähendab väiksem kui \\(2\\times10^{-16}\\)) ja me võime olla päris kindlad, et koefitsient erineb nullist. Vabaliikme p-väärtus on aga \\(0.012\\), ehk kui me kasutaksime usaldusnivood \\(99\\%\\) (mille puhul p-väärtus peaks olema väiksem kui \\(0.01\\)), siis me ei saaks järeldada, et see on statistiliselt oluliselt erinev nullist. Lisaks kuvab R iga p-väärtuse taha ka tärnid, mis indikeerivad selle väärtuse suurust lähtuvalt allolevast legendist. Miks meil on üldse vaja teada kas koefitsiendid erinevad oluliselt nullist? Aga sellepärast, et kui regressioonisirge oleks \\(0\\), siis meie tunnuste vahel ei oleks seost (kui \\(X\\) muutub \\(1\\) ühiku võrra, siis \\(Y\\) muutub \\(0\\) ühiku võrra, ehk siis \\(Y\\) väärtus ei sõltu \\(X\\)i väärtusest). Aga kuidas on lood vabaliikmega? Kas ka see peab erinema nullist, et meie mudelist mingit tolku oleks? Tegelikult ju ei pea. Võib täitsa vabalt juhtuda, et regressioonisirge lähebki läbi \\(X\\) ja \\(Y\\) telgede ristumiskoha (\\(Y\\) on \\(0\\) kui \\(X\\) on \\(0\\)). Sellisel juhul oleks vabaliikme t-väärtus väiksem kui \\(2\\) ja p-väärtus suurem kui 0.05, kuid mudeli tõlgendust see ei mõjutaks. Ehk siis tavaliselt meid vabaliikme p ja t väärtused väga ei huvita. Küll aga peaks jälgima, et standardviga väga suur (võrreldes vabaliikme endaga) ei oleks. 3.4.3 Jääkide standardviga ## Residual standard error: 555.8 on 3982 degrees of freedom ## (3648 observations deleted due to missingness) Kuidas hinnata regressiooniprognoosi täpsust, ehk siis seda kui hästi regressioonimudel sobitub andmetega (model fit)? Üheks võimaluseks on lähtuda samast loogikast mida kasutame tunnuse keskväärtuse täpsuse hindamisel. Ehk kui palju vaatlused keskmiselt erinevad keskväärtusest. Regressioonijoone puhul ei ole meil ühte keskväärtust, mille suhtes vaatluste hälbimist määrata. Kuid iga vaatluse sõltumatu tunnuse väärtuse \\(x\\) kohta on meil hinnatud sõltuva tunnuse väärtus \\(\\hat{y}\\). Seega tuleb meil lihtsalt vaadata kui palju vaatluste \\(y\\) ja \\(\\hat{y}\\) väärtused keskmiselt erinevad, ehk kui suur on keskmine viga meie mudelis. Regressioonanalüüsi kontekstis kutsutakse seda vaatluste varieeruvuse näitajat keskmiseks ruutveaks (mean squared error) ehk lühidalt \\(MSE\\)9. Kuna aga \\(MSE\\) väärtus on ruudus, siis on seda keeruline interpreteerida (samamoodi nagu ka dispersiooni). Kui me võtame ruutjuure \\(MSE\\)st, \\(\\sqrt{MSE}\\), saame regressiooni jääkide standardhälbe, mida nimetatakse jääkide standardveaks (residual standard error ehk RSE). Mida väiksem on mudeli RSE, seda paremini mudel andmetega sobitub (seda vähem hälbivad vaatlused regressioonijoonest ehk seda väiksemad on regresiooni jäägid). See, kui väike peaks RSE väärtus hea mudeli korral olema, sõltub eelkõige kontekstist ja sõltuva tunnuse skaalast (samamoodi nagu keskväärtuse standardhälve). Mingeid konkreetseid piirväärtusi siinkohal tuua ei ole võimalik. Lisaks on siin ära toodud ka degrees of freedom ehk vabadusastmete arv jääkide standardvea arvutamisel. Sisuliselt on siin kirjas analüüsi kaasatud vaatluste arv (miinus regressioonikordajate arv, siinses mudelis 2). Ära on toodud ka analüüsist välja jäetud vaatluste arv. Need on need, kellel puudus väärtus vähemalt ühe analüüsitava tunnuse jaoks. 3.4.4 R ruut ## Multiple R-squared: 0.07408, Adjusted R-squared: 0.07385 Vast oluliseimaks mudeli headuse näitajaks on \\(R^2\\). Regressioonanalüüsi eesmärk on seletada mingit osa sõltuva tunnuse variatiivsusest sõltumatu tunnuse abil. Seega saame regressioonimudeli puhul hinnata ja mudeli kvaliteedi iseloomustusena kasutada sõltumatu tunnuse poolt seletatud variatiivsuse osakaalu sõltuva tunnuse koguvariatiivsusest. Sõltuva tunnuse variatiivsuse (seda nimetatakse \\(TSS\\) ehk total sums of squares) saab jagada komponentideks: variatiivsus, mis on seletatud regressioonijoone poolt (\\(ESS\\) ehk explained sums of squares) ja variatiivsus, mis ei ole regressioonijoone poolt seletatud ehk siis mudeli seisukohast viga (\\(RSS\\) ehk residual sums of squares): \\[TSS=RSS+ESS\\] \\[\\begin{equation} ESS=\\sum_{i=1}^{n}(\\hat{y}_i-\\bar{y})^2 \\end{equation}\\] \\[\\begin{equation} RSS=\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2 \\end{equation}\\] \\[\\begin{equation} TSS=\\sum_{i=1}^{n}(y_i-\\bar{y})^2 \\end{equation}\\] Teades erinevaid variatiivsuse komponente, saame määrata kui suur osa (mitu protsenti) sõltuva tunnuse koguvariatsioonist on seletatav regressioonijoone poolt (ehk siis sõltumatu tunnuse poolt). Seda suurust nimetatakse determinatsioonikordajaks ehk lühidalt \\(R^2\\)-ks. \\[\\begin{equation} R^2=\\frac{TSS-RSS}{TSS}=1-\\frac{RSS}{TSS} \\end{equation}\\] Joonis 3.3: Variatsiivsuse jagunemine \\(R^2\\) jääb vahemikku \\(0-1\\). See mõõdab seose tugevust, st mida lähemal \\(R^2\\) on \\(1\\)le, seda tugevam lineaarne seos tunnuste vahel on ja seda enam sõltumatu tunnus sõltuva tunnuse variatsiooni seletab, seega seda efektiivsem on regressioonifunktsiooni kasutamine selle asemel, et lihtsalt sõltuva tunnuse keskmist hinnata (kui \\(R^2\\) on \\(0\\), siis regressioonijoon langeb kokku sõltuva tunnuse keskmist tähistava joonega, st et \\(ESS=0\\) ja \\(TSS=RSS\\)). R annab meile lisaks tavalisele \\(R^2\\) väärtusele (Multiple R-squared) ka nn korrigeeritud \\(R^2\\) väärtuse (Adjusted R-squared). Korrigeeritud \\(R^2\\) puhul võetakse arvesse ka sõltumatute tunnuste arvu. Iga lisanduva sõltumatu tunnusega läheb tavaline \\(R^2\\) suuremaks. Kui lisanduv tunnus eriti midagi ei seleta, siis võib see tõus olla väga väike, kuid mingi tõus paratamatult on. Korrigeeritud \\(R^2\\), arvestades oma valemis ka sõltumatute tunnuste arvu, annab mitme sõltumatu tunnuse korral korrektsema tulemuse. Hetkel, lihtsa regressiooni kontekstis, kus meil on ainult üks sõltumatu tunnus, annavad mõlemad variandid (enam-vähem) sama tulemuse. 3.4.5 F-väärtus ja F-test ## F-statistic: 318.6 on 1 and 3982 DF, p-value: &lt; 2.2e-16 F-väärtus, sarnaselt t-väärtusele, aitab meil hinnata kas meie mudel on statistiliselt oluline, ehk siis kas meie analüüsitavate tunnuste vahel on oluline lineaarne seos. F-väärtuseks nimetatakse mudeli abil seletatud variatiivsuse ja seletamata variatiivsuse suhet10: \\[\\begin{equation} \\text{F-suhe} = \\frac{\\text{regressioonimudeli poolt seletatud variatiivsus}}{\\text{regressioonimudeli poolt seletamata variatiivus}} \\end{equation}\\] Kui mudeli regressioonisirge on \\(0\\), siis peaks see suhe olema \\(1\\). See tähendab, et regressioonisirge ei seleta üldse sõltuva tunnuse varieeruvust. Kui regressioonisirge on suurem kui \\(0\\) siis peaks regressioonisirge poolt seletatud varieeruvus (koos juhusliku varieeruvusega) olema suurem kui ainult juhuslik dispersioon. Saame jällegi kasutada F-väärtusega kaasnevat p väärtust, et hinnata kas see F-väärtus on piisavalt suur, et saaksime mudelist lähtuvalt mingeid sisukaid järeldusi teha. Võite märgata, et need kaks testi regressioonimudeli kohta annavad sama p väärtuse. Ja tegelikult annavad nad ka sama teststatistiku. t-statistik on lihtsalt ruutjuur F statistikust11. Võib tekkida küsimus, et miks me siis kahte testi peame kasutama. Ühe sõltumatu tunnusega regressioonimudelis otseselt ei peagi. Samas kui meil on mitu sõltumatut tunnust (nagu meil hiljem on), siis F ja t väärtused muutuvad. F-testiga saab sel juhul testida terve mudeli headust, st kas meie sõltumatud tunnused koos suudavad seletada piisavalt sõltuva tunnuse variatiivsust (tegelikult testib F-test seda, et kas vähemalt üks koefitsientidest erineb nullist). t-statistikud aga arvutatakse igale regressioonikoefitsiendile eraldi ning nendega saame kontrollida iga üksiku koefitsiendi erinevust nullist. Ülesanne! Looge regressioonimudel, millega hindate numeracy mõju literacyle. Salvestage see mudel ja uurige summary() funktsiooniga. Kas numeracy mõju literacyle on statistiliselt oluline? Mitu protsenti literacy variatsioonist on selgitatav läbi numeracy? 3.5 Kategoriaalsed tunnused regressioonis 3.5.1 Üks binaarne sõltumatu tunnus Siiani oleme käsitlenud ainult mudeleid, kus sõltumatuteks tunnusteks on pidevad muutujad. Kuid me saame mudelisse lülitada ka kategoriaalseid tunnuseid. Vaatame esmalt mudelit, kus on üks kategoriaalne sõltumatu muutuja12. Teeme Piaaci andmete põhjal mudeli, millega hindame soo mõju sissetulekule mudel2 &lt;- lm(sissetulek ~ sugu, data = piaac) summary(mudel2) ## ## Call: ## lm(formula = sissetulek ~ sugu, data = piaac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -974.0 -344.0 -122.7 198.1 2755.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1077.90 13.35 80.73 &lt;2e-16 *** ## suguNaine -383.15 17.52 -21.86 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 545.8 on 3982 degrees of freedom ## (3648 observations deleted due to missingness) ## Multiple R-squared: 0.1072, Adjusted R-squared: 0.107 ## F-statistic: 478 on 1 and 3982 DF, p-value: &lt; 2.2e-16 Kuidas seda tulemust tõlgendada? Regressioonimudeliga hindame \\(\\hat{y_i}\\) väärtust vaatlusele \\(i\\), kui sõltumatu tunnuse väärtus muutub ühe ühiku võrra. \\(\\hat{y_i}\\)i väärtus kujuneb siis lähtuvalt vabaliikme \\(\\beta_0\\) ja regressioonikoefitsiendi \\(\\beta\\) ning sõltumatu tunnuse \\(x_i\\)i korrutise summast (pluss mingi viga): \\(\\hat{y_i}=\\beta_0+\\beta x_i+\\epsilon\\). Sealjuures vabaliige on \\(y\\) väärtus kui \\(x\\) on \\(0\\). Meil on tunnus \\(x\\) (sugu) kahe kategooriaga. Mis juhtub, kui kodeerime selle ümber väärtusteks \\(0\\) ja \\(1\\) (vastavalt mees ja naine). R kusjuures teeb seda automaatselt. \\[ x_{i} = \\begin{cases} 1 &amp; \\quad \\text{kui on naine}\\\\ 0 &amp; \\quad \\text{kui on mees} \\end{cases} \\] Kui me selle tunnuse nüüd regressioonivõrrandisse paneme, siis mis on \\(y_i\\) väärtus kui \\(x_i\\) on \\(1\\) (ehk siis vaatluse sugu on naine) ja mis on \\(y_i\\) väärtus kui \\(x_i\\) on \\(0\\) (ehk siis vaatluse sugu on mees)? \\[ \\hat{y_i}=\\beta_0+\\beta_1 x_i = \\begin{cases} \\beta_0+(\\beta_1 \\times 1) = \\beta_0+\\beta_1 &amp; \\quad \\text{kui on naine}\\\\ \\beta_0+(\\beta_1 \\times 0) = \\beta_0 &amp; \\quad \\text{kui on mees} \\end{cases} \\] Ehk siis kui \\(x_i\\) väärtus on \\(0\\) (mehed), siis võrdub \\(\\hat{y_i}\\) vabaliikmega \\(\\beta_0\\) (sest \\(\\beta_1\\) korrutatakse läbi nulliga) ja kui \\(x_i\\) väärtus on \\(1\\) (naised), siis vabaliikme ja regressioonikoefitsiendi summaga \\(\\beta_0+\\beta\\). Mida \\(\\hat{y}\\) antud juhul üldse tähistab? Pidevmuutujaga regressioonis tähistas see keskmist \\(y\\)-i väärtust erinevate \\(x\\) väärtuste korral. Ja siin täpselt samamoodi. Aga nüüd on meil ainult kaks \\(x\\) väärtust ja \\(\\hat{y}\\) on vastavate gruppide (meeste ja naiste) keskmine \\(y\\). Seega saame regressioonivõrrandiga väljenda binaarse tunnuse mõju sõltuva tunnuse keskmisele. Lihtsalt käsitleme ühte kategooriat nn referentskategooriana ja kodeerime selle \\(0\\)ks. Kui \\(x\\) on \\(0\\), siis \\(y\\) väärtus on võrdne vabaliikme väärtusega. Ja kui sõltumatu tunnuse väärtus muutub ühe ühiku võrra (ja rohkem ta ei saagi muutuda), siis on \\(y\\) väärtus võrdne vabaliikme väärtus pluss regressioonikoefitsiendi väärtus. Kuidas me eelneva valguses oma näidet siis tõlgendama peaksime? ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1077.90 13.35 80.73 &lt;2e-16 *** ## suguNaine -383.15 17.52 -21.86 &lt;2e-16 *** Sugu oli tekstiline tunnus. R saab aru, et tegemist on kategoriaalse tunnusega ja kodeerib selle sisemiselt ümber \\(0\\)-ks ja \\(1\\)-ks. Antud juhul määras ta kategooria Naine \\(1\\)-ks ja kategooria Mees \\(0\\)-ks. Kuna tegemist oli tekstilise tunnusega, siis lähtub R siin tähestikulisest järjekorrast. Ümberkodeeritud (dihhotomiseeritud) tunnuse nimeks on alati mitte-referentskategooria. Antud juhul siis suguNaine. Regressioonivõrrand oli järgmine: \\[\\hat{y_i}=\\beta_0+\\beta x_i+\\epsilon\\] kus \\[\\begin{align} x_{i} = \\begin{cases} 1 &amp; \\quad \\text{kui on naine}\\\\ 0 &amp; \\quad \\text{kui on mees} \\end{cases} \\end{align}\\] Paneme mudeli tulemused sellesse võrrandisse: \\[\\begin{align} \\text{keskmine sissetulek}&amp;=1077.90+(-383.15)\\times \\text{naine}\\\\ &amp;= \\begin{cases} 1077.90-383.15\\times 1 &amp; \\quad \\text{kui on naine}\\\\ 1077.90-383.15\\times 0 &amp; \\quad \\text{kui on mees} \\end{cases}\\\\ &amp;= \\begin{cases} 1077.90-383.15 &amp; \\quad \\text{kui on naine}\\\\ 1077.90-0 &amp; \\quad \\text{kui on mees} \\end{cases}\\\\ &amp;= \\begin{cases} 694.75 &amp; \\quad \\text{kui on naine}\\\\ 1077.90 &amp; \\quad \\text{kui on mees} \\end{cases} \\end{align}\\] Ehk siis naiste keskmine sissetulek on \\(694.8\\) eurot (vabaliige + regressioonikoefitsient) ja meeste oma \\(1077.9\\) eurot (vabaliige). Erinevus on statistiliselt oluline, kuna p-väärtused nii koefitsiendi t-testi kui ka mudeli F-testi puhul olid olulisusnivool \\(95\\%\\) olulised (väiksemad kui \\(0.05\\)). Kui me paneme need keskmised joonisele ja ühendame nad joonega, näeme, et selle joone tõus (slope) on võrdne regressioonikoefitsiendiga, täpselt samuti nagu pidevtunnusega regressioonis. 3.5.2 Kolme või enama kategooriaga sõltumatu tunnus Kusjuures me ei pea piirduma vaid binaarsete tunnustega. Aga kui kategooriaid on rohkem, tuleb meil nad binaarseks teha ehk dihhotomiseerida. Määratleme ühe kategooria referentskategooriana ja ülejäänud kategooriad kodeerime erinevates tunnustes \\(1\\)ks. Seega, kui meil on näiteks hariduse tunnus kolme kategooriaga (põhiharidus, keskharidus, kõrgharidus), peame määratlema ühe referentskategooria (näiteks põhiharidus) ja tegema kaks uut tunnust (vastavalt keskhariduse ja kõrghariduse kategooriatele): \\[ kesk_{i} = \\begin{cases} 1 &amp; \\quad \\text{kui inimene on keskharidusega}\\\\ 0 &amp; \\quad \\text{kui inimene ei ole keskharidusega} \\end{cases} \\] \\[ korg_{i} = \\begin{cases} 1 &amp; \\quad \\text{kui inimene on kõrgharidusega}\\\\ 0 &amp; \\quad \\text{kui inimene ei ole kõrgaridusega} \\end{cases} \\] Nüüd saame iga inimese hariduse määratleda kahe tunnuse kaudu. Ehk siis inimene, kelle puhul \\(kesk = 1\\) ja \\(korg = 0\\), on keskharidusega; inimene kelle puhul \\(kesk = 0\\) ja \\(korg = 1\\), on kõrgharidusega ja inimene kelle puhul \\(keks = 0\\) ja \\(korg = 0\\), on põhiharidusega. \\(y\\) väärtus kujuneb täpselt samamoodi nagu binaarse tunnuse puhul: \\[y_i=\\beta_0+\\beta_1 \\times kesk_i+\\beta_2 \\times korg_i\\] \\[\\begin{align} &amp;= \\begin{cases} \\beta_0+\\beta_1 \\times 1+\\beta_2 \\times 0 &amp; \\quad \\text{keskharidusega}\\\\ \\beta_0+\\beta_1 \\times 0+\\beta_2 \\times 1 &amp; \\quad \\text{kõrgaridusega}\\\\ \\beta_0+\\beta_1 \\times 0+\\beta_2 \\times 0 &amp; \\quad \\text{põhiharidusega} \\end{cases}\\\\ &amp;= \\begin{cases} \\beta_0+\\beta_1 &amp; \\quad \\text{keskharidusega}\\\\ \\beta_0+\\beta_2 &amp; \\quad \\text{kõrgaridusega}\\\\ \\beta_0 &amp; \\quad \\text{põhiharidusega} \\end{cases} \\end{align}\\] Vaatame kuidas see kõik R-is välja näeb. Hindame hariduse (tunnus haridustase) mõju sissetulekule: mudel3 &lt;- lm(sissetulek ~ haridustase, data = piaac) summary(mudel3) ## ## Call: ## lm(formula = sissetulek ~ haridustase, data = piaac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -868.3 -362.5 -145.2 187.1 2881.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 763.35 13.33 57.279 &lt;2e-16 *** ## haridustaseKõrge 217.23 19.06 11.400 &lt;2e-16 *** ## haridustaseMadal -22.20 30.17 -0.736 0.462 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 567.1 on 3981 degrees of freedom ## (3648 observations deleted due to missingness) ## Multiple R-squared: 0.03631, Adjusted R-squared: 0.03583 ## F-statistic: 75 on 2 and 3981 DF, p-value: &lt; 2.2e-16 R sai jällegi ise aru, et haridustase on tekstiline tunnus ja dihotomiseeris selle automaatselt ära, tehes kaks uut tunnust: haridustaseKõrge (kus kõik kõrgharitud on kodeeritud \\(1\\)-na ja kõik teised \\(0\\)-na) ja haridustaseMadal (kus kõik madala haridustasemega on kodeeritud \\(1\\)-na ja kõik teised \\(0\\)-na). Referentskategooriaks võttis ta tähestiku järjekorras esimese kategooria Keskmine (kõik vaatlused, mille puhul nii haridustaseKõrge kui ka haridustaseMadal on \\(0\\)-d, on keskmise haridustasemega). Tulemuste interpreteerimine toimub samamoodi nagu binaarse tunnuse puhul. Vabaliige tähistab referentskategooria, ehk antud juhul keskmise haridustasemega inimeste, keskmistsissetulekut (\\(763.35\\)), haridustaseKõrge regressioonikordaja tähistab kõrge haridustasemega inimeste skoori erinevust referentskategooria keskmisest (vabaliikmest) ja haridustaseMadal madala haridustasemega inimeste skoori erinevust referentskategooria keskmisest (vabaliikmest). Võrrandi kujul näeb tulem välja järgmine: \\[y_i=\\beta_0+\\beta_1 \\times korge_i+\\beta_2 \\times madal_i\\] \\[\\begin{align} &amp;= \\begin{cases} 763.35+217.23 \\times 1+(-22.20) \\times 0 &amp; \\quad \\text{kõrge haridustase}\\\\ 763.35+217.23 \\times 0+(-22.20) \\times 1 &amp; \\quad \\text{madal haridustase}\\\\ 763.35+217.23 \\times 0+(-22.20) \\times 0 &amp; \\quad \\text{keskmine haridustase} \\end{cases}\\\\ &amp;= \\begin{cases} 763.35+217.23+0 &amp; \\quad \\text{kõrge haridustase}\\\\ 763.35+0-22.20 &amp; \\quad \\text{madal haridustase}\\\\ 763.35+0+0 &amp; \\quad \\text{keskmine haridustase} \\end{cases}\\\\ &amp;= \\begin{cases} 980.58 &amp; \\quad \\text{kõrge haridustase}\\\\ 741.15 &amp; \\quad \\text{madal haridustase}\\\\ 763.35 &amp; \\quad \\text{keskmine haridustase} \\end{cases} \\end{align}\\] Kui kategoriaalne sõltumatu tunnus on tekstiline (character), siis valib R referentskategooriaks tähestikuliselt esimese kategooria. Kui tunnus on faktortunnus (factor), siis valib R esimese faktortaseme. Faktortasemeid saame me aga muuta. Tihti tahame referentskategooria ise valida (näiteks kõige suurema grupi või grupi, mida on loogiline teistega võrrelda). Näiteks tahame haridustasemete puhul määrata referentskategooriaks põhihariduse. Selleks teeme tunnuse faktoriks ja määrame tasemete järjestuse nii, et madal haridustase oleks esimene: # vaatame kõigepealt mis kategooriad tunnuses on unique(piaac$haridustase) ## [1] &quot;Keskmine&quot; &quot;Madal&quot; &quot;Kõrge&quot; NA # Laeme forcats paketti, millega on mugav faktoritega toimetada library(forcats) # Kasutame funktsiooni fct_relevel() # Meil on antud juhul vaja määrata ainult esimene tasand, # ülejäänud tulevad tähestiku järjekorras. piaac &lt;- piaac %&gt;% mutate(haridustase_f = fct_relevel(haridustase, &quot;Madal&quot;)) # Baas-R-is käiks faktori tegemine nii: #piaac$haridustase_f &lt;- factor(piaac$haridustase, # levels = c(&quot;Madal&quot;,&quot;Keskmine&quot;,&quot;Kõrge&quot;)) # ja kui me nüüd regressiooni jooksutame, on referentsiks madal tase summary(lm(sissetulek ~ haridustase_f, data = piaac)) ## ## Call: ## lm(formula = sissetulek ~ haridustase_f, data = piaac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -868.3 -362.5 -145.2 187.1 2881.2 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 741.15 27.07 27.381 &lt; 2e-16 *** ## haridustase_fKeskmine 22.20 30.17 0.736 0.462 ## haridustase_fKõrge 239.43 30.30 7.902 3.54e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 567.1 on 3981 degrees of freedom ## (3648 observations deleted due to missingness) ## Multiple R-squared: 0.03631, Adjusted R-squared: 0.03583 ## F-statistic: 75 on 2 and 3981 DF, p-value: &lt; 2.2e-16 Kui meil juba on faktortunnus, aga tahame selle tasemete järjekorda muuta, saame jälle kasutada käsku fct_relevel(). Muudame haridustaseme faktortunnuses kõrgema hariduse esimeseks tasemeks: piaac$haridustase_f &lt;- fct_relevel(piaac$haridustase_f, &quot;Kõrge&quot;) # Baas-R-is käiks see nii: #piaac$haridustase_f &lt;- relevel(piaac$haridustase_f, ref = &quot;Kõrge&quot;) summary(lm(numeracy ~ haridustase_f, data = piaac)) ## ## Call: ## lm(formula = numeracy ~ haridustase_f, data = piaac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -202.991 -26.446 2.328 28.800 150.020 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 289.2173 0.8172 353.93 &lt;2e-16 *** ## haridustase_fMadal -40.8718 1.4011 -29.17 &lt;2e-16 *** ## haridustase_fKeskmine -21.0568 1.0944 -19.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 42.72 on 7583 degrees of freedom ## (46 observations deleted due to missingness) ## Multiple R-squared: 0.1065, Adjusted R-squared: 0.1063 ## F-statistic: 451.9 on 2 and 7583 DF, p-value: &lt; 2.2e-16 Ülesanne! Piaaci andmestikus on tunnus meeldib_oppida. Tehke see faktortunnuseks nii, et esimene kategooria oleks Mõningal määral (kategooriate nimed saate teada näiteks funnktsiooniga unique(piaac$meeldib_oppida)) Tehke regressioonimudel, kus hindate õppimishimu mõju sissetulekule 3.6 Mitmene regressioon Siiani oleme käsitlenud lineaarset regressiooni, kus sõltumatuid tunnuseid oli üks. Aga on võimalik lülitada ühte mudelisse ka mitu sõltumatut tunnust. Miks see hea peaks olema? Valdavalt üritame välja selgitada (või tegelikult mingi teooria põhjal testida) mingi tunnuse kausaalset mõju teisele tunnusele (sõltumatu tunnuse mõju sõltuvale tunnusele). Kausaalsusel on aga teatud eeldused: Tunnuste vaheline seos (seose olemasolu ei tähenda muidugi kohe põhjalikkust) Ajaline järgnevus (vastupidi ei saaks ju kuidagi olla) Alternatiivse seletuse/põhjuse kõrvaldamine (sõltuv tunnus võib olla sõltumatu tunnuse poolt mõjutatud läbi mõne muu tunnuse, st kaudselt) Mitmene regressioon võimaldabki meil testida sõltumatute tunnuste otsest mõju sõltuvale tunnusele, kontrollides samal ajal teiste mudelisse lülitatud sõltumatute tunnuste mõjude suhtes (hoides teisi tunnuseid konstantsetena). Regressioonivõrrand mitme sõltumatu tunnuse puhul on sarnane ühese regressiooni võrrandiga, välja arvatud siis sõltumatute tunnuste arv. Mudel \\(y\\) prognoosimiseks \\(p\\) sõltumatute tunnuste kaudu on väljendatav järgmiselt: \\[\\begin{equation} y_{i}=\\beta_{0}+\\beta_{1}x_{i,1}+\\beta_{2}x_{i,2}+\\ldots+\\beta_{p}x_{i,p}+\\epsilon_{i} \\end{equation}\\] Kus: \\(\\beta_0\\) on vabaliige (ehk \\(y\\) väärtus kui kõik sõltumatud tunnused on \\(0\\)id) \\(\\beta_1\\) regressioonikoeffitsient esimesele sõltumatule tunnusele \\(x_1\\) \\(\\beta_2\\) regressioonikoeffitsient teisele sõltumatule tunnusele \\(x_2\\) \\(\\beta_{p}\\) regressioonikoeffitsient tunnusele \\(x_{p}\\) \\(\\epsilon\\) on mudeli jääk igale vaatlusele \\(\\beta\\) coefitsinedid on leitud nii, et nendega kaalutud tunnuste väärtused minimeerivad \\(\\epsilon\\)i ehk mudeli viga (kogu mudeli mõistes minimeerivad ruuthälvete summat). \\(\\beta\\) väärtus on tõlgendatav kui muutus \\(y\\) väärtuses, kui vastava sõltumatu tunnuse väärtus muutub ühe ühiku võrra, hoides samal ajal teisi sõltumatuid tunnuseid konstantsetena. See tähendab, et koefitsientides on teiste tunnuste mõju arvesse võetud ja meie tulemused peegeldavad nn puhast mõju. Mudeli defineerimisel R-is saame sõltumatuid tunnuseid lisada + märgi abil: mudel4 &lt;- lm(sissetulek ~ numeracy + sugu, data = piaac) summary(mudel4) ## ## Call: ## lm(formula = sissetulek ~ numeracy + sugu, data = piaac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1181.73 -323.46 -98.77 167.86 2813.48 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 140.7631 55.0470 2.557 0.0106 * ## numeracy 3.3533 0.1915 17.509 &lt;2e-16 *** ## suguNaine -365.0623 16.9196 -21.576 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 526 on 3981 degrees of freedom ## (3648 observations deleted due to missingness) ## Multiple R-squared: 0.171, Adjusted R-squared: 0.1706 ## F-statistic: 410.7 on 2 and 3981 DF, p-value: &lt; 2.2e-16 Mitmese regressiooni tõlgendus on analoogne lihtsa regressiooni tõlgendusega. Võrrandi kujul on see väljendatav järgmiselt: \\[\\hat{y}_{sissetulek}=\\beta_0+\\beta_1 \\times numeracy + \\beta_2 \\times naine\\] \\[\\begin{align} &amp;= \\begin{cases} \\beta_0+\\beta_1 \\times numeracy + \\beta_2 \\times 1 = &amp; \\quad \\text{naine}\\\\ \\beta_0+\\beta_1 \\times numeracy + \\beta_2 \\times 0 = &amp; \\quad \\text{mees} \\end{cases}\\\\ &amp;= \\begin{cases} (\\beta_0+\\beta_2)+\\beta_1 \\times numeracy &amp; \\quad \\text{naine}\\\\ \\beta_0+\\beta_1 \\times numeracy &amp; \\quad \\text{mees} \\end{cases}\\\\ \\end{align}\\] Vabaliige näitab kategoriaalse tunnuse referentskategooria (antud juhul mees) keskmist sõltuva tunnuse väärtust. Aga kuna nüüd on meil mudelis ka sõltumatu pidevtunnus, siis on see referentskategooria keskmine juhul, kui sõltumatu pidevtunnus on \\(0\\). Ehk siis meie näite puhul tähistab vabaliige (\\(140.8\\)) meeste sissetulekut juhul kui nende matemaatilise kirjaoskuse skoor on \\(0\\). suguNaine regressioonikordaja näitab naiste sissetuleku erinevust meestest. See võtab arvesse ka matemaatilise kirjaoskuse skoori. Ehk siis kõikide matemaatilise kirjaoskuse väärtuste puhul on on naiste sissetulek \\(365\\) eurot meestest madalam (st soo mõju on kontrollitud matemaatilise kirjaoskuse suhtes). numeracy regressioonikordaja näitab jällegi sissetuleku muutust (\\(3.35\\)) kui matemaatiline kirjaoskus muutub ühe ühiku võrra. Kuna ka sugu on mudelis arvesse võetud, kehtib see muutus võrdselt nii naistele kui meestele (mõju on kontrollitud soo suhtes). Vaatame, milliseks kujunevad mudeli järgi meeste ja naiste keskmised sissetulekud, kui nende numeracy skoor on 300. \\[\\hat{y}_{sissetulek}=140.8+3.35 \\times numeracy + (-365) \\times naine\\] \\[\\begin{align} &amp;= \\begin{cases} 140.8+3.35 \\times 300 + (-365) \\times 1 &amp; \\quad \\text{naine}\\\\ 140.8+3.35 \\times 300 + (-365) \\times 0 &amp; \\quad \\text{mees} \\end{cases}\\\\ &amp;= \\begin{cases} (140.8-365)+3.35 \\times 300 &amp; \\quad \\text{naine}\\\\ 140.8+3.35 \\times 300 &amp; \\quad \\text{mees} \\end{cases}\\\\ &amp;= \\begin{cases} -224.2+1005 &amp; \\quad \\text{naine}\\\\ 140.8+1005 &amp; \\quad \\text{mees} \\end{cases}\\\\ &amp;= \\begin{cases} 780.8 &amp; \\quad \\text{naine}\\\\ 1145.8 &amp; \\quad \\text{mees} \\end{cases} \\end{align}\\] Mõnevõrra lihtsam on seda tulemust interpreteerida graafiliselt: piaac %&gt;% ggplot(aes(x = numeracy, y = sissetulek, color = sugu))+ geom_point(alpha = 0.1, size = 0.3)+ geom_abline(intercept = 140.8, slope = 3.35, color = &quot;#972D15&quot;)+ geom_abline(intercept = 140.8-365, slope = 3.35, color = &quot;#02401B&quot;)+ scale_colour_manual(values = c(&quot;Mees&quot; = &quot;#972D15&quot;, &quot;Naine&quot; = &quot;#02401B&quot;))+ theme_minimal()+ guides(color = guide_legend(override.aes = list(size = 2, alpha = 1))) Lihtsam võimalus seoseid graafiliselt esitada on kasutada paketist interactions funktsiooni interaction_plot(). See on küll mõeldud eelkõige koosmõjude plottimiseks, kuid toimib ka tavalisete seoste kujutamisel. library(interactions) interact_plot(mudel4, pred = numeracy, modx = sugu, colors = c(&quot;#972D15&quot;, &quot;#02401B&quot;)) 3.6.1 Kaks pidevat sõltumatut muutujat Vaatame ka olukorda, kus meil on kaks pidevat sõltumatut tunnust - matemaatiline kirjaoskus ja vanus: mudel5 &lt;- lm(sissetulek~numeracy+vanus, data = piaac) summary(mudel5) ## ## Call: ## lm(formula = sissetulek ~ numeracy + vanus, data = piaac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1036.6 -349.5 -128.9 178.7 2944.0 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.3372 67.3417 0.376 0.707 ## numeracy 3.4841 0.2033 17.137 &lt; 2e-16 *** ## vanus -3.2210 0.7138 -4.512 6.59e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 554.5 on 3981 degrees of freedom ## (3648 observations deleted due to missingness) ## Multiple R-squared: 0.07879, Adjusted R-squared: 0.07833 ## F-statistic: 170.3 on 2 and 3981 DF, p-value: &lt; 2.2e-16 Tõlgendame seda järmiselt: Kui matemaatiline kirjaoskus tõuseb ühe punkti võrra, siis sissetulek tõuseb \\(3.48\\) euro võrra, hoides vanust konstantsena (st see seos kehtib kõikide vanuste jaoks). Kui vanus tõuseb ühe aasta võrra, siis sissetulek langeb \\(3.2\\) euro võrra, hoides funktsionaalset lugemisoskust konstantsena (st see seos kehtib kogu funktsionaalse lugemisoskuse skaala ulatuses). Juhul kui nii vanus oleks \\(0\\) aastat ja matemaatiline kirjaoskus oleks \\(0\\) punkti, oleks sissetulek \\(25.3\\) eurot (kuna selline olukord on suhteliselt võimatu, siis me sellistel puhkudel vabaliiget ei interpreteeri). Et taolisest mudelist paremini aru saada võime kasutada 3D punktdiagrammi #library(car) #scatter3d(piaac$numeracy,piaac$sissetulek, piaac$vanus) Ülesanne! Looge regressioonimudel, millega hindate numeracy, vanus, sugu ja haridustase mõju sissetulekule. Milliste tunnuste mõju sissetulekule on statistiliselt oluline? Esitage vanuse ja soo mõju sissetulekule graafiliselt. 3.7 Koosmõjud Eelnevas näites vaatasime sissetuleku sõltuvust matemaatilise kirjaoskuse tasemest soo lõikes, ja nägime, et kui lisame mudelisse soo tunnuse, siis saame klasside kohta eraldi regressioonijooned. Kuid need regressioonijooned olid paralleelsed, mis tähendab et nii meeste kui naiste hulgas oli funktsionaalse lugemisoskuse ja matemaatilise kirjaoskuse suhe mudeli järgi sama. Kuid kas see on alati väga realistlik eeldus? Võib ju vabalt olla, et see seos erineb soo lõikes. Kui me arvame, et see võib nii olla, st sõltumatu tunnuse mõju sõltuvale tunnusele sõltub omakorda mingist muust tunnusest, saame mudelisse lisada nende kahe tunnuse koosmõju (interaktsiooni). Selleks peame moodustame uue tunnuse, mis tuleneb nende tunnuste, mille suhtes me koosmõju hinnata tahame, korrutisest. Kui me nüüd selle uue tunnuse mudelisse kaasame, siis hindame sellele ka regressioonikoefitsiendi. Regressioonivõrrand pidevtunnuse ja kategoriaalse tunnuse koosmõjuga näeks välja nii: \\[\\hat{y}_{sissetulek}=\\beta_0+\\beta_1 \\times numeracy + \\beta_2 \\times naine + \\beta_3 \\times naine \\times numeracy \\] \\[\\begin{align} &amp;= \\begin{cases} \\beta_0+\\beta_1 \\times numeracy + \\beta_2 \\times 1 + \\beta_3 \\times 1 \\times numeracy &amp; \\quad \\text{naised}\\\\ \\beta_0+\\beta_1 \\times numeracy + \\beta_2 \\times 0 + \\beta_3 \\times 0 \\times numeracy &amp; \\quad \\text{mehed} \\end{cases}\\\\ &amp;= \\begin{cases} (\\beta_0+\\beta_2)+(\\beta_1+\\beta_3) \\times numeracy &amp; \\quad \\text{naised}\\\\ \\beta_0+\\beta_1 \\times numeracy &amp; \\quad \\text{mehed} \\end{cases} \\end{align}\\] Mis siin nüüd siis toimub? Meeste keskmine sissetulek on, nagu varasemaltki, defineeritud vabaliikme ja \\(\\beta_1\\) regressioonikoefitsiendiga (ülejäänud kaks koefitsienti lähevad meeste jaoks \\(0\\)-ks, kuna naine tunnus on nende jaoks \\(0\\)). Naistel on aga lisaks veel kaks koefitsienti. \\(\\beta_2\\), mis nagu varasemaltki kirjeldab naiste vabaliikme erinevust meeste vabaliikmest, ning siis veel \\(\\beta_3\\), mis kirjeldab naiste regressioonisirge erinevust meeste regressioonisirgest. Koosmõjudega mudelis on naiste numeracy koefitsient \\(\\beta_1+\\beta_3\\) (sest \\(\\beta_1 \\times numeracy + \\beta_3 \\times numeracy = (\\beta_1+\\beta_3) \\times numeracy\\)). R-is saame taolise mudeli defineerida järgmiselt: mudel8 &lt;- lm(sissetulek ~ numeracy * sugu, data = piaac) summary(mudel8) ## ## Call: ## lm(formula = sissetulek ~ numeracy * sugu, data = piaac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1213.35 -322.79 -99.79 166.87 2802.32 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 28.4609 79.7161 0.357 0.7211 ## numeracy 3.7552 0.2815 13.340 &lt;2e-16 *** ## suguNaine -157.9875 107.6870 -1.467 0.1424 ## numeracy:suguNaine -0.7476 0.3840 -1.947 0.0516 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 525.8 on 3980 degrees of freedom ## (3648 observations deleted due to missingness) ## Multiple R-squared: 0.1718, Adjusted R-squared: 0.1712 ## F-statistic: 275.2 on 3 and 3980 DF, p-value: &lt; 2.2e-16 # Sama tulemuse saaksime, kui kirjutaksime: #lm(numeracy ~ literacy + sugu + literacy:sugu, data = piaac) Koosmõjudega mudeli vabaliikmed ja regressioonikoefitsiendid kujunevad järgmiselt: \\[\\hat{y}_{sissetulek}=\\beta_0+\\beta_1 \\times numeracy + \\beta_2 \\times naine + \\beta_3 \\times naine \\times numeracy\\] \\[\\begin{align} &amp;= \\begin{cases} 28.5+3.8 \\times numeracy + (-157) \\times 1 + (-0.7) \\times 1 \\times numeracy &amp; \\quad \\text{naised}\\\\ 28.5+3.8 \\times numeracy + (-157) \\times 0 + (-0.7) \\times 0 \\times numeracy &amp; \\quad \\text{mehed} \\end{cases}\\\\ &amp;= \\begin{cases} (28.5-157)+(3.8-0.7) \\times numeracy &amp; \\quad \\text{naised}\\\\ 28.5+3.8 \\times numeracy &amp; \\quad \\text{mehed} \\end{cases}\\\\ &amp;= \\begin{cases} -128.5+3.1 \\times numeracy &amp; \\quad \\text{naised}\\\\ 28.5+3.8 \\times numeracy &amp; \\quad \\text{mehed} \\end{cases} \\end{align}\\] Seega selles mudelis erinevad kategoriaalse tunnuse lõikes nii vabaliikme väärtused kui ka regressioonisirge tõusud. Kui me nüüd selle mudeli tulemused graafikule paneme, siis näeme, et regressioonisirged ei ole enam paralleelsed. Mida suurem on matemaatilise kirjaoskuse tase, seda suurem on erinevus meeste ja naiste sissetulekutes. interact_plot(mudel8, pred = numeracy, modx = sugu, colors = c(&quot;#972D15&quot;, &quot;#02401B&quot;)) 3.7.1 Koosmõjud kategoriaalsete tunnuste puhul Enne oli juttu, et kahe kategoriaalse sõltumatu tunnusega mudel ilma koosmõjudeta pole väga mõistlik. Vaatame nüüd kuidas see koosmõjudega välja näeks: mudel9 &lt;- lm(sissetulek ~ sugu * haridustase, data = piaac) summary(mudel9) ## ## Call: ## lm(formula = sissetulek ~ sugu * haridustase, data = piaac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1126.25 -311.40 -99.32 163.85 2639.98 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1004.53 17.95 55.967 &lt;2e-16 *** ## suguNaine -458.80 24.76 -18.533 &lt;2e-16 *** ## haridustaseKõrge 252.60 28.59 8.836 &lt;2e-16 *** ## haridustaseMadal -73.55 37.63 -1.955 0.0507 . ## suguNaine:haridustaseKõrge 50.69 36.65 1.383 0.1667 ## suguNaine:haridustaseMadal 10.76 56.52 0.190 0.8491 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 526.1 on 3978 degrees of freedom ## (3648 observations deleted due to missingness) ## Multiple R-squared: 0.1715, Adjusted R-squared: 0.1705 ## F-statistic: 164.7 on 5 and 3978 DF, p-value: &lt; 2.2e-16 Sellisest mudelist saame välja lugeda kõikide gruppide ristlõigete (kõrge haridustasemega naised, kõrge haridustasemega mehed jne) keskmised. Arvutame näiteks välja kõrge haridustasemega (koh) naiste ja madala haridustasemega (mh) meeste keskmised matemaatilise lugemisoskuse skoorid: \\[\\hat{y}_{sissetulek}=\\beta_0+\\beta_1 \\times naine + \\beta_2 \\times koh + \\beta_3 \\times mh + \\beta_4 \\times naine \\times koh + \\beta_5 \\times naine \\times mh\\] \\[\\begin{align} &amp;= \\begin{cases} 1004.5+(-458.8) \\times 1 + 252.60 \\times 1 + (-73.55) \\times 0 + 50.69 \\times 1 + 10.76 \\times 0 &amp; \\quad \\text{kõrge haridustasemega naised}\\\\ 1004.5+(-458.8) \\times 0 + 252.60 \\times 0 + (-73.55) \\times 1 + 50.69 \\times 0 + 10.76 \\times 0 &amp; \\quad \\text{madala haridustasemega mehed} \\end{cases}\\\\ &amp;= \\begin{cases} 1004.5+(-458.8) + 252.60 + 50.69 &amp; \\quad \\text{kõrge haridustasemega naised}\\\\ 1004.5 + (-73.55) &amp; \\quad \\text{madala haridustasemega mehed} \\end{cases}\\\\ &amp;= \\begin{cases} 849 &amp; \\quad \\text{kõrge haridustasemega naised}\\\\ 931 &amp; \\quad \\text{madala haridustasemega mehed} \\end{cases} \\end{align}\\] Vaatame seda mudelit ka graafiliselt (kasutame paketi interactions funktsiooni cat_plot()): cat_plot(mudel9, pred = haridustase, modx = sugu, colors = c(&quot;#972D15&quot;, &quot;#02401B&quot;)) Ülesanne! Looge koosmõjuga regressioonimudel, millega hindate soo ja laste olemasolu mõju sissetulekule. Esitage koosmõjud graafikul (cat_plot() abiga) 3.8 Mudelite võrdlemine Milline on hea mudel? See peaks muidugi seletama võimalikult palju sõltuva tunnuse varieeruvusest. Samas peaks see olema ka võimalikult ökonoomne, st see peaks sisaldama ainult tunnuseid, mis mudelit oluliselt paremaks teevad. Siin on rõhk sõnal oluliselt. Iga lisanduv tunnus teeb mudeli mingil määral paremaks, kuid see paranemine võib olla mikroskoopiline. Kuidas siis hinnata, kas mudel \\(n+1\\) tunnusega on oluliselt parem kui \\(n\\) tunnusega mudel? Me saame vaadata lisanduva tunnuse standardviga, t-väärtust ja sellega seonduvat p-väärtust. Kuid nagu enne jutuks oli, testib see ainult konkreetse koefitsiendi erinevust nullist. Meid aga huvitab kogu mudeli kvaliteet. Võimalus on ka võrrelda mudelite \\(R^2\\) väärtusi, kuid need on pigem kirjeldavad, ega anna meile indikatsiooni sellest kas üks väärtus on oluliselt parem kui teine. Erinevate mudelite statistiliselt olulist erinevust saame testida hii-ruut testiga kasutades anova() funktsiooni. Seda saab teha ainult siis kui mudelid on omavehl seotud (nested), st keerukam (rohkemate tunnustega) mudel peab sisdaldama kõiki lihtsama mudeli tunnuseid. mudel_test1 &lt;- lm(numeracy ~ literacy, data = piaac) mudel_test2 &lt;- lm(numeracy ~ literacy + sugu, data = piaac) anova(mudel_test1, mudel_test2, test = &quot;Chisq&quot;) ## Analysis of Variance Table ## ## Model 1: numeracy ~ literacy ## Model 2: numeracy ~ literacy + sugu ## Res.Df RSS Df Sum of Sq Pr(&gt;Chi) ## 1 7584 4848657 ## 2 7583 4734885 1 113772 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Tõlgendame jällegi testi p-väärtust. Kui see on väiksem kui \\(0.05\\) (usaldusnivool \\(95\\%\\)), siis võime järeldada, et mudelid on oluliselt erinevad, mis tähendab omakorda, et lisatud tunnus tõstis mudeli selgitusvõimet olulisel määral. 3.9 Regressioonimudeli eeldused Nagu iga meetodi puhul, on ka lineaarsel regressioonanalüüsil rida eeldusi, mis peavad olema täidetud, et analüüsist korrektseid järeldusi oleks võimalik teha. Esimene ja vahest ka kõige olulisem eeldus on lineaarne suhe sõltuva ja sõltumatu(te) tunnuse vahel. Kõrvaloleval joonisel on esitatud neli andmestikku, mille regressioonisirged on identsed (\\(y=3+0.5x\\)). Tegelikult on identsed ka kõik muud andmete statistilised omadused (\\(x\\)i keskmine, \\(y\\)i keskmine, \\(x\\)i dispersioon, \\(y\\)i dispersioon ja ka korrelatsioon). Ometi on visuaalselt näha, et kõik andmestikud on väga erinevad. Seega peaks regressioonanalüüsi (või tegelikult ükskõik mis analüüsi) puhul olema alati esimene samm neid graafiliselt uurida. Kui tunnuste vaheline seos ei ole lineaarne, piisab mõnel juhul tunnuste mittelineaarsest transformeerimisest (see peaks olema ka muidugi teoreetiliselt põhjendatud). Kui seos on eksponentsiaalne, siis võib kaaluda log-transformatsiooni. Kui seos on paraboolne, siis võib kaaluda ruutu tõstetud tunnuse lisamist (\\(y = \\beta_0+\\beta_1x+\\beta_2x^2\\)). Taoliste transformatsioonide juures peab meeles pidama, et koos nendega muutub ka mudeli tõlgendus. Joonis 3.4: Anscombe kvartett Lineaarse regressiooni puhul peaks tähelepanelik olema ka erindite (outliers)suhtes, st vaatluste suhtes, mis erinevad teistest väga olulisel määral (nagu ka kõrvalolevalt jooniselt näha). Mõnede andmete puhul on erindid paratamatud (näiteks sissetuleku puhul, kus suurem osa inimesi on koondunud keskmise sissetuleku ümber, kuid mõned üksikud teenivad sellest oluliselt enam). Sellisel juhul tasuks kaaluda jällegi tunnuse transformeerimist (sissetuleku puhul näiteks log-skaalale). Kui tegemist on mõne üksiku erindiga, võiks ju selle aluseks oleva vaatluse ka lihtsalt analüüsist välja jätta. Siin tuleks aga olla väga ettevaatlik. Andmete või sellest saadava informatsiooni tahtlik vähendamine (näiteks pidevtunnuste kategoriseerimine) ei ole üldiselt kunagi hea mõte. Seda enam ei ole hea mõte andmete vähendamine eesmärgiga mudelit paremaks teha. Kui aga erindite tekkimine on mingil moel teoreetiliselt seletatav või tulenenud näiteks veast andmekorjel, siis võib seda loomulikult teha. Jääkide dispersiooni homogeensus (homoscedasticity). Jäägid peaksid hinnatud väärtuste lõikes olema homogeense ja konstantse variatiivsusega, st ühtlaselt jaotunud kõikide \\(\\hat{y}\\) väärtuste ümber. Selle eelduse rikkumine mõjutab eelkõige standardvigu (need ei kehti enam kõikidele \\(\\hat{y}\\) väärtustele ühtlaselt) ja seeläbi loomulikult ka usaldusintervalle ning p-väärtusi. Lahenduseks võivad olla nn robustsed standardvead (robust standard errors), mis võtavad varieeruvuse erinevust arvesse. ## Robust standard errors mudel6 &lt;- lm(numeracy ~ literacy * sugu, data = piaac) # Tavalised standardvead summary(mudel6) ## ## Call: ## lm(formula = numeracy ~ literacy * sugu, data = piaac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -102.441 -16.551 -0.094 16.867 88.714 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 36.129029 2.620050 13.789 &lt; 2e-16 *** ## literacy 0.871523 0.009417 92.543 &lt; 2e-16 *** ## suguNaine 4.596729 3.619797 1.270 0.204164 ## literacy:suguNaine -0.044937 0.012972 -3.464 0.000535 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 24.97 on 7582 degrees of freedom ## (46 observations deleted due to missingness) ## Multiple R-squared: 0.6948, Adjusted R-squared: 0.6946 ## F-statistic: 5753 on 3 and 7582 DF, p-value: &lt; 2.2e-16 # Robustsed standardvead library(sandwich) library(lmtest) coeftest(mudel6, vcov. = vcovHC(mudel6)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 36.1290294 2.7887350 12.9553 &lt; 2.2e-16 *** ## literacy 0.8715229 0.0098976 88.0538 &lt; 2.2e-16 *** ## suguNaine 4.5967288 3.8313081 1.1998 0.2302621 ## literacy:suguNaine -0.0449372 0.0135552 -3.3151 0.0009203 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Saab ka nii library(sandwich) vcovHC(mudel6) %&gt;% diag() %&gt;% sqrt() ## (Intercept) literacy suguNaine literacy:suguNaine ## 2.788735004 0.009897618 3.831308067 0.013555193 Jääkide normaaljaotus. Regressiooni jäägid peaksid olema normaaljaotusega \\(e_i \\sim N(0, \\sigma^2)\\), seega enamus jääke peaks jääma nulli ümber ning mida suuremad jäägid, seda vähem neid olema peaks. See eeldus on eelkõige oluline regressioonikoefitsientide t-testi jaoks. Jääkide sõltumatus. Ühe vaatluse jäägid ei tohiks olla korreleeritud teise vaatluse jääkidega. Selline olukord võib tekkida näiteks siis kui meil mudelist välja jäänud mingi oluline tunnus (ühe tunnuse regressiooni puhul on see muidugi vaid hüpoteetiline olukord), näiteks hindame õpilaste testiskoore lähtuvalt nende õppimisele kulunud ajast, kuid ei arvesta, et õpilased tulevad näiteks erinevatest koolidest, kus võib olla erinev tase. Seega õpilaste tulemused ei ole enam sõltumatud, vaid sõltuvad koolist. Regressioonikoefitsientide standardvigade arvutamisel lähtutakse eeldusest, et jäägid on sõltumatud. Kui jäägid on korreleeritud, siis võib juhtuda, et me alahindame standardvigade suurust ehk siis oleme oma tulemustes ülemäära kindlad (usaldusintervallid ning p-väärtused tulevad liialt väikesed) ning võime näha seoseid seal kus neid tegelikult ei ole. Lahenduseks võiks olla puuduolevate tunnuste lisamine mudelisse (konkrteetse näite puhul kooli tunnus). Kui kaks sõltumatut tunnust on teineteisega väga tugevalt seotud põhjusteab see nn kollineaarsust. See võib tekitada probleeme mudeli hindamisel ning ka tõlgendusel. Lisaks kipuvad standardvead liialt suureks minema, mis tähendab seda, et kaotame oma tulemuste täpsuses ja võime mitte näha seoseid seal, kus need tegelikult olemas on. Seega võiks tähele panna, et korrelatsioon sõltumatute muutujate vahel peaks alati olema väiksem kui korrelatsioon sõltuva ja sõltumatu muutuja vahel. 3.10 Kuidas eelduste täidetust hinnata? Eelduste hindamiseks on loomulikult mitmeid teste, kuid kõige lihtsam on seda mudeli diagnostiliste joonistega. mod &lt;- lm(formula = sissetulek ~ numeracy + vanus + sugu + haridustase, data = piaac) plot(mod, 1) Kontrollime mittelineaarse seose olemasolu. Punktid peaksid olema ühtlaselt ümber keskmise joone jaotunud. Ei tohiks olla mingit ilmset mustrit. plot(mod, 2) Kas jäägid on normaalselt jaotunud? Punktid peaksid ühtima diagonaalse joonega. plot(mod, 3) Kas jääkide dispersioon on homogeenne? Punane joon peaks olema horisontaalne ja punktid peaksid olema ühtlaselt jaotunud ega tohiks mingit mustrit moodustada. plot(mod, 5) Kas mudelis on mudelit oluliselt mõjutavaid erindeid? Kui on, siis peaksid need olema paremal all või paremal üleval nurgas ning kaugemal kui punktiirjoon (antud juhul neid ei ole ja seega ei ole ka punktiirjoont näha). Inglisekeelses terminoloogias kasutatakse sõltuva tunnuse puhul peale dependent variable ka nimetusi response või outcome variable ja sõltumatu tunnuse puhul peale independent variable ka predictor või explanatory variable. Prediktor on ka eesti keeles kasutusel. Defaultis annab geom_smooth meile mittelineaarse regressioonijoone (vastavalt sellele palju vaatlusi on, kas gam või loess), mis üritab tunnustevahelist suhet andmete kõikides punktides võimalikult täpselt kirjeldada. Hiljem, kui meil on mitu sõltumatut tunnust, eristame tunnused plussiga: sõltuv_tunnus ~ sõltumatu_tunnus_1 + sõltumatu_tunnus_2 + ... Tegelikult ei ole selline mudel korrektne. Sissetuleku jaotus ei vasta hästi regressiooni nõuetele. Miks ei vasta ja kuidas see vastama panna, sellest natuke hiljem. Kuid hetkel kasutame seda puhtalt didaktilistest kaalutustest lähtuvalt. Mida saab väljendada kui \\(\\epsilon_i=y_i-\\hat{y}_i\\) \\(RSS= = e_1^2 + e_2^2 + ... + e_n = \\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\\) Korrektse valimi võtmise all peame siinkohal silmas eelkõige juhuvalikut. Kõikidel populatsiooni liikmetel/elementidel peab olema võrdne võimalus valimisse sattuda. Kui üldpopulatsiooniks on Eesti elanikkond, aga valimisse võtaksime ainult Tallinna elanikud, siis antud valimi põhjal tehtavad järeldused ei oleks kuidagi üldistatavad kõigile Eesti elanikele, vaid ikkagi ainult tallinnlastele. Lisaks juhuvalimile on veel terve rida spetsiifilisemaid valimidisaine (stratifitseeritud valim, klastervalim jne) mida me hetkel ei käsitle. Kuid tuleb meeles pidada, et keerulisemate valimidisainide puhul tuleb hilisemas analüüsis ja järelduste tegemise käigus valimi moodustamise loogikat arvesse võtta. Ka summary() ei anna välja kogu mudeliobjektis sisalduvat infot. Et näha mida mudeliobjekt veel sisaldab, võib kasutada str(mudeliobjekt) käsku. \\(MSE=\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2}{n-2}\\) Natuke täpsemalt väljendades \\(F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\\), kus \\(n\\) on valimi suurus ja \\(p\\) on regressioonikoefitsientide (sõltumatute muutujate) arv. \\((t^{*}_{(n-2)})^2=F^{*}_{(1,n-2)}\\) Sellist mudelit nimetatakse ka ANOVA-ks või täpsemlat One-Way ANOVA-ks (kuna tegemist on ainult ühe kategoriaalse sõltumatu muutujaga) "],["logistiline-regressioon.html", "Peatükk 4 Logistiline regressioon 4.1 ansid 4.2 Logit 4.3 Logit mudel 4.4 Mudeli tõlgendus 4.5 Logistiline regressioon R-is 4.6 Mudeli kvaliteet 4.7 Predict 4.8 Marginaalsed efektid 4.9 Prognoosi täpsus", " Peatükk 4 Logistiline regressioon Logistilise regressiooniga (logit-mudeliga) saame hinnata sõltumatute tunnuste mõju binaarsele sõltuvale tunnusele (töötav/töötu, käis valimas/ei käinud valimas, surnud/ei ole surnud). Teisisõnu, hindame tõenäosust mingi sündmuse toimumiseks (success/failure). Sõltuva tunnuse \\(y\\) jaotus on määratletud kui sündmuse toimumise tõenäosus \\(P(Y=1)=\\pi\\). Tavalise regressiooni mudel oli väljendatav kui \\(\\bar{y}=\\beta_0+\\beta_p x_p\\). Miks me ei võiks pidevtunnuselise \\(y\\) keskmist asendada \\(\\pi\\)ga: \\(\\bar{\\pi}=\\beta_0+\\beta_k x_k\\)? Aga sellepärast, et tõenäosus on piiritletud \\(0\\) ja \\(1\\)ga, samas kui lineaarne funktsioon hõlmab kõiki reaalarvulisi väärtusi. Seega on ülimalt tõenäoline, et mingite \\(x\\)i väärtuste puhul on prognoositav \\(y\\) väärtus suurem kui \\(1\\) või väiksem kui \\(0\\). Lisaks tekivad probleemid jääkide struktuuriga (tavaline regressioon eeldab normaaljaotust) ja jääkide dispersiooniga (tavaline regressioon eeldab konstantset hajuvust). Joonis 4.1: Lineaarse regressiooni kasutamine binaarse sõltuva tunnusega 4.1 ansid Kuidas me saaksime tõenäosuse skaala (\\(0 \\dots1\\)) teisendada pidevaks skaalaks (\\(-\\infty \\dots \\infty\\))? Et saada lahti maksimaalsest väärtusest (\\(1\\)), on võimalik kasutada sündmuse toimumise tõenäosuse asemel sündmuse toimumise anssi (odds). anssideks nimetatakse sündmuse toimumise ja mittetoimumise suhet: \\[\\text{ansid}=\\frac{p}{(1-p)}\\] Näiteks kulli ja kirja viskamisel on kulli saamise anss \\(\\frac{0.5}{(1-0.5)}=1\\). anss võtta kaardipakist ruutu on \\(\\frac{0.25}{(1-0.25)}=\\frac{1}{3}=0.33\\). Joonis 4.2: ansside ja tõenäosuse suhe ansid saab omakorda teisenda tagasi tõenäosuseks: \\[p=\\frac{\\text{ansid}}{1+\\text{ansid}}\\] 4.2 Logit Kuid tõenäosuse alumine piir jääb sel juhul ikkagi ette. Ka ansid on altpoolt piiratud (nad ei saa olla väiksemad kui \\(0\\)). Lahenduseks on võtta ansside logaritm. Saadud väärtust nimetatakse logit-iks (log odds): \\[\\text{logit}=\\log \\bigg(\\frac{p}{(1-p)}\\bigg)\\] Joonis 4.3: Logit-i ja tõenäosuse suhe 4.3 Logit mudel Lõpuks saame mudeli võrrandi kokku panna: \\[\\text{logit}(\\pi_i)=\\text{log} \\left(\\dfrac{\\pi_i}{1-\\pi_i}\\right)=\\beta_0+\\beta_1 x_i\\] Või kui võtame mõlemast poolest eksponendi: \\[\\frac{\\pi_i}{1-\\pi_i}=e^{({\\beta_0}+\\beta_1 x_i)}\\] Sama võrrandit saab esitada ka nii: \\[\\pi_i=Pr(Y_i=1|X_i=x_i)=\\dfrac{e^{(\\beta_0+\\beta_1 x_i)}}{1+e^{(\\beta_0+\\beta_1 x_i)}}\\] või hoopis nii: \\[\\pi_i=Pr(Y_i=1|X_i=x_i)=\\frac{1}{1+e^{-\\beta_0-\\beta_1 x_i}}\\] Joonis 4.4: Logistiline regressioon võrdluses lineaarse regressiooniga 4.4 Mudeli tõlgendus Tavalise regresioonimudeliga saime prognoosida \\(y\\) väärtust mingite \\(x\\) väärtuste korral (ja \\(y\\) muutust, kui \\(x\\) muutub ühe ühiku võrra). Sama kehtib ka logistilise regressiooni korral. Kuid mida me siinjuures täpsemalt prognoosime? Tahaksime kindlasti prognoosida (uuritava sündmuse toimumise) tõenäosust. Kuid kuna me teisendasime tõenäosuse logititeks, siis tegelikult saame prognoosida hoopis logitit. Ja ka ühe ühikuline muutus \\(x\\)-is ei peegelda mitte \\(y\\) tõenäosuse muutust, vaid muutust logit(\\(y\\))-is. Logiteid ei oska me (vähemalt esialgu) kuidagi tõenäosuslikult tõlgendada. Mida siis teha? Lahenduseks on võtta logit-i võrrandi mõlemast poolest eksponent \\(exp(logit) = exp(\\beta_0+\\beta_1 x_i) \\implies \\frac{\\pi_i}{1-\\pi_i}=e^{({\\beta_0}+\\beta_1 x_i)}\\). Sellisel juhul saab \\(y\\)-t tõlgendada kui ansse ja \\(\\beta\\)-t kui muutust anssides (mitu korda \\(x\\)-i ühe ühiku muutudes \\(y\\) ansid suurenevad või vähenevad). Seda ansside muutust väljendavat kordajat nimetatakse ansside suhteks. 4.4.1 ansside suhe ansid saime leida valemiga: \\[\\text{ansid}=\\frac{p}{(1-p)}\\] Valemist võime välja lugeda järgmist: ansid on alati positiivsed Kui ansid on \\(1\\), siis on sündmuse toimumise ja mittetoimumise tõenäosus võrdsed (\\(p=0.5\\)). Kui ansid on suuremad kui \\(1\\), siis on sündmuse toimumise tõenäosus suurem kui mittetoimumise tõenäosus (\\(p&gt;0.5\\)) ja vastupidi. Näiteks kui sündmuse toimumise tõenäosus on \\(0.8\\), siis on ansid \\(\\frac{0.8}{1-0.8}=\\frac{0.8}{0.2}=4\\). Seega sündmuse toimumise tõenäosus on \\(4\\) korda suurem kui selle mittetoimumise tõenäosus. Kui sündmuse toimumise tõenäosus on \\(0.2\\), siis on ansid \\(\\frac{0.2}{1-0.2}=\\frac{0.2}{0.8}=\\frac{1}{4}=0.25\\). Sündmuse toimumise tõenäosus on \\(4\\) korda väiksem kui selle mittetoimumise tõenäosus. Vaatame näidet, kus hindame hääletamise tõenäosust ning abielu mõju sellele: Hääletab Ei hääleta Abielus 0.75 0.25 Ei ole abielus 0.54 0.46 Abielus inimeste puhul on anss hääletamiseks \\(\\frac{0,75}{0,25} = \\frac{3}{1}= 3\\) (iga mittehääletaja kohta on kolm hääletajat). Vallaliste puhul on anss hääletamiseks \\(\\frac{0,54}{0,46} = 1,17\\) (iga mittehääletaja kohta on \\(1,17\\) hääletajat). Meid huvitab kuidas sõltumatu tunnuse muutus sündmuse toimumise ansse mõjutab, ehk kui palju muutuvad ansid kui sõltumatu tunnus muutub ühe ühiku võrra. Seda muutust väljendabki ansside suhe (odds ratio ehk OR) \\[OR=\\frac{y \\text{ anss juhul kui } x \\text{ väärtus muutub ühe ühiku võrra}}{y \\text{ anss juhul kui } x \\text{ väärtus jääb samaks}}\\] Kui palju on abielus olijate ansid hääletamiseks suuremad kui vallalistel? ansside suhe on \\(\\frac{3}{1,17}=2,56\\). Ehk siis abielus olijate anss hääletada on kaks ja pool korda suurem. Abielu tunnuse ühe ühiku muutumisega muutuvad ansid \\(2,56\\) korda ehk suurenevad \\(156\\%\\). 4.5 Logistiline regressioon R-is Võtame R-i näidisandmestiku Titanic, mis kirjeldab Titanicul hukkunute ja ellujäänute sugu, vanust ja reisijaklassi. Üritame hinnata kuidas ja kas need tunnused mõjutasid ellujäämist. GLMi mudeleid saab Ris defineerida glm() funktsiooniga. Selle loogika ja argumendid on sarnased lm() funktsiooni omadele. Peamiseks erinevuseks on see, et nüüd peame defineerima ka sõltuva tunnuse jaotuse ja linkfunktsiooni. See käib argumendiga family. Logistilise regressiooni jaoks peame defineerima family = binomial(link = 'logit') (sõltuva tunnuse jaotus on binoomjaotus ja linkfunktsioon on logit). Vaatame kõigepealt soo mõju: # Andmestik on algselt tabeli kujul. # Saaksime seda ka sellisel kujul analüüsida, # kuid mugavam ja selgem on, kui keerame ta # nn tavalisele kujule. Kasutame selleks # paketi tidyr funktsiooni uncount() titanic &lt;- datasets::Titanic %&gt;% as.data.frame() %&gt;% tidyr::uncount(Freq) # vaatame andmestiku esimesi ridu head(titanic) ## Class Sex Age Survived ## 1 3rd Male Child No ## 2 3rd Male Child No ## 3 3rd Male Child No ## 4 3rd Male Child No ## 5 3rd Male Child No ## 6 3rd Male Child No # Defineerime mudeli mudel7 &lt;- glm(I(Survived == &quot;Yes&quot;)~Sex, data = titanic, family = binomial()) summary(mudel7) ## ## Call: ## glm(formula = I(Survived == &quot;Yes&quot;) ~ Sex, family = binomial(), ## data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6226 -0.6903 -0.6903 0.7901 1.7613 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.3128 0.0588 -22.32 &lt;2e-16 *** ## SexFemale 2.3172 0.1196 19.38 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2769.5 on 2200 degrees of freedom ## Residual deviance: 2335.0 on 2199 degrees of freedom ## AIC: 2339 ## ## Number of Fisher Scoring iterations: 4 # I(Survived == &quot;Yes&quot;) notatsiooniga saame tekstilise tunnuse # teisendada loogilisek tunnuseks # saaksime seda teha ka näiteks nii: # titanic$surv &lt;- titanic$Survived == &quot;Yes&quot; # glm(surv~Sex, data = titanic, family = binomial()) # Tulemus on sama Väljundist leiame kõigepealt regressioonikoefitsiendid, nende standardvead, z-väärtused ja z-testi p-väärtuse13. Kuid koefitsiendid on nüüd logititskaalal ja seepärast küllaltki raskesti tõlgendatavad. Saame siiski järeldada, et naiste tõenäosus ellu jääda oli suurem kui meestel (koefitsient on positiivne). Mõnevõrra lihtsam on tõlgendada ansside suhet. Selleks peame koefitsientidest eksponendi võtma: exp(coef(mudel7)) ## (Intercept) SexFemale ## 0.2690616 10.1469660 Vabaliiget tõlgendame kui referentsgrupi (antud juhul meeste) ansse ellu jääda. Seega mehe anss Titanicul ellu jääda oli 0.26, ehk siis iga hukkunud mehe kohta jäi ellu 0.26 meest, või vastupidi \\(1 / 0.269 = 3.7\\), iga ellujäänud mehe kohta hukkus 3.7 meest. Saame välja arvutada ka meeste ellujaamise tõenäosuse: \\[\\pi=\\frac{\\text{ansid}}{1+\\text{ansid}} = \\frac{0.269}{1+0.269} = 0.21\\] Naiste puhul tõlgendame ansside suhet. Ehk kui palju muudab naiseksolemine võrreldes meestega ellujäämise ansse. Tuleb välja, et ligi 10 korda. Seega naiste ansid ellu jääda olid \\(10.147 \\times 0.269 = 2.73\\). Iga hukkunud naise kohta jäi 2.7 naist ellu. Naiste ellujäämise tõenäosus oli: \\[\\pi=\\frac{\\text{ansid}}{1+\\text{ansid}} = \\frac{2.73}{1+2.73} = 0.73\\] Saame selle tõenäosuse ka otse välja arvutada, kui paneme koefitsiendid regressioonivõrrandisse (eelnevalt toodud valemi järgi): \\[\\pi=\\dfrac{e^{(\\beta_0+\\beta_1 x_i)}}{1+e^{(\\beta_0+\\beta_1 x_i)}} = \\dfrac{e^{(-1.313+2.317 \\times 1)}}{1+e^{(-1.313+2.317 \\times 1)}} = 0.73\\] Vaatme ka, kuidas muudab ellujäämise tõenäosust lisaks soole vanus (Age on siin kategoriaalne tunnus kategooriatega Child ja Adult). Eeldame ka soo ja vanuse koosmõju: mudel8 &lt;- glm(I(Survived == &quot;Yes&quot;)~Sex*Age, data = titanic, family = binomial()) summary(mudel8) ## ## Call: ## glm(formula = I(Survived == &quot;Yes&quot;) ~ Sex * Age, family = binomial(), ## data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6497 -0.6732 -0.6732 0.7699 1.7865 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.1881 0.2511 -0.749 0.4539 ## SexFemale 0.6870 0.3970 1.731 0.0835 . ## AgeAdult -1.1811 0.2584 -4.571 4.86e-06 *** ## SexFemale:AgeAdult 1.7465 0.4167 4.191 2.77e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2769.5 on 2200 degrees of freedom ## Residual deviance: 2312.8 on 2197 degrees of freedom ## AIC: 2320.8 ## ## Number of Fisher Scoring iterations: 4 Täiskasvanuks olemine mõnevõrra langetab ellujäämise tõenäosust, kuid seda ainult meeste puhul (soo ja vanuse interaktsioon on positiivne). Tulemuste tõlgendamiseks võtame jälle koefitsientidest eksponendi: exp(coef(mudel8)) ## (Intercept) SexFemale AgeAdult SexFemale:AgeAdult ## 0.8285714 1.9878296 0.3069458 5.7344228 Vabaliige kirjeldab ellujäämise ansse juhul kui sõltumatud tunnused on nullid. Ehk siis antud juhul ellujäämise ansse referentsgruppide kombinatsiooni puhul (lastest mehed ehk poisid). Seega poiste ellujäämise tõenäosus oli: \\[\\pi = \\frac{0.83}{1+0.83} = 0.45\\] Tüdrukute (lastest naised) ellujäämise hansid olid ca kaks korda (1.99) suuremad kui poistel (tõenäosus \\(\\frac{0.83\\times1.99}{1+(0.83\\times1.99)} = 0.62\\)). Täiskasvanud meeste ansid olid \\(0.3\\times0.83 = 0.24\\) ja seega tõenäosus \\(\\frac{0.24}{1+0.24} = 0.19\\). Täiskasvanud naiste puhul peame appi võtma koosmõju koefitsiendi. Täiskavanud naiste ansid moodustuvad \\(0.83\\times1.99\\times0.3\\times5.7 = 2.8\\). Tõenäosusena teeb see \\(0.74\\). Näeme, et koosmõju on antud mudeli puhul vägagi sisukas. Meeste puhul täiskavanuks olemine langetas ellujäämise ansse, naiste puhul aga tõstis. Ülesanne! Piaaci andmestikus on tunnus staatus3. Võtke see aluseks ja tehke uus loogiline (TRUE/FALSE) tunnus hoiv, mis kirjeldaks kas inimene on või ei ole hõivatud. Hinneke logistilise regressiooniga, kas hõivatus on mõjutatud inimese haridusest ja vanusest. 4.6 Mudeli kvaliteet Kuidas hinnata mudeli kvaliteeti? Meile ei anta ei jääkide standardviga ega determinatsioonikordajat. Küll on aga väljunis toodud Null deviance ja Residual deviance. Deviance kirjeldab mudeli hälvet ehk seda kui hästi (või õigem oleks öelda kui halvasti) meie mudel andmetega sobitub. Mida väiksem on deviance, seda paremini mudel andmetes leiduvaid seoseid peegeldab. Null deviance kirjeldab hälbimust nullmudelis, ehk ainult vabaliikmega mudelis (ainult keskmisega mudelis), ning Residual deviance hälbimust sõltumatute tunnustega mudelis. 4.6.1 Mudeli sobivus Mudeli sobivust andmetega (goodness of fit) saame hinnata jääkhälbimuse (Residual deviance) näitaja abil. Jääkhälbimus näitab kui palju mudeliga hinnatud \\(Y\\) väärtused empiirilistest \\(Y\\) väärtustest erinevad (analoogne asi lineaarse regressiooni puhul oli residual sum of squares). Jääkhälbimuse abil saame võrrelda kui palju meie sobitatud mudel erineb küllastunud (saturated) mudelist, st mudelist mis sobituks täiel määral andmetega ehk kus jääkälbimus oleks \\(0\\). Jääkhälbimus näitabki sisuliselt meie mudeli ja küllastunud mudeli erinevust. Juhul kui mudel on andmetega sobiv, siis peaks jääkhälbimus olema võimalikult väike. Seda, kas see on piisavalt väike, saame testida hi-ruut testiga (arvestades mudeli vabadusasteid (degrees of freedom). res_dev &lt;- deviance(mudel8) res_df &lt;- df.residual(mudel8) pchisq(res_dev, res_df, lower.tail = F) ## [1] 0.04209986 pchisq() funktsiooniga saame testitulemusele ka p-väärtuse. Näeme, et see on väiksem kui \\(0.05\\), mis tähendab, et meie mudel ei sobitu andmetega väga hästi (siin tahame, et p-väärtus oleks võimalikult suur). Reaaleluliste andmetega ongi tegelikult väga keeruline hästi sobituvat mudelit leida. Seega üldjuhul me lihtsalt lepime, et meie mudel ei ole täiuslik ja jätame selle testi tähelepanuta. 4.6.2 Mudeli statistiline olulisus Näeme, et sisuka mudeli hälve on võrreldes nullmudeliga tunduvalt väiksem14. See tähendab, et tänu sõltumatutele tunnustele suudame me sõltuva tunnuse variatsiooni seletada paremini kui ainult keskmise abil. Aga kas mudeli hälve läks väiksemaks piisavalt paju, et me saaksime selle kohta ka statistiliselt olulisi järeldusi teha? Ehk siis kas me saame järeldada, et sõltumatud tunnused seletavad statistiliselt olulisel määral sõltuva tunnuse variatsiooni ja meie mudel on parem kui lihtsalt sõltuva tunnuse keskmine? Saame seda testida likelihood ratio testiga. Arvutame esmalt hälvete erinevuse: dev_vahe &lt;- mudel8$null.deviance - mudel8$deviance dev_vahe ## [1] 456.6809 Ja ka vabadusasteme erinevuse: df_vahe &lt;- mudel8$df.null-mudel8$df.residual df_vahe ## [1] 3 Hälvete vahe on jaotunud hii-ruut jaotuse alusel, seega saame hii-ruut jaotuse alusel määrata selle olulisust. Arvutame hälvete vahele olulisustõenäosuse. Kasutame selleks hii-ruut jaotuse funktsiooni pchisq(), mis tahab sisendina teatstatisikut (hälvete vahe) ja vabadusastemeid (vabadusasteme vahe). Samuti peame ütlema, et meid huvitab jaotuse parempoolse saba alla jääv tõenäosus. pchisq(dev_vahe, df_vahe, lower.tail = F) ## [1] 1.163316e-98 Võime kasutada ka anova() funktsiooni, kus võrdleme kahte mudelt: # kasutame update() funktsiooni, millega # uuendame oma mudelit nii, et selle prediktoriks # oleks ainult vabaliige (tähistatud ~1) anova(mudel8, update(mudel8, ~1), test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: I(Survived == &quot;Yes&quot;) ~ Sex * Age ## Model 2: I(Survived == &quot;Yes&quot;) ~ 1 ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 2197 2312.8 ## 2 2200 2769.5 -3 -456.68 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Või kasutame lmtest paketi lrtest() funtsiooni: library(lmtest) lrtest(mudel8) ## Likelihood ratio test ## ## Model 1: I(Survived == &quot;Yes&quot;) ~ Sex * Age ## Model 2: I(Survived == &quot;Yes&quot;) ~ 1 ## #Df LogLik Df Chisq Pr(&gt;Chisq) ## 1 4 -1156.4 ## 2 1 -1384.7 -3 456.68 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Kõikide eelnevate testide puhul huvitab meid eelkõige p väärtus. Kui see on piisavalt väike (näiteks väiksem kui \\(0,05\\)), siis saame järeldada, et meie testitavad mudelid on piisavalt erinevad ehk siis sõltumatute tunnuste lisamine vähendas deviancei olulisel määral. Eelnevas näites on p- väärtus on väga väike, seega meie mudel on võrreldes nullmudeliga oluliselt parem. Sama loogikaga saame ka testida kas uue sõltumatu tunnuse lisamine teeb mudeli oluliselt paremaks. Lisaks saame anova() funktsiooniga testida kui palju iga sõltumatu tunnus mudelit paremaks tegi ja kas see paranemine oli statistiliselt oluline: anova(mudel8, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: I(Survived == &quot;Yes&quot;) ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 2200 2769.5 ## Sex 1 434.47 2199 2335.0 &lt; 2.2e-16 *** ## Age 1 5.89 2198 2329.1 0.0152 * ## Sex:Age 1 16.32 2197 2312.8 5.352e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.6.3 Pseudo-\\(R^2\\) Kui tavalise regressiooni puhul hindasime mudeli sobivust andmetega determinatsioonikordaja (\\(R^2\\)) abil, siis GLM-ide puhul vastavat näitajat ei ole. Küll on aga nn pseudo-\\(R^2\\) statistikud, mida võib analoogsel viisil kasutada (need ei näita küll päris sama asja). Üheks selliseks on näiteks Mcfaddeni \\(R^2\\): library(pscl) titanic$surv &lt;- titanic$Survived == &quot;Yes&quot; mudel_r2 &lt;- glm(surv~Sex*Age, data = titanic, family = binomial()) pR2(mudel_r2) ## fitting null model for pseudo-r2 ## llh llhNull G2 McFadden r2ML ## -1156.3879160 -1384.7283644 456.6808969 0.1648991 0.1873769 ## r2CU ## 0.2617525 4.7 Predict Sageli tahame oma mudeli alusel prognoosida mingitele kindlatele sõltumatute tunnuste väärtustele sõltuva tunnuse hinnanguid. Saame loomulikult need sõltumatute tunnuste väärtused regressioonivõrrandisse sisse panna ja hinnangu käsitsi välja arvutada. Aga on ka mugavam variant. Nimelt predict() funktsioon15. predict() vajab sisendiks mudelit ning referentsandmestikku vajalike sõltumatute tunnuste kategooriate kombinatsioonidega. Referentsandmestiku saame valmis teha käsitsi või kasutada näiteks expand.grid() funktsiooni. Tahame teada titanic andmestiku põhjal täiskasvanud meeste tõenäosust ellu jääda: # Teeme referentsandmestiku ref_data &lt;- data.frame(Sex = &quot;Male&quot;, Age = &quot;Adult&quot;) # Kasutame predict() funktsiooni ja lisame referentsandmestikule # pred tunnuse, millesse kirjutame prognoosi # Kuna tegemist on logit mudeliga, siis defaultis # prognoosib predict() logiteid Kui tahame teada # tõenäosusi, siis peame määrama type = &#39;response&#39; ref_data$pred &lt;- predict(mudel8, newdata = ref_data, type = &quot;response&quot;) ref_data ## Sex Age pred ## 1 Male Adult 0.2027594 Kui tahame prognoosi rohkematele kategooriate kombinatsioonidele, saame kasutada expand.grid() funktsiooni: # Teeme kõigepealt uue andmestiku, kus on sees kõik # tunnuse ja väärtused, mille kohta predictioni tahame ndata &lt;- expand.grid(Sex = c(&quot;Male&quot;, &quot;Female&quot;), Age = c(&quot;Adult&quot;, &quot;Child&quot;)) # Lisame andmestikule predictioni ndata$pred &lt;- predict(mudel8, newdata = ndata, type = &quot;response&quot;) ndata ## Sex Age pred ## 1 Male Adult 0.2027594 ## 2 Female Adult 0.7435294 ## 3 Male Child 0.4531250 ## 4 Female Child 0.6222222 Nüüd saame oma tulemused näiteks joonisele panna: ggplot(ndata, aes(x = Sex, y = pred, color = Age))+ geom_point(position = position_dodge(width = 0.5), size = 3)+ labs(y = &quot;Survival probability&quot;)+ scale_y_continuous(labels = scales::percent)+ scale_color_manual(values = c(&quot;#972D15&quot;, &quot;#02401B&quot;))+ theme_minimal() 4.7.1 Broom Prognoositud väärtused kõikidele meie andmetes olevatele vaatlustele saame mõnevõrra lihtsamalt kätte paketi broom abil. broomi funktsioon augment() loob mudeli objektist andmestiku, milles on lisaks algsetele tunnusetele ka kõikidele vaatlustele prognoositud väärtudsed (.fitted), prognoositud väärtuste standardvead (.se.fit), jäägid (.resid) jne. library(broom) # Kasutame broomi funktsiooni augment mudel_fit &lt;- augment(mudel8, type.predict = &quot;response&quot;) head(mudel_fit) ## # A tibble: 6 x 9 ## `I(Survived == &quot;~` Sex Age .fitted .resid .std.resid .hat .sigma .cooksd ## &lt;I&lt;lgl&gt;&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FALSE Male Child 0.453 -1.10 -1.11 0.0156 1.03 0.00334 ## 2 FALSE Male Child 0.453 -1.10 -1.11 0.0156 1.03 0.00334 ## 3 FALSE Male Child 0.453 -1.10 -1.11 0.0156 1.03 0.00334 ## 4 FALSE Male Child 0.453 -1.10 -1.11 0.0156 1.03 0.00334 ## 5 FALSE Male Child 0.453 -1.10 -1.11 0.0156 1.03 0.00334 ## 6 FALSE Male Child 0.453 -1.10 -1.11 0.0156 1.03 0.00334 4.8 Marginaalsed efektid Marginaalsed efeketid (marginal effects) kirjeldavad sõltuva tunnuse muutust kui mingi sõltumatu tunnus muutub ühe ühiku võrra. Seega võimaldavad need logistilise regressiooni puhul kasutada lineaarse regressiooniga analoogset tõlgendamisloogikat. Marginaalsete efektide arvutamiseks on erinevaid viise. Üheks levinuimaks meetodiks on nn Keskmised marginaalsed efektid (Average Marginal Effects ehk AME). Oletame, et tahame Titanicu andmestiku alusel hinnata kui palju muutub inimese ellujäämise tõenäosus sõltuvalt tema soost. Logistilise regressioonimudeli abil saame teada vastava ansside suhte. Meid aga huvitaks tõenäosus. Me saame ka tõenäosuse välja arvutada (näiteks predict() funktsiooniga), kuid selleks peame defineerima mingi konkreetse grupi, kellele me regressioonivõrrandi abil tõenäosust prognoosime (näiteks saame võrrelda esimese klassi kajutis elvate täiskasvanud meeste ellujäämise tõenäosust esimese klassi kajutis elavate täsikasvanud naiste ellujäämise tõenäosusega). Meid aga huvitaks lihtsalt keskmine tõenäosuse erinevus meeste ja naiste vahel. Kuidas seda saavutada? Marginaalsete efektide (täpsemalt selle AME variandi) leidmiseks prognoositakse kõikidele andmestiku vaatlustele mudelipõhine hinnang kahel juhul - esimesel juhul nii, et kõikide vaatluste puhul määratakse nende sooks mees ja teisel juhul nii, et kõikide vaatluste puhul määratakse nende sooks naine. Kõik muud tunnused on mõlemal puhul nii nagu nad algselt olid. Keskmine marginaalne efekt ongi keskmine kahe prognoositud hinnangu vahe. library(margins) summary(margins(mudel8)) ## factor AME SE z p lower upper ## AgeAdult -0.1710 0.0521 -3.2827 0.0010 -0.2731 -0.0689 ## SexFemale 0.5224 0.0227 23.0123 0.0000 0.4779 0.5669 Saame järeldada, et täiskasvanute tõenäosus ellu jääda oli \\(17\\%\\) väiksem kui lastel ning naiste tõenäosus ellu jääda oli \\(52\\%\\) kõrgem kui meestel. 4.9 Prognoosi täpsus Confusion matrixi (segaduse maatriks?) abiga saame hinnata oma prognoosi täpsust. Võrdleme tegelikke ja hinnatuid väärtusi. Kasutame jälle predict() funktsiooni ning prognoosime seekord kõikidele titanic andmestiku vaatlustele mudelipõhised hinnangud. Seejärel võrdleme neid hinnanguid vaatluste tegelike väärtustega: # Anname table() funktsioonile ette kaks loogilist vektorit. # Kui me predict funktsioonile newdata argumeti ei anna, # siis võtab ta automaatselt mudeli objektist kogu andmestiku # ja prognoosib hinnangu igale vaatlusele. Kuna prognoos on # tõenäosusskaalal, siis teeme selle loogiliseks vektoriks nii, # et kõik üle 0.5 tõenäosused oleksd T ja väiksemad F vaadeldud &lt;- titanic$Survived == &quot;Yes&quot; prognoos &lt;- predict(mudel8, type = &quot;response&quot;)&gt; 0.5 table(vaadeldud, prognoos) ## prognoos ## vaadeldud FALSE TRUE ## FALSE 1364 126 ## TRUE 367 344 Saadud maatriksist näeme, et prognoosisime oma mudeliga õigesti \\(1364 + 344 = 1708\\) juhul ning valesti \\(367+126 = 493\\) juhul, ehk siis meie mudeli täpsus (accuracy) on \\(\\frac{1364 + 344}{1364 + 344 + 367+126} = 0.776 = 78\\%\\). Maatriksist saame välja lugeda ka prognoosi tundlikkuse (sensitivity) ja spetsiifilisuse (specificity). Tundlikkus väljendab õigesti prognoositud positiivsete väärtuste osakaalu kõikidest positiivsetest väärtustest \\[\\text{tundlikkus} = \\frac{\\text{õige positiivne}}{\\text{õige positiivne} + \\text{vale negatiivne}} = \\frac{344}{(344+367)} = 0.48\\] Spetsiifilisus omakorda väljendab õigesti prognoositud negatiivsete väärtuste osakaalu kõikidest negatiivsetest väärtustest \\[\\text{spetsiifilisus} = \\frac{\\text{õige negatiivne}}{\\text{õige negatiivne} + \\text{vale positiivne}} = \\frac{1364}{(1364+126)} = 0.92\\] Saame need arvutused teha ka caret paketi ja confusionMatrix() funktsiooniga. library(caret) # confusionMatrix vajab sisendiuna faktoreid, # positive = TRUE arguimendiga ütleme, et ellujäämine oli positiivne sündmus confusionMatrix(data = as.factor(prognoos), reference = as.factor(vaadeldud), positive = &#39;TRUE&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction FALSE TRUE ## FALSE 1364 367 ## TRUE 126 344 ## ## Accuracy : 0.776 ## 95% CI : (0.758, 0.7933) ## No Information Rate : 0.677 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.4381 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.4838 ## Specificity : 0.9154 ## Pos Pred Value : 0.7319 ## Neg Pred Value : 0.7880 ## Prevalence : 0.3230 ## Detection Rate : 0.1563 ## Detection Prevalence : 0.2135 ## Balanced Accuracy : 0.6996 ## ## &#39;Positive&#39; Class : TRUE ## Nii mudeli täpsus, tundlikkus, kui ka spetsiifilisus lähtusid eeldusest, et me klassifitseerisime vaatlused positiivseteks või negatiivseteks lähtuvalt sellest kas nende prognoositud tõenäosus oli suurem või väiksem kui \\(0.5\\) (nn treshold või cutoff value). Mida suurem on see cutoff, seda rohkem õigeid positiivseid väärtusi saame prognoosida. Kuid samas, seda vähem saame prognoosida õigeid negatiivseid väärtusi. Ehk siis tundlikkuse ja spetsiifilisuse vahel on pöördvõrdeline seos. Mida suurem on üks, seda väiksem peab teine olema ja vastupidi. Seda seost saame vaadelda ROCi (receiver operating characteristics) graafiku abil. library(ROCit) library(broom) # Kasutame broomi funktsiooni augment mudel_fit &lt;- augment(mudel8, type.predict = &quot;response&quot;) roc_obj &lt;- rocit(score = mudel_fit$.fitted,class=mudel_fit$`I(Survived == &quot;Yes&quot;)`) plot(roc_obj) Mida suurem on pind graafiku kurvi all, seda parema mudeliga meil tegemist on (seda täpsemini võimaldab mudel prognoosida). Seda kurvi alust pindala suurust kasutataksegi prognoosi täpsuse hindamiseks. Vastavat statistikut kutsustaksegi kurvialuseks pindalaks (AUC ehk area under the curve). Mida lähemal AUC \\(1\\)le on, seda parema prognoosivõimega mudeliga meil tegemist on. summary(roc_obj) ## ## Method used: empirical ## Number of positive(s): 711 ## Number of negative(s): 1490 ## Area under curve: 0.7133 z-test puhul on tegemist t-testi analoogiga, mis ei lähtu mitte t-jaotusest, vaid normaaljaotusest. Tõlgendus on aga sama Peame siin arvestama ka erinevust vabadusastmetes. Kuigi sisuka mudeli hälve on väiksem, on selles ka vähem vabadusastmeid predict() funktsiooni saab kasutada ka tavalise regressiooni puhul "],["poissoni-regressioon.html", "Peatükk 5 Poissoni regressioon 5.1 Mudeli tõlgendus 5.2 Poissoni regressiooni eeldused 5.3 Mudeli hindamine R-is 5.4 Mudeli eelduste kontroll 5.5 Mudeli sobivus (model fit)", " Peatükk 5 Poissoni regressioon Poissoni regressioon kuulub üldistatud lineaarsete mudelite (GLM) raamistikku ja sellega saame hinnata sõltumatute tunnuste mõju mingile loendavale (count) sõltuvale tunnusele (mitu inimest on kursusel, mitu inimest on poejärjekorras, mitu last on peres jne)16. Sarnaselt logistilise regressiooniga (ja isegi õigustatumalt) tekib küsimus, et miks me ei saa taoliste tunnuste korral kasutada tavalist lineaarset regressiooni? Loend on ju suhteliselt sarnane tavalisele arvtunnusele. Välja arvatud asjaolu, et nii nagu tõenäosus logistilise regressiooni puhul, ei saa ka loend olla negatiivne. Kui me modelleeriksime loendi tunnust tavalise lineaarse regressiooniga, siis võib vabalt juhtuda, et mingite sõltumatute tunnuste väärtuste korral oleks prognoositav sõltuv tunnus väiksem kui 0. See aga ei ole loendilise tunnuse puhul realistlik. Lisaks, nii nagu ka logistilise regressiooni ja binaarsete sõltuvate tunnuste puhul, tekivad probleemid jääkide struktuuriga (tavaline regressioon eeldab normaaljaotust) ja jääkide dispersiooniga (tavaline regressioon eeldab konstantset hajuvust)17. Joonis 5.1: Poissoni jaotus erinevate keskmiste loendusväärtuste (Lambda) korral Logistilise regressiooni puhul saime tõenäosuse ülemisest piirist lahti seeläbi, et teisendasime tõenäosused anssideks ning alumise piiri seeläbi, et võtsime anssidest logaritmid. Loendilise tunnuse puhul meil tunnuse ülemise piiriga probleemi ei ole. Loend võib potentsiaalselt olla lõputu. Alumise piiri puhul saame aga kasutada sedasama logit mudelitest tuttavat logaritmimise nippi. Kui \\(Y\\) on meie hinnatav loendiline sõltuv tunnus ja \\(x\\) seda selgitav sõltumatu tunnus, siis Poissoni regressioonimudel on väljendatav järgmiselt18: \\[log(Y_i)=\\beta_0+\\beta_1 x_i\\] kui me võtame mõlemast võrrandi pooles eksponendi, saame sama asja väljendada ka nii: \\[Y_i = e^{\\beta_0+\\beta_1 x_i}\\] Eeldame siin, et \\(Y_i\\) järgib Poissoni jaotust. GLMi sõnavara kasutades ütleme, et mudeli juhuslik komponent (ehk siis sõltuv tunnus) on Poissoni jaotusega ning linkfunktsioonina (funktsiooon, mille abil sõltuva ja sõltumatute tunnuste vaheline mittelineaarne seos muudetakse lineaarseks seoseks) kasutame log-funktsiooni. Poissoni jaotuse kuju sõltub tunnuse keskmisest (võrdluseks, normaaljaotuse kuju sõltub tunnuse keskmisest ja standardhälbest). Mida suurem on keskmine, seda enam sarnaneb Poissoni jaotus normaaljaotusele (vt joonis 1). Seega suurte loendite puhul saaksime põhimõtteliselt ka tavalist lineaarset regressiooni kasutada (kuigi negatiivsete väärtuste probleem jääb ka sel juhul). Üldiselt on mõistlik Poissoni regressiooni kasutada siis, kui loendite maksimaalsed väärtused ei ole väga suured ja tunnus on eripäraselt Poissoni jaotuse kujuga. Kui meil on tegemist suuremate väärtustega loendiga (näiteks ülikoolide tudengite arvud), siis saame tunnuse näiteks mingi arvuga läbi jagada. Loendilise tunnuse puhul tuleb tihti ette olukordi, kus tunnuses on palju nulle. Näiteks tunnus, mis kirjeldab bakalaureusetudengite laste arvu. Põhimõtteliselt on muidugi tegemist loendilise tunnusega, aga kuna suuremal osal tudengitest veel ei ole lapsi, siis enamik vaatlusi on paratamatult nullid. Sellise tunnuse jaotus ei vasta väga hästi Poissoni jaotusele ja selle kasutamine Poissoni regressioonimudeliga ei anna tõenäoliselt väga head tulemust19. 5.1 Mudeli tõlgendus Kuidas me neid regressioonimudeli \\(\\beta_0\\) ja \\(\\beta_1\\) koefitsiente tõlgendama peaksime? Lineaarse regressiooni puhul oli asi lihtne: vabaliige (\\(\\beta_0\\)) oli tõlgendatav \\(Y\\) väärtusena kui \\(x\\) on \\(0\\) ja regressioonikoefitsient (\\(\\beta_1\\)) näitas \\(Y\\) muutust kui \\(x\\) muutub ühe ühiku võrra. Logistilise regressiooni puhul pidime aga esmalt koefitsientidest eksponendi võtma ja saime neid seejärel tõlgendada ansside ja ansside suhetena. Mis siis sekord? Kuna meil on jälle tegemist logaritmidega, siis koefitsientide otsene (lineaarse regressiooni moodi) tõlgendamine on keeruline. Mõistlikum on koefitsientidest jällegi eksponent võtta, misjärel saame vabaliiget tõlgendada tavapärasel moel (\\(Y\\) väärtus kui \\(x\\) on \\(0\\)) ja regressioonikoefitsienti kui \\(Y\\)-i multiplikatiivset muutust kui \\(x\\) muutub ühe ühiku võrra. Ehk kui \\(\\beta_1\\) väärtus on näiteks 0.25, siis tema eksponent on \\(e^{0.25} = exp(0.25) = 1.28\\) ja saame järeldada, et kui \\(x\\) kasvab ühe ühiku võrra, siis \\(Y\\) kasvab \\(1.28\\) korda. Teisisõnu, \\(Y\\) kasvab \\(28\\%\\). Või kui \\(\\beta_1\\) väärtus on \\(-0.5\\), siis tema eksponent on \\(e^{-0.5} = exp(-0.5) = 0.6\\) ja saame järeldada, et kui \\(x\\) kasvab ühe ühiku võrra, siis \\(Y\\) kasvab \\(0.6\\) korda (ehk siis tegelikult kahaneb). Kui \\(\\beta = 0\\), siis \\(e^{0} = 1\\) ehk multiplikatiivne efekt on \\(1\\) (\\(Y\\times1\\)) ja \\(Y\\) \\(x\\)-i kasvades või kahanedes ei muutu. Kui \\(\\beta &lt; 0\\), siis \\(Y\\) \\(x\\)-i kasvades väheneb, kui \\(\\beta &gt; 0\\), siis \\(Y\\) \\(x\\)-i kasvades kasvab. 5.2 Poissoni regressiooni eeldused Sõltuv tunnus \\(Y\\) peaks enam-vähem vastama Poissoni jaotusele (st olema loendiline tunnus). Vaatlused peavad olema üksteisest sõltumatud (st kogu vaatluste vaheline seos peaks olema kirjeldatud mudeli sõltumatute tunnuste poolt). Dispersioon (variance) peaks olema võrdne keskmisega. Juhul kui see eeldus ei ole täidetud, ja tihti juhtub, et ei ole, on meil tegemist nn üledispersiooniga (overdispersion). Sellisel juhul tuleks Poissoni mudeli asemel kasutada nn quasipoissoni mudelit. Sõltuva tunnuse ja sõltumatute prediktorite seos peaks läbi linkfunktsiooni olema lineaarne. 5.3 Mudeli hindamine R-is 5.3.1 Andmete kirjeldus ja ettevalmistus Kasutame näitena PhDPublications andmestikku20 paketist AER. Andmestikus on loendatud biokeemia doktorantide publikatsioonide arv (tunnus atricles) kolme aasta jooksul. Sõltumatute tunnustena on kasutada: gender, married - kas doktorant oli abielus, kids - mitu last doktorandil oli, prestige - kooli maine skoor) ja mentor - juhendaja publikatsioonide arv. # Kui pakett ei ole installitud, # tuleb seda teha käsuga: install.packages(&quot;AER&quot;) # Loeme paketi sisse library(AER) # Võtame andmestiku data(PhDPublications) # Paneme andmestikule lihtsama nime phd &lt;- PhDPublications Vaatame artiklite tunnust lähemalt: ggplot(phd)+ geom_histogram(aes(x = articles), binwidth = 1, fill = &#39;grey&#39;, color = &#39;black&#39;)+ theme_minimal() Ilmselgelt on tegemist loendilise tunnusega. Samas päris Poissoni jaotusega tegemist vist siiski ei ole, kuna tundub, et nulle on selleks natukene liiga palju. ggploti abil saame joonisele panna ka tunnust iseloomustava teoreetilise Poissoni jaotuse (lähtuvalt tunnuse keskmisest ehk \\(\\lambda\\) parameetrist). Vaatame kuidas see võrreldes reaalse jaotusega välja näeb: # Poissoni jaotuse parameetrina on meil vaja keskmist keskmine &lt;- mean(phd$articles) phd %&gt;% # standardiseerime artiklite arvu, # et saaksime seda jaotusega võrrelda group_by(articles) %&gt;% summarise(n = n()) %&gt;% mutate(n_scaled = n/sum(n)) %&gt;% ggplot(aes(x = articles, y = n_scaled))+ # kasutame stat = &#39;identity&#39;, # st kasutame joonisel olemasolevaid väärtusi # (mitte ei lase ggplot&#39;il neid välja arvutada) geom_histogram(stat = &#39;identity&#39;, fill = &#39;grey&#39;, color = &#39;black&#39;)+ # dpois funktsioon annab meile poissoni tihedusfunktsiooni geom_line(aes(x = articles, y = dpois(articles, lambda = keskmine)), color = &#39;red&#39;)+ theme_minimal() Nagu näeme, siis tõesti, nulle on natuke liiga palju ja artiklite loendi jaotus ei vasta jaotuse alguses päris täpselt Poissoni jaotuse kujule. Aga jätame selle asjaolu hetkel tähelepanuta ja kasutame ikkagi Poissoni regressiooni. Vaatame üle ka teised andmestiku tunnused: phd %&gt;% group_by(gender) %&gt;% summarize(n()) ## # A tibble: 2 x 2 ## gender `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 male 494 ## 2 female 421 phd %&gt;% group_by(married) %&gt;% summarize(n()) ## # A tibble: 2 x 2 ## married `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 no 309 ## 2 yes 606 Soo ja abielu tunnus tunduvad korras olevat, kui välja arvata see, et kuidagi paljud doktorandid paistavad abielus olevat. Aga on nagu on. phd %&gt;% group_by(kids) %&gt;% summarize(n()) ## # A tibble: 4 x 2 ## kids `n()` ## &lt;int&gt; &lt;int&gt; ## 1 0 599 ## 2 1 195 ## 3 2 105 ## 4 3 16 Laste tunnus on originaalis arvuline. Me kindlasti ei taha nelja väärtusega tunnust arvulisena käsitleda. Parem oleks see faktoriks teha ja seda mudelis kategoriaalsena käsitleda. Antud juhul, kuna lastetuid doktorante on niivõrd palju, oleks vast kõige mõistlik see tunnus üldse binaarseks teha, st kas on või ei ole lapsi (kuigi teoreetiliselt võiks ju eeldada, et laste arv võib mõjutada artiklite kirjutamiseks jäävat aega, siis siin, mulle tundub, on laste arvu variatiivsus selle kasutamiseks liiga väike). # kasutame laste arvu teisendamiseks ifelse() funktsiooni # vajadusel vaadake sellekohast abiinfot ?ifelse phd &lt;- phd %&gt;% mutate(kids2 = ifelse(kids == 0, &#39;Ei&#39;, &#39;Jah&#39;)) # kontrollime uut tunnust phd %&gt;% group_by(kids, kids2) %&gt;% summarise(n()) ## # A tibble: 4 x 3 ## # Groups: kids [4] ## kids kids2 `n()` ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 0 Ei 599 ## 2 1 Jah 195 ## 3 2 Jah 105 ## 4 3 Jah 16 Pidevast tunnusest ülevaate saamiseks on mugav kasutada histogrammi: hist(phd$prestige) PhD programmi maine küsimus tundub suht OK. hist(phd$mentor) Juhendaja artiklit arvu tunnus on iseenesest samuti loendav tunnus (kuigi Poissoni jaotust see samuti päris täpselt ei järgi). 5.3.2 Poissoni mudeli defineerimine Vaatame esmalt kuidas sugu artiklite avaldamist mõjutab. Defineerime mudeli kasutades glm() funktsiooni ja selles family = argumendina poisson() funktsiooni (kõlbaks ka family = \"poisson\" või family = poisson). poisson() funktsiooni puhul on vaikimisi eeldatud linkfunktsioonina log-linki, kuid vajadusel saaksime ka mingit muud linkfunktsiooni kasutada või selle eksplitsiitselt välja tuua: poisson(link = 'log'). Salvestame mudeli kõigepealt andmeobjektiks ning seejärel uurime seda summary() funktsiooniga. m1 &lt;- glm(articles~gender, family = poisson(), data = phd) summary(m1) ## ## Call: ## glm(formula = articles ~ gender, family = poisson(), data = phd) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9404 -1.7148 -0.4119 0.4139 7.3221 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.63265 0.03279 19.293 &lt; 2e-16 *** ## genderfemale -0.24718 0.05187 -4.765 1.89e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1817.4 on 914 degrees of freedom ## Residual deviance: 1794.4 on 913 degrees of freedom ## AIC: 3466.1 ## ## Number of Fisher Scoring iterations: 5 Nagu näeme, siis Poissoni regressiooni väljund on praktiliselt identne logistilise regressiooni väljundiga (mis on ka loogiline, kuna mõlemad on loodud sama funktsiooniga). Tõlgendame kõigepealt regressioonikoefitsiente. Nagu eelnevalt mainitud, siis tuleks neist eelnevalt eksponent võtta. Aga ka ilma selleta saame öelda, et naised on meestest vähem artikleid avaldanud. Mehed on antud juhul referentsgrupp ning genderfemale koefitsient kirjeldab naiste erinevust meestest. Kuna koefitsient on negatiivne, siis saame järeldada, et keskmine artiklite arv on naiste hulgas väiksem kui meeste hulgas. Aga et teada saada kui palju väiksem, peame koefitsientidest eksponendi võtma: # koefitsiendid saame mudeli objektist kätte coef() funktsiooniga exp(coef(m1)) ## (Intercept) genderfemale ## 1.8825911 0.7810027 Vabaliige on \\(Y\\) väärtus kui \\(x\\) on \\(0\\). Kuna mehed on referentsgrupp (ehk siis \\(0\\)), siis kirjeldab vabaliige siinkohal meeste keskmist artiklite arvu. Võime tulemuse dplyriga verifitseerida: phd %&gt;% filter(gender == &#39;male&#39;) %&gt;% summarize(mean(articles)) ## mean(articles) ## 1 1.882591 Tundub tõesti nii olevat. Eksponenti võetud genderfemale koefitsient näitab naiste multiplikatiivset erinevust meestest. Ehk siis naiste keskmine artiklite arv peaks olema \\(1.88 \\times 0.78 = 1.47\\). Teiste sõnadega naiste keskmine artiklite arv on \\(1-0.78 = 22\\%\\) väiksem kui meestel. Kontrollime üle: phd %&gt;% filter(gender == &#39;female&#39;) %&gt;% summarize(mean(articles)) ## mean(articles) ## 1 1.470309 Jälle sama tulemus. Mida me veel tulemist näeme ja näha tahame? Eelkõige huvitab meid muidugi see, kas meie tulemused on statistiliselt usaldusväärsed. Regressioonikordajate statistilise olulisuse hindamiseks on meil analoogselt logistilise regressiooni väljundiga kasutada z-test, mis testib kas regressioonikordaja on oluliselt erinev nullist. Näeme nii teststatistikut (z-väärtust), kui ka z-testi iseloomustavat p-väärtust. p väärtused (tulp Pr(&gt;|z|)) on nii vabaliikme kui regressioonikoefitsiendi puhul oluliselt väiksemad kui \\(0.05\\), misläbi saame järeldada, et usaldusnivool \\(95\\%\\) on meie mudeli koefitsiendid statistiliselt oluliselt erinevad nullist21, ehk siis sugu mõjutab oluliselt avaldatud artiklite arvu. Kui koefitsient ei oleks statistiliselt oluline (\\(p &gt; 0.05\\)), siis peaksime järeldama, et meie koefitsient ei erine oluliselt nullist ning seost sõltumatu ja sõltuva tunnuse vahel ei ole. Sama järelduse saaksime tegelikult teha ka z-väärtuste ja standardvigade (tulp Std. Error) põhjal. Mida väiksem on z-väärtus, seda väiksem on p. Ja kui z-väärtus on (absoluutarvuna) suurem kui \\(1.96\\), siis on p on väiksem kui \\(0.05\\). Ning z-väärtus (ja z-test) omakorda tuleneb standardveast: \\(\\frac{-0.24718}{0.05187} = -4.765\\). Aga vaatame oma mudelit edasi. Lisame ka teised sõltumatud tunnused: m2 &lt;- glm(articles~gender+prestige+married+kids2+mentor, family = poisson(), data = phd) summary(m2) ## ## Call: ## glm(formula = articles ~ gender + prestige + married + kids2 + ## mentor, family = poisson(), data = phd) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4477 -1.5669 -0.3587 0.5705 5.4715 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.306243 0.103093 2.971 0.00297 ** ## genderfemale -0.217925 0.054717 -3.983 6.81e-05 *** ## prestige 0.010275 0.026460 0.388 0.69776 ## marriedyes 0.151697 0.063028 2.407 0.01609 * ## kids2Jah -0.249563 0.063342 -3.940 8.15e-05 *** ## mentor 0.025817 0.002019 12.788 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1817.4 on 914 degrees of freedom ## Residual deviance: 1640.9 on 909 degrees of freedom ## AIC: 3320.7 ## ## Number of Fisher Scoring iterations: 5 Näeme, et ülikooli maine ei mõjuta statistiliselt oluliselt artiklite arvu. Ka abielustaatus on suhteliselt piiripealse mõjuga. Jätame maine tunnuse mudelist välja (tahame alati leida võimalikult lihtsa mudeli, seega tunnused, mis mudelisse ei ei panusta, jätame välja). m3 &lt;- glm(articles~gender+married+kids2+mentor, family = poisson(), data = phd) Kontrollime igaks juhuks ka anova()-ga, kas maine tunnuse väljajätmine ikka oli õigustatud: anova(m3, m2, test = &#39;Chisq&#39;) ## Analysis of Deviance Table ## ## Model 1: articles ~ gender + married + kids2 + mentor ## Model 2: articles ~ gender + prestige + married + kids2 + mentor ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 910 1641.1 ## 2 909 1640.9 1 0.15093 0.6976 Hii-ruut test ütleb meile, et keerulisema ja lihtsama mudeli vahel ei ole statistiliselt olulist erinevust (\\(p &gt; 0.05\\)), seega võime tunnuse vabalt välja jätta. Näeme, et nii sugu, abielustaatus, laste arv, kui ka juhendaja publikatsioonide arv mõjutavad artiklite arvu oluliselt. Kontrollime igaks juhuks ka soo ja laste olemasolu koosmõju: m4 &lt;- glm(articles~gender*kids2+married+mentor, family = poisson(), data = phd) summary(m4) ## ## Call: ## glm(formula = articles ~ gender * kids2 + married + mentor, family = poisson(), ## data = phd) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4995 -1.5593 -0.3582 0.5639 5.4570 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.34220 0.06249 5.476 4.35e-08 *** ## genderfemale -0.22492 0.06299 -3.571 0.000356 *** ## kids2Jah -0.25695 0.07148 -3.595 0.000325 *** ## marriedyes 0.14875 0.06291 2.365 0.018045 * ## mentor 0.02601 0.00196 13.268 &lt; 2e-16 *** ## genderfemale:kids2Jah 0.02522 0.12539 0.201 0.840628 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1817.4 on 914 degrees of freedom ## Residual deviance: 1641.0 on 909 degrees of freedom ## AIC: 3320.8 ## ## Number of Fisher Scoring iterations: 5 Koosmõju koefitsient ei ole statistiliselt oluline, seega jääme mudeli m3 juurde: summary(m3) ## ## Call: ## glm(formula = articles ~ gender + married + kids2 + mentor, family = poisson(), ## data = phd) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.5080 -1.5615 -0.3626 0.5614 5.4494 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.33873 0.06011 5.635 1.75e-08 *** ## genderfemale -0.21864 0.05470 -3.997 6.42e-05 *** ## marriedyes 0.14953 0.06279 2.381 0.0172 * ## kids2Jah -0.25029 0.06332 -3.953 7.73e-05 *** ## mentor 0.02600 0.00196 13.268 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1817.4 on 914 degrees of freedom ## Residual deviance: 1641.1 on 910 degrees of freedom ## AIC: 3318.8 ## ## Number of Fisher Scoring iterations: 5 Järgmiseks võtame mudeli koefitsientide eksponendid, et neid natukene inimlikumal kujul kuvada ja tõlgendada: exp(coef(m3)) ## (Intercept) genderfemale marriedyes kids2Jah mentor ## 1.4031677 0.8036087 1.1612850 0.7785778 1.0263406 Mida need koefitsiendid meile ütlevad? Vabaliige on antud näite puhul artiklite arv juhul kui kõikide sõltumatute tunnuste väärtused on nullid (ehk siis kõik kategoorilised tunnused on referentsväärtusega ja pidevtunnused lihtsalt nullid). Konkreetsel juhul on see ka sisukas tulemus, kuna mudeli ainsa pidevtunnuse nullväärtus on täiesti realistilik (juhendaja artiklite arv võib vabalt \\(0\\) olla). Aga üldiselt see pigem nii ei ole (mida me hakkame peale \\(Y\\) väärtusega, mis kehtib olukorras kus vanus või kehakaal on \\(0\\)). Seega, meie mudeli järgi on vallaliste, ilma lasteta meeste, kelle juhendajad pole viimase kolme aasta jooksul ühtegi publikatsiooni avaldanud, artiklite arv keskmiselt 1.4. Naiste keskmine publikatsioonide arv (hoides teisi tunnuseid konstantsetena) on keskmiselt 20% väiksem (\\(1-0.8 = 0.2 = 20\\%\\)) kui meestel. Abielus doktorantide keskmine publikatsioonide arv on \\(1.16\\) ehk \\(16\\%\\) suurem kui vallalistel doktorantidel. Laste olemasolu pärsib artiklite avaldamist keskmiselt \\(22\\%\\) võrra. Iga juhendaja lisanduv artikkel tõstab juhendatava publitseerimisvõimekust \\(2\\%\\) võrra. 5.4 Mudeli eelduste kontroll 5.4.1 Üledispersioon Mudeli üledispersioon (overdispersion) on olukord, kus mudeli dispersioon on suurem kui mudeli aluseks olev jaotusfunktsioon eeldaks. Kui see on nii, siis on mudeli standardvead tõenäoliselt liiga väikesed (ja seega mudeli alusel tehtavad järldused valed). Üledispersiooni olemasolu saame kontrollida võrreldes jääkhälbimust (Residual deviance) ja selle vabadusastemid (degrees of freedom ehk df). Kui need on enam-vähem võrdsed, ehk \\(\\frac{\\text{Residual deviance}}{\\text{df}} \\approx 1\\), siis üledispersiooni ei ole. Aga kui see suhe on oluliselt suurem kui \\(1\\), siis on tegemist probleemiga. Juhul kui taoline olukord esineb, peaksime Poissoni mudeli asemel kasutama quasipoissoni mudelit, kus üledispersiooni on eraldi dispersiooni parameetrina mudelis arvesse võetud. Ka meie näite puhul on tegemist kerge üledispersiooniga (mitte küll väga suurega, aga siiski), seega võiksime kasutada quasipoissonit: m5 &lt;- glm(articles~gender+married+kids2+mentor, family = quasipoisson, data = phd) summary(m5) ## ## Call: ## glm(formula = articles ~ gender + married + kids2 + mentor, family = quasipoisson, ## data = phd) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.5080 -1.5615 -0.3626 0.5614 5.4494 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.338732 0.081482 4.157 3.53e-05 *** ## genderfemale -0.218643 0.074151 -2.949 0.00327 ** ## marriedyes 0.149527 0.085110 1.757 0.07928 . ## kids2Jah -0.250286 0.085834 -2.916 0.00363 ** ## mentor 0.026000 0.002656 9.788 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 1.837376) ## ## Null deviance: 1817.4 on 914 degrees of freedom ## Residual deviance: 1641.1 on 910 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 Näeme, et punkthinnangud ei muutunud, küll aga läksid standardvead suuremaks. See tähendab, et quaipoissoniga meie mudeli täpsusaste kahaneb (või õigemini esialgse, tavalise Poissoni mudeli puhul hindasime me mudeli täpsust üle). Samuti näeme, et tänu sellele ei ole abielustaatuse koefitsient enam \\(95\\%\\) usaldusnivoo korral statistiliselt oluline ja peaksime selle tunnuse välja jätma. m6 &lt;- glm(articles~gender+kids2+mentor, family = quasipoisson, data = phd) summary(m6) ## ## Call: ## glm(formula = articles ~ gender + kids2 + mentor, family = quasipoisson, ## data = phd) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4818 -1.5758 -0.3663 0.5443 5.5757 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.423844 0.064597 6.561 8.93e-11 *** ## genderfemale -0.233279 0.073885 -3.157 0.00164 ** ## kids2Jah -0.179615 0.076785 -2.339 0.01954 * ## mentor 0.025776 0.002659 9.695 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 1.843047) ## ## Null deviance: 1817.4 on 914 degrees of freedom ## Residual deviance: 1646.8 on 911 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 5.5 Mudeli sobivus (model fit) Mudeli sobivust andmetega (goodness of fit) saame analoogselt logistilise regressiooniga hinnata jääkhälbimuse (Residual deviance) abil: res_dev &lt;- deviance(m6) res_df &lt;- df.residual(m6) pchisq(res_dev, res_df, lower.tail=FALSE) ## [1] 5.095965e-45 Näeme, et p-väärtus on väiksem kui \\(0.05\\), mis tähendab, et meie mudel ei sobitu andmetega väga hästi (siin tahame, et p-väärtus oleks võimalikult suur). Reaaleluliste andmetega ongi tegelikult väga keeruline hästi sobituvat mudelit leida. Seega üldjuhul me lihtsalt lepime, et meie mudel ei ole täiuslik ja jätame selle testi tähelepanuta. 5.5.1 Mudeli statistiline olulisus Mudeli statistilist olulisust saame jällegi hinnata analoogselt logistilisele regressioonil hälbimuse näitajate abil. Võrdleme nullmudeli hälbimust (Null deviance) ja jääkhälbimust (Residual deviance). Nullmudeli hälbimus hinnatud \\(Y\\) hälbimust tegelikest \\(Y\\) väärtustest juhul kui ühtegi prediktorit mudelis ei ole. Seega kahe hälbimuse vahe näitab kui palju meie mudel tänu sõltumatutele tunnustele paremaks on läinud. See hälbumuste vahe on jälle jaotunud hii-ruut jaotsue alusel, mille vabadusastemeteks on nullmudeli ja hinnatava mudeli vabadusastmete vahe. dev_vahe &lt;- m6$null.deviance - m6$deviance df_vahe &lt;- m6$df.null-m6$df.residual pchisq(dev_vahe, df_vahe, lower.tail = F) ## [1] 9.269198e-37 p-väärtus on väiksem kui \\(0.05\\), seega nullmudel ja meie mudel erinevad olulisel määral ning võime järeldada, et meie mudel on statistiliselt oluline. 5.5.2 Jääkide jaotus Poissoni mudeli üheks eelduseks olid üksteisest sõltumatud ja normaaljaotuse järgi jaotunud jäägid. Paneme joonisele mudeli jäägid (hälbimused) ja (log) prognoositud väärtused. Jäägid peaksid üle prognoositud väärtuste suhteliselt ühtlaselt jaotuma ning mingit selgelt eristuvat mustrit ei tohiks täheldada. Antud juhul ei ole olukord just ideaalne, aga ka mitte kõige hullem. Näeme, et prognoositud väärtuste paremas otsas koonduvad jäägid pigem allapoole, samas jääkide variatiivsus on keskjoonest kõrgemal mõnevõrra suurem. Võib eeldada, et nad ei ole päris normaaljaotuse järgi jaotunud. res &lt;- residuals(m6, type=&quot;deviance&quot;) plot(log(predict(m6)), res) Kontrollime seda ka histogrammiga: hist(res) Ka siit nähtub, et jäägid ei ole tõesti päris normaalselt jaotunud. Lõppkokkuvõtteks tuleb tõdeda, et meie mudel ei ole päris ideaalne - ei sobitu kõige paremini andmetega ning ka jääkide struktuur jätab soovida. Põhjus võib olla näiteks selles, et mõni oluline sõltumatu tunnus on mudelist puudu või ka asjaolu, et sõltuva tunnuse jaotus ei vastanud väga hästi Poissoni jaotusele. Ka kategoriaalsetest tunnustest moodustatud risttabelite sagedused on loendilised väärtused. Taoliste risttabelite sageduste baasil moodustatud mudeleid kutsutakse log-lineaarseteks mudeliteks. Lisaks loenditele kasutatakse Poissoni regressiooni ka määrade (rates) mudeldamiseks, kuna määrasid võib käsitleda kui standardiseeritud loendeid. Siinkohal me taolisi mudeleid lähemalt ei käsitle, aga olgu see lihtsalt ära mainitud. Loendiline tunnus \\(Y\\) järgib Poissoni jaotust, mille puhul \\(E(Y)=Var(Y)=\\lambda\\), kus \\(\\lambda\\) on nn keskmine loend (keskmine kursuse suurus, järjekorra pikkus, laste arv). Seega Poissoni jaotusega tunnuse puhul peaks keskmine ja dispersioon võrdsed olema ning suurem keskmine tähendab ka suuremat dispersioon. \\(log(Y_i)\\) ei ole siin sama mis lineaarse regressiooni puhul log-transformeeritud tunnus (kui me kasutame analüüsis eelnevalt logaritmitud sõltuvat tunnust). Teine ja sellest aspektist võib-olla selgem notatsioon oleks \\(log(E(Y))=\\beta_0+\\beta_1 x_i\\), millest nähtub, et me peame logaritmima \\(Y\\) oodatavat väärtust (expected value) ehk \\(Y\\) keskmist. Keskmine logaritmitud \\(Y\\) ja logaritm keskmisest \\(Y\\)-ist ei ole aga samad asjad (võite järgi proovida: mean(log(c(1,2,3,4))) vs log(mean(c(1,2,3,4)))). Paljude nullidega tunnuse puhul oleks mõistlikum kasutada näiteks zero-inflated Poissoni regressiooni (zeroinfl() funktsioon pscl paketis) või negative binomial regressiooni (glm.nb() funktsioon MASS paketis) Long, J.S. (1997). The Origin of Sex Differences in Science. Social Forces, 68, 12971315. p-väärtus ütleb meile, et juhul kui nullhüpotees oleks tõsi (antud juhul on nullhüpoteesiks, et tunnuste vahel ei ole seost, st regressioonikoefitsient on \\(0\\)), siis saaksime sellise tulemuse nagu me saime (täpsemalt öeldes sellise z-väärtuse) tõenäosusega mis on võrdne p-väärtusega. Kui p-väärtus on näiteks \\(0.05\\), siis uuringut korrates saaksime sellise tulemuse \\(5\\%\\) kordadest. Antud juhul on p-väärtus oluliselt väiksem kui \\(0.5\\), st tõenäosus, et meie regressioonikordaja on tegelikult \\(0\\), on hästi väike. "],["mitmetasandiline-regressioon.html", "Peatükk 6 Mitmetasandiline regressioon 6.1 Andmete hierarhiline struktuur 6.2 Mitmetasandiline analüüs 6.3 Mitmetasandilise analüüsi eeldused 6.4 Mitemtasandiline analüüs Ris", " Peatükk 6 Mitmetasandiline regressioon 6.1 Andmete hierarhiline struktuur Valdav osa meie uuritavatest nähtustest ning uurimisobjektidest on mingil määral hierarhiliselt struktureeritud või klasterdunud: õpilased klasside kaupa patsiendid eri arstide juures elanikud linnaosades, linnades, regioonides jne töötajad ettevõetetes paneeluuringutes (longituuduuringutes) vaatluskorrad inimeste kaupa jne Vaatlused klastrite sees kipuvad olema sarnasemad kui klastrite vahel. Samas klassis õppivaid õpilasi ühendavad samad õpetajad ja nende kasutatavad õppemeetodid ning õppematerjalid, üldine klassi mentaliteet jms. Taolised ühised kogemused ühendavad sama klassi õpilasi ja samas eristavad neid teiste klasside õpilastest. Kui näiteks tahame mõõta õpilaste lugemust, ning hinnata, kuivõrd seda mõjutab lapse vanemate kultuuriline kapital, siis peame korrektsete järelduste tegemiseks seda klassidesisest sarnasust ja klassidevahelist erinevust arvesse võtma. Klassis, kus on hea kirjanduse õpetaja või lugemist väärtustav keskkond, võivad ka madala kultuurilise kapitaliga vanemate lapsed lugeda märksa rohkem, kui mõnes teises klassis vähem lugemist soodustavas keskkonnas õppivad kõrge kultuurilise kapitaliga vanemate lapsed. Võib-olla kõige selgemini on vaatluste hierarhiline struktuur tajutav longituudandmete puhul, kus me mõõdame või vaatleme mingeid omadusi või nähtusi samade respondentide puhul mitmeid kordi mingi aja jooksul. Ehk siis kui tahame näiteks teada kuidas lugemisharjumused muutuvad üle aja, siis võime võime võtta juhuvalimina 100 inimest ning küsida kümne aasta jooksul igal aastal nende selle aasta lugemuse kohta. Nii toimides oleks meil lõpuks koos 1000 erinevat vaatlust või vastust. Kuid on ju ilmne, et me ei saa kõiki neid tuhandet vaatlust sõltumatutena käsitleda. Inimesed, kellele meeldib lugeda, teevad seda tõenäoliselt igal aastal ning inimesed, kes eelistavad muul viisil aega veeta, loevad igal aastal vähem. Seega andmete hierarhilist struktuuri ja loomulikku klasterdumist arvesse võtmata ei ole meil võimalik korrektselt mingeid mõjusid või tendentse hinnata, sest need mõjud sõltuvad klastreid ühendavatest karakteristikutest. See aga tähendab, et tavaline lineaarne regressioon taoliste mõju hindamiseks ei sobi. Nimelt oli lineaarse regressiooni üheks eelduseks vaatluste (ja jääkide) sõltumatus. Kui me selle eeldustega ei arvesta, siis võivad effektide standardvead olla liiga väikesed, mistõttu võime näha statistiliselt olulisi seoseid seal kus neid tegelikult ei ole. Mõningatel eriti hulludel juhtudel võivad lisaks standardvigadele olla valed ka regressiooniseosed ise. Sellisel puhul oleksid meie järelduse mitte ainult ebatäpsed vaid totaalselt valed. Vaatame järgmist näidet, kus on matemaatikatestiks õppimisele kulunud aja kaudu üritatakse seletada testi tulemust22. Kasutame hinnanguks kõigepealt tavalist lineaarset regressiooni: Tulemused tundub kuidagi väga imelikud. Mida kauem õpitakse, seda halvem on testi skoor. Kuida seda seletada? Asi on selles, et tegemist on kolme erineva klassi õpilastega, kelle matemaatilised oskused on suhteliselt erinevad: Kui me seda klasside erinevust arvesse võtame ja igas klassis eraldi regressioonivõrrandi hindame, on tulemused juba märgatavalt loogilisemad: Kõikides klassides eraldivõetuna on seos õppimisele kulunud aja ja testitulemuse vahel ikkagi positiivne. Lihtsalt korrelatsioonikoefitsiendi suurus (sirge tõus) ja vabaliige (sirge lõikumine \\(y\\)-teljega kui \\(x\\) on 0) on kõikides klassides erinev. Seega antud näite puhul oleks meie järeldused, kui me andmete hierarhilist struktuuri arvesse ei võtaks, vastupidised tegelikule olukorrale. Taolist olukorda nimetatakse Simpsoni paradoksiks. Näide on loomulikult mõnevõrra utreeritud, kuid illustreerib hästi andmestiku hierarhilise struktuuriga arvestamise vajalikkust. 6.1.1 Mida me uurida tahame? Mida siis teha, kui me teame või eeldame, et meie uurimisobjektid on hierarhiliselt struktureeritud või moodustavad mingite karakteristikute alustel gruppe? Võimalusi on tegelikult mitmeid ning valik nende võimaluste seast sõltub eelkõige nenede hierarhiate või gruppide iseloomust, aga ka sellest, mis meid konkreetselt huvitab. Klasterdavatest tunnustest lähtuvalt saame analüüsiühikud jagada tasanditeks. Esimese tasandi analüüsiühikud on need mida või keda me otseselt vaatleme või küsitleme. Teise tasandi analüüsiühikud grupid, millesse esimese tasandi ühikud kuuluvad. Näiteks kui õpilased on esimese tasandi analüüsiühikud, siis klassid on teise tasandi analüüsiühikud. Või kui patsiendid on esimene tasand, siis neid ühendavad arstid on teine tasand. Tasandeid võib loomulikult olla ka rohkem. Õpilaste puhul saavad kolmandaks tasandiks olla näiteks koolid, neljandaks tasandiks riigid jne. Meie uurimishuvi võib seisneda ainult esimese tasandi mõjude hindamises. Sellisel juhul käitleme teisi tasandeid lihtsalt takistustena, mida peame oma esimese tasandi analüüsis arvesse võtma, kuid mis meid eraldiseisvalt ei huvita (kas õppimisele kulunud aeg üldiselt mõjutab testitulemust). Võime lihtsalt olla huvitatud ka sellest, kas esimese tasandi mõju on võimalik eristada struktuursetest teguritest (teise tasandi mõjust). Meie uurimishuvi võib seisneda ka teise tasandi mõjude hindamisel esimese tasandi väljunditele. Ehk siis kas makrotasandi tegurid mõjutavad indiviiditasandi väljundeid (arvestades sealjuures ka indiviidide erinevusi). Näiteks kas klassi tüüp mõjutab testitulemusi. Lõuks võib meie uurimishuvi seisneda ka teise tasandi tegurite mõjus esimese tasandi mõjudele. Kas klassi tüüp mõjutab seost õppimisele kulunud aja ja testitulemuse vahel? Või kas klassis kasutusel olev õppekava maht mõjutab testitulemusi? Ehk siis kas mingid struktuursed tegurid mõjutavad indiviiditasandi mõjusid. 6.1.2 Kuidas vaatluste hierahilisust analüüsis arvesse võtta? Viimase, õppimisele kulunud aja ja testitulemuste seoste näite puhul, ehk siis olukorras, kus grupitunnuse kategooriaid on suhteliselt vähe (näites oli neid ainult kolm), oleks kõige mõistlikum kasutada tavalist lineaarset regressiooni, kuid kaasta grupeeriv tunnus sõltumatu tunnusena mudelisse. Sellisel juhul on meil tegemist tavalise mitmese regressiooniga (taolist mudelit, kus meil on nii pidev kui kategoriaalne sõltumatu tunnus, nimetatakse ka ANCOVA (Analysis of Covariane) mudeliks). Kui me kaasame mudelisse ka sõltumatute tunnuste vahelised interaktsioonid, saame hinnata ka seda kas erinevate gruppide regressioonikoefitsiendid erinevad (ehk siis kas gruppide regressioonisirgete tõusud erinevad). Kõnealuses näites see nii ka oli. Kui gruppe on natukene rohkem (näiteks 15 gruppi) ja meid huvitvad eelkõige vaid esimese tasandi mõjud (ehk siis meid ei huvita kuidas grupeeriv tunnus meie uuritavat mõju mõjutab või me eeldame, et ta seda ei mõjuta), siis saame kasutada fikseeritud mõjudega mudelit (fixed effects model). Näiteks tahame küsitlusandmetega uurida kuidas sissetulekute suurused mõjutavad eluga rahulolu, kuid kuna meie valim on üle-eestiline, siis peaksime arvestama piirkondlike (näiteks maakondlike) erinevustega sissetulekute suurustes. Meid ei huvita niiväga see sissetulekute erinevus, kuivõrd üldine sissetulekute mõju rahulolule. Seega tahame lihtsalt oma tulemust piirkondlike erinevuste suhtes kontrollida. Fikseeritud mõjudega mudel on tegelikult jällegi tavaline regressioonimudel, millesse on kaasatud grupeeriv kategoriaalne tunnus. Kategoriaalne sõltumatu tunnus on regressioonimudelis alati dummy-tunnusena, (see tähendab et kui meil on näiteks 15 kategooriat, siis peame sellest tegema (või no R teeb) 14 binaarset tunnust, mille kaudu meil on kõik 15 kategooriat defineeritud). Seega, kui tahame hinnata mingi sõltumatu pidevtunnuse mõju sõltuvale tunnusele ning andmestiku hierarhilise struktuuri tõttu lisame sinna ka 15 kategooriaga kategoriaalse tunnuse, siis saame mudeli tulemina 15 regressioonikoefitsienti (1 pidevtunnuse koefitsient ja 14 grupeeriva tunnuse koefitsienti). Muidugi võime ka neid neid kategoriaalsete tunnuste koefitsiente tõlgendada (need näitavad kuidas gruppide keskmised referentsgategooriast erinevad), kuid üldjuhul tahame sellise mudeliga lihtsalt oma uuritavat mõju grupeeriva tunnuse suhtes kontrollida (leida grupeerivast tunnusest sõltumatut mõju). Taolise mudeli puhul eeldame vaikimisi, et meie uuritav mõju on kõikide grupeerivate tunnuste lõikes sama (regressioonisirge tõus on kõikides gruppides sama) ja erinevad vaid gruppide keskmised (mudeli kontekstis vabaliikmed). Kui see nii ei ole, siis peaksime mudelisse kaasma ka koosmõjud. Kuid sellisel juhul palju õnne meile nende interaktsioonide tõlgendamiel. Kui lisaks grupeerivale tunnusele on mudelis vaid üks sõltumatu tunnus, siis oleks see tegelikult tehtav (eriti kui referentskategooria on hästi valitud), kuid üldiselt on meil ju mudelis rohkem kui üks sõltumatu tunnus. Kui meid huvitavad eelkõige teise tasandi mõjud, siis saame kasutada nn kaheastmeliste mudelit (two-stage model). Näiteks kui tahame teada kuidas omavalitsuste investeeringud infrastruktuuri mõjutavad nende elanike eluga rahulolu ja nende elanike sissetulekute mõju eluga rahulolule. Kaheastmeliste mudelite puhul moodustatakse igas klastris või grupis omaette regresioonimudel, seejärel võetakse nende mudelite koefitsiendid ning kasutatake neid omakorda sisenitena teise astme regressioonimudelis. Meie näite puhul peaksime igas KOVis moodustama mudeli, millega hindame sissetulekute mõju eluga rahulolule. Eesti puhul oleks siis vaja moodustada 79 mudelit. Seejärel võtame kõikide mudelite koefitsiendid ning kasutame neid teise tasandi mudelis sõltuva tunnusena. Teise tasandi sõltumatuks tunnuseks oleks KOVide investeeringute suurused. Taolise lähenemisega saame juhul, kui esimeses tasandis on piisavalt vaatlusi, küllaltki adekvaatse ning keerulisemate mudelitega võrreldava tulemuse. Mudeli kasuks räägib eelkõige see, et tegemist on väga lihtsa ja arusaadava moodusega teise tasandi effektide tuletamiseks. Kuid vägagi ilmsed on ka taolise lähenemise kitsaskohad. Gruppe kokku agregeerides kaotame väga palju informatsiooni. Ja mida vähem on meil informatsiooni, seda vähem kindlad me oma järeldustes olla saame. See reegel kehtib ka laiemalt - mida rohkem me oma algseid andmeid agregeerime, seda ebatäpsemad meie järeldused saavad olema. Kui meid huvitavad jällegi ainult esimese tasandi mõjud, kuid samas me teame, et meie andmestik on hierahiliselt struktureeritud (ja võib-olla meil ei ole selle hierarhilise struktuuri kohta väga palju infot), siis saame regressioonimudeli hindamisel kasutada nn robustseid standardvigu (robust standard errors, clustered standard errors). See on puhtalt analüütiline lahendus, mille kaudu me saame oma mudeli tulemuse võimaliku klasterdumise eest kindlustada. Viimaks on meil võimalik kasutada ka mitmetasandilist analüüsi/mudelit (multilevel analysis/models, nimetatakse ka hierarchical linear models, linear mixed-effect models, mixed models, nested data models, random coefficient models või random-effects models). Järgenvalt vaatamegi seda meetodit lähemalt. 6.2 Mitmetasandiline analüüs Mitmetasandiline analüüs võimaldab: ühiskonna või muu uurimisobjekti struktureeritust, heterogeensust ja kontekstuaalsust mõjude hindamisel arvesse võtta ja hinnata sotsiaalse, kultuurilise, institutsionaalse või muu konteksti mõju uuritavatele nähtustele ja uurimisobjektidele Eelkõige huvitab meid mitmetasandilise analüüsi juures just see teine punkt - gruppide erinevuste (nii grupisiseste mõjude kui gruppide keskmiste erinevuste) selgitamine kõrgema tasandi sõltumatute tunnuste abil. Mitmetasandiline analüüs on oma olemuselt sarnande eelnevalt kirjeldatud kaheastmelisele mudelile, kus saime teise tasandi tunnuste abil (näiteks KOVide investeeringute suurused) seletada gruppidevahelist regressioonikoefitsientide varieeruvust. Kuid erinevalt kaheastmelisest mudelist ei agregeerita siin gruppide koefitsiente enne teise tasandi juurde asumist, vaid käsitletakse esimese ja teise (või ka kolmana või neljana vne) tasandite variatiivsust samas mudelis. Seega ei kaota me mudeli hindamisel meile vajalikku informatsiooni ning järeldused saavad olla täpsemad. 6.2.1 Fikseeritud ja juhuslikud efektid Mitmetasandilises analüüsis käsitleme regressioonikoefitsiente ja vabaliiget mitte konkreetsete ja fikseeritutena, vaid vaid mingi teise tasandi grupeeriva tunnuse lõikes varieeruvatena. Regressiooni koefitsientides (vabaliige ja regressioonikordajad) eristatakse fikseeritud (fixed) ja juhuslikku (random) osa: fikseeritud osa on koefitsientide gruppideülesed üldkeskmised; juhuslik osa näitab gruppide koefitsientide varieeruvust ümber üldkeskmiste. Mida see sisuliselt tähendab? Oletame, et meil on regressioonivõrrand iga grupi kohta: \\[Y_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij}+\\epsilon_{ij}\\] kus: \\(Y_{ij}\\) on esimese tasandi sõltuva tunnuse väärtus indiviidi \\(i\\) ja grupi \\(j\\) jaoks; \\(\\beta_{0j}\\) on grupi \\(j\\) vabaliige; \\(\\beta_{1j}\\) on regressioonikordaja grupis \\(j\\); \\(X_{ij}\\) on esimese tasandi sõltumatu tunnuse väärtus grupi \\(j\\) kuuluva indiviidi \\(i\\) jaoks; \\(\\epsilon_{ij}\\) on regressiooni jääk (viga) indiviidi \\(i\\) jaoks, kes kuulub gruppi \\(j\\). Me saame gruppide vabvaliikmetes eristada gruppideülest keskmist vabaliiget \\(\\gamma_{00}\\) ja iga grupi hälbimust sellest keskmisest \\(u_{0j}\\) (ehk siis iga grupi erinevust üldkeskmisest vabaliikmest): \\[Y_{ij} = \\underbrace{\\beta_{0j}}_{\\beta_{0j}= \\gamma_{00}+ u_{0j}} + \\beta_{1j}X_{ij}+\\epsilon_{ij}\\] kus: \\(\\gamma_{00}\\) on esimese tasandi vabaliikmete keskmine ehk vabaliikme fikseeritud osa; \\(u_{0j}\\) on gruppide erinevus keskmisest vabaliikmest ehk vabaliikme juhuslik osa. Samamoodi saame gruppide regressioonikoefitsientide puhul eristada gruppideülest keskmist koefitsienti \\(\\gamma_{10}\\) ja iga grupi hälbimust sellest keskmisest \\(u_{1j}\\): \\[Y_{ij} = \\beta_{0j} + \\underbrace{\\beta_{1j}}_{\\beta_{1j}= \\gamma_{10}+ u_{1j}}X_{ij}+\\epsilon_{ij}\\] kus: \\(\\gamma_{10}\\) on esimese tasandi regressioonikoefitsientide gruppideülene keskmine ehk regressioonikordaja fikseeritud osa; \\(u_{1j}\\) on gruppide erinevus keskmisest regressioonikordajast ehk regrssioonikordaja juhuslik osa. Kuna nii vabaliikme kui regressionikoefitseindi puhul on meil nüüd varieeruvad juhuslikud osad (võime neist mõelda kui tavalise regressiooni sõltvatest tunnustest), siis saame nende variatiivsust teise tasandi sõltumatute tunnuste abil regressioonga selgitada. Lisame teise tasandi sõltumatu tunnuse \\(W_j\\), millega üritame seletada esimese tasandi vabaliikme varieeruvust: \\[Y_{ij} = \\underbrace{\\beta_{0j}}_{\\beta_{0j}= \\gamma_{00}+ \\gamma_{01}W_j + u_{0j}} + \\beta_{1j}X_{ij}+\\epsilon_{ij}\\] kus: \\(\\gamma_{01}\\) on regressioonikordaja, mille läbi teise tasandi sõltumatu tunnus \\(W_j\\) selgitab esimese tasandi vabaliikmete varieeruvust; \\(u_{0j}\\) on teise tasandi regressioonivõrrandi viga ehk siis see osa vabaliikmete varieeruvusest, mida me teise tasandi sõltumatu tunnusega selgitada ei suuda. Täpselt samamoodi saame toimida ka regressioonikoefitseindi juhusliku osaga, mille varieeruvust on jällegi võimalik teise tasandi sõltumatu tunnuse \\(W_j\\) abil selgitada: \\[Y_{ij} = \\beta_{0j} + \\underbrace{\\beta_{1j}}_{\\beta_{1j}= \\gamma_{10}+ \\gamma_{11}W_j + u_{1j}}X_{ij}+\\epsilon_{ij}\\] kus: \\(\\gamma_{11}\\) on regressioonikordaja, mille läbi teise tasandi sõltumatu tunnus \\(W_j\\) selgitab esimese tasandi regressioonikoefitseintide varieeruvust; \\(u_{1j}\\) on teise tasandi regressioonivõrrandi viga ehk siis see osa regressioonikoefitseintide varieeruvusest, mida me teise tasandi sõltumatu tunnusega selgitada ei suuda. Kui me nüüd nii vabaliikmete kui ka regressioonikoefitsientide fikseeritud ja juhuslikud osad kokku paneme: \\[Y_{ij} = \\underbrace{\\beta_{0j}}_{\\gamma_{00}+ \\gamma_{01}W_j + u_{0j}} + \\underbrace{\\beta_{1j}}_{\\gamma_{10}+ \\gamma_{11}W_j + u_{1j}}X_{ij}+\\epsilon_{ij}\\] siis saamegi mitmetasandilise regressiooni võrrandi: \\[Y_{ij} = \\gamma_{00}+ \\gamma_{01}W_j + \\gamma_{10}X_{ij}+ \\gamma_{11}W_jX_{ij} + u_{0j} + u_{1j}X_{ij} + \\epsilon_{ij}\\] kus võime eristada fikseeritud osa ja juhuslikku osa: \\[Y_{ij} = \\underbrace{\\gamma_{00}+ \\gamma_{01}W_j + \\gamma_{10}X_{ij}+ \\gamma_{11}W_jX_{ij}}_{\\text{fikseeritud osa}} + \\underbrace{u_{0j} + u_{1j}X_{ij} + \\epsilon_{ij}}_{\\text{juhuslik osa}}\\] Võrrand võib esmapilgul väga kirju ja segane tunduda, kuid kui eelnev tuletuskäik rahulikult läbi mõelda ning kreeka tähtedest end mitte häirida lasta, siis on kõik tegelikult väga loogiline ning võiks neile, kes tavalise lineaarse regressiooniga tuttavad on, mõistetav olla. 6.2.2 Mitmetasandilise analüüsi etapid Mitmetasandilisi mudeleid koostatakse üldjuhul etapiviisiliselt, alustades lihtsamast mudelist ning liikudes edasi keerulisemate mudelite poole: Esmalt hinnatakse nullmudel (null model või empty model), millega hinnatakse vaid gruppidest tulenevat esimese tasandi sõltuva tunnuse varieeruvust. Kui esimese tasandi sõltuv tunnus gruppide vahel statistiliselt oluliselt ei varieeru, siis ei ole mõtet mitmetasandilise analüüsiga edasi minna (kuna siis me ju ei saa tasandeid eristada). \\[Y_{i,j} = \\gamma_{00} + u_{0j} + \\epsilon_{ij}\\] Kui nullmudel indikeerib gruppidevahelist erinevust, siis hinnatakse juhusliku vabaliikmega mudel (random intercept model), kus lisatakse esimese tasandi sõltumatu tunnus ning hinntakse vabaliikme varieeruvust gruppide vahel. \\[Y_{i,j} = \\gamma_{00}+ \\gamma_{11}X_{ij} + u_{0j} + \\epsilon_{ij}\\] Kui ka juhusliku vabaliikmega mudel näitab, et vabaliikmed varieeruvad gruppide vahel, siis saab moodustada juhusliku regressioonikordajaga mudeli (random slope model), kus lastakse ka esimese tasandi regressioonikordajad gruppide vahel varieeruma. \\[Y_{i,j} = \\gamma_{00}+ \\gamma_{11}X_{ij} + u_{0j} + u_{1j}X_{ij} + \\epsilon_{ij}\\] Kui vabaliikmed või regressioonikordajad varieeruvad olulisel määral, siis saame vajadusel ja võimalusel nende variatsiooni mingite teise tasandi sõltumatute tunnustega selgitada, ehk siis lülitada mudelisse ka teise tsandi prediktorid (kui näeme variatiivsust vabaliikmete hulgas, kuid mitte regressioonikordajate hulgas, siis saame teise tasandi prediktoreid kasutada loomulikult vaid vabaliikmete selgitamiseks). Teise tasandi prediktor selgitamas vabaliikme varieeruvust: \\[Y_{i,j} = \\gamma_{00}+ \\gamma_{01}W_j + \\gamma_{10}X_{ij} + u_{0j} + u_{1j}X_{ij} + \\epsilon_{ij}\\] Teise tasandi prediktor selgitamas nii vabaliikme kui regressioonikordaja varieeruvust (esimese ja teise tsandi prediktorite koosmõju): \\[Y_{i,j} = \\gamma_{00}+ \\gamma_{01}W_j + \\gamma_{10}X_{ij}+ \\gamma_{11}W_jX_{ij}+ u_{0j} + u_{1j}X_{ij} + \\epsilon_{ij}\\] Vaatame selgitava näitena ühe uneuuringu raames tehtud eksperimendi andmestikku23. Eksperimendiga uuriti kuidas unepuudus reaktsioonikiirust mõjutab. Respondentide reaktsioonikiirust mõõdeti kümnel järjestikusel päeval. Enne esimest mõõtmist (0 päev) lasti respondetidel magada nii kaua nagu nad tavaliselt magavad. Kõikidel järgevatel öödel lasti neil magada kolm tundi. Tulemustena esitatav reaktsioonikiirus on keskmine reaktsiooniaeg erinevate testide alusel, mis päeva jooksul sooritati. Andtud juhul on siis tegemist longituudandmetikuga (kutsutakse ka paneelandmestikuks), kus teise tasandi ühikuks on respondent ning esimese tasandi vaatlused on grupeeritud indiviidide kaupa. Vaatame kõigepealt kuidas näeks välja tavaline lineaarne regressioon: Pilt näeb välja küllaltki ettearvatav - mida pikemalt und piiratakse, seda suuremaks läheb reaktsiooniaeg. Kuid kuna me teame, et tegemist on paneelandmetega, siis tagamaks oma järelduste korrektsust, peame vähemalt kontrollima teise tasandi efektide olemasolu ja potentsiaalset mõju. Defineerime nullmudeli: Näeme, et inimeste keskmised reaktsiooniajad (võtmata arvesse undefitsiidi pikkust) erinevad päris olulisel määral (reaalses analüüsis peame muidugi sellele järeldusele jõudma teststatistiku abil). Seega on õigustatud juhusliku vabaliikmega mudeli defineerimine, kus me jätame regressioonikordajad kõikidel respondentidel samaks, kuid laseme vabaliikmed varieeruma: Näeme, et ka vabaliikmed varieeruvad indiviidide vahel päris suurel määral ning liigume edasi juhuliku regressioonikordajaga mudeli juurde: Ka regressioonikordajad varieeruvad indiviidide vahel päris olulisel määral. Ühe inimese puhul unedefitsiit isegi vähendab reaktsiooniaega ja päris mitme puhul on muutused üle aja marginaalsed, eristudes seeläbi keskmisest reaktsioonikiirusest. Edasi saaksime vaadata millised individuaalsed karakteristikud seda mõju erinevust mõjutavad. Kuna tegemist on paneeluuringuga, kus teise tasandi ühikuks on indiviid, siis on ka võimalikud teise tasandi sõltumatud tunnused indiviiditunnused. Näiteks sugu, vanus vms. 6.3 Mitmetasandilise analüüsi eeldused Esimese tasandi ühikud peavad moodustama teise tasandi ühikute suhtes (pseudo)populatsiooni Teise tasandi ühikud on valim teise tasandi ühikute populatsioonist Kas teise tasandi ühikuid saab mõista juhuvalimina? Kas teise tasandi ühikuid on piisavalt palju (vähemalt 20, parem kui vähemalt 50), et nende põhjal järeldusi teha? Esimese tasandi jäägid peaksid olema jaotunud normaaljaotuse alusel (keskmisega 0) Jääkide dispersioon peaks gruppide lõikes võrdne olema Teise tasandi jäägid peaksid esimese tasandi jääkidest sõltumatud olema Tavalised lineaarse regressiooni eeldused Sõltuva ja sõltumatute tunnuste seosed peaksid olema lineaarsed Sõltumatud tunnused peaksid olema mõõdetud ilma mõõtmisveata 6.4 Mitemtasandiline analüüs Ris Ris on mitmetasandilise analüüsi teostamiseks mitmeid pakette. Kasutame siinkohal lme4 paketti ja sellest lmer() funktsiooni. lme4 pakett ei anna meile koefitsientide p-väärtusi. Et neid saada, tuleb laadida ka pakett lmerTest24. Kõigepealt laeme sessiooniks sisse vajalikud paketid (kui need ei ole installitud, siis tuleb seda teha käsuga install.packages()) library(nlme) # siit saame andmed library(ggplot2) # joonised library(dplyr) # andmete töötlemine library(lme4) # mitmetasanilise analüüsi pakett library(lmerTest) # võimaldab mudelile ka p väärtused külge saada Kasutame andmestikuna MathAchieve näidisandmestikku paketist nlme. Andmestik sisaldab USAs 1982. aastal läbi viidud küsitluse High School and Beyond tulemusi hõlmates 7185 keskkoliõpilast 160-st koolist. Ehk siis esimene tasand on õpilased ja teine tasand koolid. Andmestik sisaldab muu hulgas õpilasete perede sotsiaalmajanduslikku seisu indeksit ning matemaatikatesti tulemust. Üritame analüüsida kas pere sotsiaalmajanduslik seis mõjutab matemaatikatesti tulemust ja kas see seos varieerub koolide vahel ning kui jah, siis leida seda variatiivsust selgitavad tegurid. 6.4.1 Andmete ettevalmistus Anname andmetele natukene lihtsama nime ja vaatame millega tegu: andmed &lt;- nlme::MathAchieve %&gt;% as.data.frame() str(andmed) ## &#39;data.frame&#39;: 7185 obs. of 6 variables: ## $ School : Ord.factor w/ 160 levels &quot;8367&quot;&lt;&quot;8854&quot;&lt;..: 59 59 59 59 59 59 59 59 59 59 ... ## $ Minority: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Sex : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 2 2 1 1 1 1 2 1 2 1 ... ## $ SES : num -1.528 -0.588 -0.528 -0.668 -0.158 ... ## $ MathAch : num 5.88 19.71 20.35 8.78 17.9 ... ## $ MEANSES : num -0.428 -0.428 -0.428 -0.428 -0.428 -0.428 -0.428 -0.428 -0.428 -0.428 ... Tunnused on järgmised: School - kooli id Minority - kas õpilane kuulub vähemuse hulka Sex - sugu SES - õpilase pere sotsialmajanduslik staatus (standardiseeritud) MathAch - matemaatikatesti tulemus MEANSES - kooli keskmine SES skoor (teise ehk kooli tasandi tunnus) Lisaks on nlme paketis ka andmestik MathAchSchool, mis sisaldab veel erinevaid kooli tasandi tunnuseid. Võtame sellest andmestikus tunnused: School - koolid id Sector - kas Public või Chatholic kool PRACAD - protsent õpilastest, kes õpivad nn akadeemilisel suunal MEANSES - kooli keskmine SES skoor koolid &lt;- nlme::MathAchSchool %&gt;% as.data.frame() %&gt;% select(School, Sector, PRACAD, MEANSES) head(koolid) ## School Sector PRACAD MEANSES ## 1224 1224 Public 0.35 -0.428 ## 1288 1288 Public 0.27 0.128 ## 1296 1296 Public 0.32 -0.420 ## 1308 1308 Catholic 0.96 0.534 ## 1317 1317 Catholic 0.95 0.351 ## 1358 1358 Public 0.25 -0.014 Taoline mitmetasandiliste andmete organiseerimise viis, kus erinevate tasandite tunnused on erinevates failides, on küllaltki tavapärane. Seega esimeseks sammuks on nii siin, kui ka mitmetasanilise analüüsi puhul üldiselt need kaks andmestikku kokku liita. Kasutame selleks dplyr-i left_join() funktsiooni. Mõlemat andmestikku ühendavaks id tunnuseks on School. Kuid kuna need tunnused on erinevatest klassidest (üks on ordered factor, teine tavaline factor), siis teeme nad mõlemad kõigepealt character tunnusteks andmed$School &lt;- as.character(andmed$School) koolid$School &lt;- as.character(koolid$School) # Nüüd saame andmestikud kokku panna andmed &lt;- left_join(andmed, koolid, by = &#39;School&#39;) # Vaatame ka tulemust head(andmed) ## School Minority Sex SES MathAch MEANSES.x Sector PRACAD MEANSES.y ## 1 1224 No Female -1.528 5.876 -0.428 Public 0.35 -0.428 ## 2 1224 No Female -0.588 19.708 -0.428 Public 0.35 -0.428 ## 3 1224 No Male -0.528 20.349 -0.428 Public 0.35 -0.428 ## 4 1224 No Male -0.668 8.781 -0.428 Public 0.35 -0.428 ## 5 1224 No Male -0.158 17.898 -0.428 Public 0.35 -0.428 ## 6 1224 No Male 0.022 4.583 -0.428 Public 0.35 -0.428 Kuna MEANSES tunnus oli mõlemas andmestikus, siis on see uues andmestikus kaks korda (vastavalt suffixitega .x ja .y). Kustutame neist esimese ja muudame MEANSES.y nime tagasi MEANSES-iks. andmed$MEANSES.x &lt;- NULL andmed &lt;- andmed %&gt;% rename(MEANSES = MEANSES.y) Paneme kõik tunnuste nimed väikestesse tähtedesse25: names(andmed) &lt;- tolower(names(andmed)) 6.4.2 Ülevaade andmetest Nagu enne igat analüüsi, uurime kõigepealt andmeid. Vaatame graafiliselt kuidas sotsiaalmajanduslik indeks matemaatika testi tulemusega seostub: # punktide jaoks geom_point() # mittelineaarse regressioonijoone jaoks geom_smooth() andmed %&gt;% ggplot(aes(x = ses, y = mathach))+ geom_point(alpha = 0.2)+ geom_smooth()+ theme_bw() Tundub et üldine seos on täitsa olemas ja see on ka suhteliselt lineaarne. Järgmiseks vaatame kas koolide keskmised matemaatikasjoorid erinevad. Neid oleks hea vaadata koos usalduspiiridega, seega peame need enne välja arvutame. Andmete töötlemiseks kasutame dplyr-i funktsioone: andmed %&gt;% group_by(school) %&gt;% summarise(keskmine = mean(mathach), sd = sd(mathach), n = n()) %&gt;% mutate(l_ci = keskmine - 1.96 * sd/sqrt(n), u_ci = keskmine + 1.96 * sd/sqrt(n)) %&gt;% mutate(school = as.factor(school), school = reorder(school, keskmine)) %&gt;% ggplot(aes(x = keskmine, xmin = l_ci, xmax = u_ci, y = school))+ geom_pointrange()+ theme_minimal()+ theme(panel.grid.minor = element_blank(), panel.grid.major.y = element_blank(), axis.text.y = element_blank()) Jah, keskmised tunduvad kooliti erinevat. On palju koole mille usalduspiirid ei kattu. Järgmiseks vaatame, kas ka sotsiaalmajandusliku seisundi ja testi tulemuste seosed kooliti erinevad: andmed %&gt;% ggplot(aes(x = ses, y = mathach))+ geom_point(alpha = 0.2)+ geom_smooth(aes(color = school), se = F, show.legend = F, method = &#39;lm&#39;)+ theme_bw()+ scale_colour_viridis_d(option = &#39;A&#39;) Tundub, et ka regressioonikoefitsiendid erinevad. Vaatame põgusalt üle ka teiste sõltumatute tunnuste jaotused. Eelkõige seetõttu, et kontrollida neid võimalike vigade suhtes. table(andmed$sex, useNA = &#39;always&#39;) ## ## Male Female &lt;NA&gt; ## 3390 3795 0 hist(andmed$ses) table(andmed$sector, useNA = &#39;ifany&#39;) ## ## Public Catholic ## 3642 3543 hist(andmed$pracad) hist(andmed$meanses) Interpretatsiooni huvides oleks regressioonimudelis mõistlik kasutada sotsiaalmajanduslikku indeksi tunnust, mis on koolide keskmiste alusel tsentreeritud. Kuna vabaliige on sõltuva tunnuse (\\(y\\)) väärtus juhul, kui sõltumatu tunnus (\\(x\\)) on 0, siis ilma tsentreerimata on vabaliikme väärtus suhteliselt sisutühi. Kui me selle koolide keskmiste lõikes tsentreerime, näitab vabaliige kooli matemaatikatesti tulemust kooli keskmise sotsiaalmajandusliku indeksi väärtuse korral. andmed &lt;- andmed %&gt;% mutate(tses = ses - meanses) 6.4.3 Nullmudel Esmalt defineerime nullmudeli: m0 &lt;- lmer(mathach ~ 1 + (1|school), data = andmed) lmer mudeli defineerimine on natukene erinev tavapärasest ml või glm mudeli defineerimisest. athach ~ 1 - sõltuv tunnus koos tildega ja 1 tähistamas vabaliiget. See on identne ml mudeliga. Tavaliselt me lihtsalt ml mudelis vabaliiget eraldi välja ei too, kuna ainult vabaliikmega ml mudeli tulem oleks lihtsalt sõltuva tunnuse keskmine. Aga kuna siin mudelis meil ühtegi sõltumatut tunnust veel ei ole, siis peame vabaliikme eksplitsiitselt ära märkima. (1|school) - sulgude sees kirjeldatud mudeli juhuslik osa, ehk siis need tunnused, mida me tahame varieeruvatena näha. 1 tähistab jällegi vabaliiget, mis tähendab, et me laseme gruppide vabaliikmetel varieeruda. | märgi järel tuleb mudeli grupeeriv tunnus. Antud juhul on selleks school. Vaatame mudeli tulemusi: summary(m0) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: mathach ~ 1 + (1 | school) ## Data: andmed ## ## REML criterion at convergence: 47116.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.0631 -0.7539 0.0267 0.7606 2.7426 ## ## Random effects: ## Groups Name Variance Std.Dev. ## school (Intercept) 8.614 2.935 ## Residual 39.148 6.257 ## Number of obs: 7185, groups: school, 160 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.6370 0.2444 156.6473 51.71 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Peamised tulemused on toodud Random effects: ja Fixed effects: kirjete all. Koolide keskmine matemaatikatesti tulemus on 12.6 Koolide keskmiste dispersioon üldkeskmisest testitulemusest on 8.6 (ja keskmine hälbimus 2.9). Õpilaste keskmine dispersioon koolide keskmistest testitulemustest on 39.1 (ja keskmine hälbimus 6.3). Nende tulemuste alusel saame arvutda intraklassi korrelatsiooni (interclass correlatsion ehk ICC), mis näitab mitu protsenti sõltuva tunnuse varieeruvusest on selgitatav grupitunnuse poole 8.614/(8.614+39.148) ## [1] 0.1803526 ca 18% testitulemuste variatiivusest on selgitatav koolie erinevustega. 6.4.4 Juhusliku vabaliikmega mudel Lisame mudelile ka sotsiaalmajandusliku indeksi (tsentreeritud variandi), mis läbi saame tulemuseks mudeli kus vabaliikmed gruppide vahel varieeruvad: m1 &lt;- lmer(mathach ~ 1 + tses + (1|school), data = andmed) summary(m1) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: mathach ~ 1 + tses + (1 | school) ## Data: andmed ## ## REML criterion at convergence: 46724 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.09686 -0.73224 0.01941 0.75720 2.91473 ## ## Random effects: ## Groups Name Variance Std.Dev. ## school (Intercept) 8.672 2.945 ## Residual 37.010 6.084 ## Number of obs: 7185, groups: school, 160 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.6493 0.2445 156.7433 51.74 &lt;2e-16 *** ## tses 2.1912 0.1087 7022.0245 20.17 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## tses 0.003 Sotsiaalmajandusliku indeksi efekti suurus on 2,2 ja mõju on statistiliselt oluline (p &lt; 0.05). Ehk siis kui sotsiaalmajanduslik indeks kasvab ühe punkti võrra, kasvab matemaatikatesti tulemus 2,2 punkti võrra (võttes arvesse ka koolide erinevust). Vabaliikmete dispersioon on 8.672 (ja standardhälve 2,9). Esimese tasandi vead läksid natuke väiksemaks (39,1 &gt; 37,0), mis on ka loogiline, kuna iga lisanduv sõltumatu tunnus peaks regressioonijääke vähendama. Saame mudeleid võrrelda anova() funktsiooni ja LRT-testiga: anova(m0, m1) ## Data: andmed ## Models: ## m0: mathach ~ 1 + (1 | school) ## m1: mathach ~ 1 + tses + (1 | school) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## m0 3 47122 47142 -23558 47116 ## m1 4 46728 46756 -23360 46720 395.4 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 m1 mudel on statistiliselt oluliselt parem kui m0 mudel. 6.4.5 Juhusliku regressioonikordajaga mudel Laseme sotsiaalmajandusliku indeksi regressioonikorda samuti vabalt varieeruma. Selleks lisame vastava tunnuse, mille koefitsiente tahame vabaks lasta juhuslike efektide sulgudesse: m2 &lt;- lmer(mathach ~ 1 + tses + (1+tses|school), data = andmed) summary(m2) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: mathach ~ 1 + tses + (1 + tses | school) ## Data: andmed ## ## REML criterion at convergence: 46714.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.09680 -0.73194 0.01858 0.75388 2.89928 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## school (Intercept) 8.682 2.9465 ## tses 0.694 0.8331 0.02 ## Residual 36.700 6.0581 ## Number of obs: 7185, groups: school, 160 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.6493 0.2445 156.7391 51.73 &lt;2e-16 *** ## tses 2.1932 0.1283 155.2180 17.10 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## tses 0.012 Sotsiaalmajandusliku indeksi regresioonikordaja dispersioon on 0.69 (ja standardhälve 0.83) Juhuslike effektide jaoks meil väljundis mingit olulisuse testi ei ole. Aga saame kasutada lmerTest paketi ranova() funktsiooni, mis testib erinevate juhuslike efektide panust mudelisse: ranova(m2) ## ANOVA-like table for random-effects: Single term deletions ## ## Model: ## mathach ~ tses + (1 + tses | school) ## npar logLik AIC LRT Df Pr(&gt;Chisq) ## &lt;none&gt; 6 -23357 46726 ## tses in (1 + tses | school) 4 -23362 46732 9.7617 2 0.00759 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Sotsiaalmajandusliku indeksi effekt on oluline (p &lt; 0.05). Saame seda testida ka LRT-testiga: anova(m1,m2) ## Data: andmed ## Models: ## m1: mathach ~ 1 + tses + (1 | school) ## m2: mathach ~ 1 + tses + (1 + tses | school) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## m1 4 46728 46756 -23360 46720 ## m2 6 46723 46764 -23356 46711 9.4331 2 0.008946 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.4.6 Teise tasandi sõltumatud muutujad Lisame teise tasandi sõltumatu muutujana kooli keskmise sotsiaalmajandusliku indeksi ja kooli tüübi: m3 &lt;- lmer(mathach ~ 1 + tses + meanses + sector + (1+tses|school), data = andmed) summary(m3) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: mathach ~ 1 + tses + meanses + sector + (1 + tses | school) ## Data: andmed ## ## REML criterion at convergence: 46543.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.1719 -0.7279 0.0123 0.7562 2.9307 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## school (Intercept) 2.3878 1.5453 ## tses 0.7007 0.8371 0.18 ## Residual 36.7097 6.0589 ## Number of obs: 7185, groups: school, 160 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.0451 0.1986 160.3808 60.639 &lt; 2e-16 *** ## tses 2.1950 0.1284 154.8896 17.092 &lt; 2e-16 *** ## meanses 5.2463 0.3683 151.3246 14.244 &lt; 2e-16 *** ## sectorCatholic 1.3722 0.3055 149.8780 4.491 1.4e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) tses meanss ## tses 0.060 ## meanses 0.245 -0.004 ## sectorCthlc -0.696 0.002 -0.356 Kui kooli keskmine sotsiaalmajanduslik indeks tõuseb 1 punkti võrra, siis tõuseb kooli keskmine matemaatikatesti tulemus 5.9 punkti võrra. Katoliiklikus koolis on keskmine matemaatika testitulemus ca 1.4 punkti parem kui tavakoolis. anova(m2, m3) ## Data: andmed ## Models: ## m2: mathach ~ 1 + tses + (1 + tses | school) ## m3: mathach ~ 1 + tses + meanses + sector + (1 + tses | school) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## m2 6 46723 46764 -23356 46711 ## m3 8 46554 46609 -23269 46538 172.84 2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.4.7 Esimese ja teise tasandi sõltumatute tunnuste koosmõjud Lõpuks lisame mudelisse ka esimese tasandi sõltumatu tunnuse (tses) ja teise tasandi sõltumatute tunnuste (meanses ja sector) koosmõjud, mille abil saame hinnata kas teise tasandi tunnused mõjutavbad esimese tasandi mõjusid: m4 &lt;- lmer(mathach ~ 1 + tses * meanses + tses * sector + (1+tses |school), data = andmed) summary(m4) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: mathach ~ 1 + tses * meanses + tses * sector + (1 + tses | school) ## Data: andmed ## ## REML criterion at convergence: 46503.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.15921 -0.72319 0.01706 0.75439 2.95822 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## school (Intercept) 2.3819 1.5433 ## tses 0.1014 0.3184 0.39 ## Residual 36.7211 6.0598 ## Number of obs: 7185, groups: school, 160 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 12.1136 0.1988 159.8921 60.931 &lt; 2e-16 *** ## tses 2.9388 0.1551 139.3043 18.948 &lt; 2e-16 *** ## meanses 5.3391 0.3693 150.9689 14.457 &lt; 2e-16 *** ## sectorCatholic 1.2167 0.3064 149.5994 3.971 0.000111 *** ## tses:meanses 1.0389 0.2989 160.5528 3.476 0.000656 *** ## tses:sectorCatholic -1.6426 0.2398 143.3450 -6.850 2.01e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) tses meanss sctrCt tss:mn ## tses 0.080 ## meanses 0.245 0.020 ## sectorCthlc -0.697 -0.056 -0.356 ## tses:meanss 0.019 0.282 0.079 -0.028 ## tss:sctrCth -0.056 -0.694 -0.029 0.082 -0.351 tses koefitsient 2.94 on nüüd sotsiaalmajandusliku indeksi keskmine effekt testitulemusele tavakoolis. Katoliiklikus koolis on see efekt 1.64 võrra väiksem. Ehk siis katoliiklikus koolis ei mõjuta pere sotsiaalmajanduslik taust õpitulemusi nii palju kui tavakoolis. Mida kõrgem on kooli keskmine sotsiaalmajanduslik indeks suurem mõju on pere majanduslikul taustal õpitulemustele. Ühe punktine kasv kooli keskmises tähendab 1.049 punktist mõju kasvu. Tegemist on reaalsete andmetega, kuid olen nende tähendust diaktilistel kaalutlustel muutnud. Reaalselt on need Iris andmestiku andmed, \\(x\\)-teljel on iiriste kroonlehtede pikkus ja \\(y\\)-teljel kroonlehtede laius (vt help(iris)) Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 112. Andmestik on kättesaadav paketis lme4 nimega sleepstudy Tegelikult piisab ka lihtsalt lmerTest paketi laadimisest, kuna see laeb automaatselt ka lme4 paketi. Lihtsalt, et oleks kergem neid trükkida "],["valimiuuringud.html", "Peatükk 7 Valimiuuringud 7.1 Tõenäosuslik valim (probability sample) 7.2 Kaalud 7.3 Valimidisain 7.4 Tulemuste valimidisaini suhtes korrigeerimine 7.5 Küsitlusandmed Ris 7.6 Valimiandmete analüüs 7.7 Plausible values", " Peatükk 7 Valimiuuringud 7.1 Tõenäosuslik valim (probability sample) Järeldava statistika kontseptsioonid eeldavad üldjuhul alati nn juhuvalimit. Juhuslikkus tähedab siikohal, et kõikidel üldkogumi liikmetel on valimisse sattumiseks võrdne tõenäosus. Suurte numbrite seadus (the law of large numbers) viitab, et ükskõik millise üldkogumi kohta vjärelduste tegemiseks oleks meil sellest üldkogumist vaja umbes 1000 vaatluselist valimit. Kui me küsiksime Eestis 1000 inimese käest nende sissetuleku suurust, siis 1000 inimese keskmine sissetulek peaks olema küllaltki lähedane terve Eesti keskmise sissetulekuga. Seda aga ainult juhul, kui need 1000 inimest on valitud juhuslikult (kõikidel eestlastel peaks olema võrdne võimalus sattuda nende 1000 sekka). Mis juhtuks, kui me juhusliku valimi asemel võtaksime hoopis igast maakonnast 67 inimest (Eestis on 15 maakonda, seega \\(1000 \\div 15 \\approx 67\\)). Tõenäoliselt me alahindaksime keskmist sissetulekut oluliselt. Kõige suuremate sissetulekutega ja samas kõige suurema rahvaarvuga maakond on Harjumaa (598059 inimest 2019 aastal). Kõige väiksemate sissetulekutega ja ka rahvaarvult kõige väiksem maakond on Hiiu maakond (9387 inimest 2019 aastal). Kui me käsitleme mõlemat maakonda võrdselt, siis keskmise arvutamisel võtaksime Hiiu maakonna väikesed sissetulekud arvesse ebaproportsionaalselt suurel määral ja Harjumaa kõrgemad sissetulekud ebaproportsionaalselt väikesel määral (Hiiumaa on üleesindatud ja Harjumaa on alaesindatud). Meie maakondade põhine valim ei oleks enam üldkogumi suhtes representatiivne. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-982b24ac{}.cl-98229382{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-9822976a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9822976b{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-9822e710{width:134.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9822e711{width:122.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9822e712{width:67.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9822e713{width:122.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9822e714{width:134.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9822e715{width:67.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9822e716{width:122.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9822e717{width:67.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9822e718{width:134.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9822e719{width:122.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-9822e71a{width:134.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-98230e16{width:67.6pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-98230e17{width:134.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-98230e18{width:122.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-98230e19{width:67.6pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Tabel 7.1: Eesti maakondade keskmised brutopalgad 2019 (Statistikaamet) MaakondKeskmine brutokuupalkRahvaarvHarju maakond1 531.0598 059.0Hiiu maakond993.09 387.0Ida-Viru maakond1 147.0136 240.0Jõgeva maakond1 066.028 734.0Järva maakond1 192.030 286.0Lääne maakond1 274.020 507.0Lääne-Viru maakond1 095.059 325.0Põlva maakond1 140.025 006.0Pärnu maakond1 172.085 938.0Rapla maakond1 200.033 311.0Saare maakond1 082.033 108.0Tartu maakond1 426.0152 976.0Valga maakond1 058.028 370.0Viljandi maakond1 201.046 371.0Võru maakond1 113.035 782.0 Kui me arvutaksime Eesti keskmise sissetuleku lähtuvalt maakondade keskmistest sissetulekutest, siis saaksime vastuseks: \\[\\frac{1531 + 993 + ... + 1113}{15} = 1179\\] Statistikaameti järgi oli Eesti keskmine sissetulek 2019 aastal 1407 eurot. Seega tõepoolest, hinnang alahindab oluliselt tegelikku keskmist sissetulekut. Võttes oma valimisse igast maakonnast 67 inimest, ei ole meie valim küll enam juhuvalim, kuid niikaua kuni me teame kõikide maakondade rahvaarvu, on see siiski tõenäosuslik valim (antud juhul stratifitseeritud valim). Seda seetõttu, et kõikidel Eesti inimestel on ikkagi võimalus sellesse valimisse sattuda. Harjumaa inimestel küll väiksem ja Hiiumaa inimestel suurem, kuid mingi võimalus on kõigil. Kui me nüüd seda erinevat valimisse sattmise tõenäosust teame, siis on meil võimalik mitte-representatiivne valim representatiivseks muuta. Selleks tuleb meil Hiiumaa elanikud väiksemaks ja Harjumaa elanikud suuremaks kaaluda. Seejärel saame (mõningate mööndustega) jälle kasutada järeldava statistika meetodeid ning teha nende alusel korrektseid järeldusi üldkogumi kohta. 7.2 Kaalud 7.2.1 Valimi kaalud Kõikide Harjumaa elanike jaoks oleks tõenäosus meie valimisse sattuda \\(67 \\div 598059 \\approx 0.0001120\\). Kõigi Hiiumaa elanike jaoks oleks see tõenäosus \\(67 \\div 9387 \\approx 0.0071375\\). Harjumaa valimis esindaks iga valimisse sattunu \\(1 \\div 0.00011 \\approx 8926\\) inimest ja Hiiumaal \\(1 \\div 0.0071 \\approx 140\\) inimest. Seega kui me teame iga inimese valimisse sattumise tõenäosust \\(\\pi\\), siis saame välja arvutada ka selle inimese valimi kaalud \\(\\frac{1}{\\pi}\\) (nimetatakse ka disainikaaludeks ehk design weights või ka base weights). Valimikaalude suurus on pöördvõrdeline valimisse sattumise tõenäosusega: \\[w_i = \\frac{1}{\\pi_i}\\] kusjuures üldkogumi suurs on võrdne kaalude summaga: \\[P = \\sum^{n}_{i = 1} w_i\\] ja mingi tunnuse \\(y\\) üldkogumi kogusumma on: \\[T_y = \\sum^{n}_{i = 1}w_i y_i\\] Kui me rakendame neid kaale erinevates maakodades elavatele inimestele, siis saame alaesindatud maakonna (Harjumaa) valimi üles kaaluda \\(67 \\times 8926 = 598042\\) ja üleesindatud maakona (Hiiumaa) valimi alla kaaluda \\(67 \\times 140 = 9380\\) (erinevused algsetest maakondade suurustest on tingitud ümardamisest, kui ei oleks seda teinud, oleksid tulemused identsed). .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-98551ad2{}.cl-984c1a22{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-984c1a23{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-984c1a24{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-984d02ca{width:122.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d02cb{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d02cc{width:60.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d02cd{width:75.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d02ce{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d02cf{width:60.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d02d0{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d02d1{width:122.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d02d2{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d02d3{width:75.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d02d4{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2b9c{width:60.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2b9d{width:75.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2b9e{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2b9f{width:122.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2ba0{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2ba1{width:122.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2ba2{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2ba3{width:60.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2ba4{width:75.6pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2ba5{width:75.6pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d2ba6{width:90.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d52a2{width:32.8pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d52a3{width:122.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-984d52a4{width:60.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Tabel 7.2: Eesti maakondade kaalud rahvaarvu alusel (Statistikaamet) MaakondNnTõenäosusKaaludHarju maakond598 059.067.00.08 926.3Hiiu maakond9 387.067.00.0140.1Ida-Viru maakond136 240.067.00.02 033.4Jõgeva maakond28 734.067.00.0428.9Järva maakond30 286.067.00.0452.0Lääne maakond20 507.067.00.0306.1Lääne-Viru maakond59 325.067.00.0885.4Põlva maakond25 006.067.00.0373.2Pärnu maakond85 938.067.00.01 282.7Rapla maakond33 311.067.00.0497.2Saare maakond33 108.067.00.0494.1Tartu maakond152 976.067.00.02 283.2Valga maakond28 370.067.00.0423.4Viljandi maakond46 371.067.00.0692.1Võru maakond35 782.067.00.0534.1 Kui me nüüd tahame arvutada Eesti keskmise sissetuleku lähtuvalt maakondade kaalutud keskmisest, siis peame esmalt maakondade keskmised kaaludega läbi korrutama, kokku liitma ning seejärel kaalude summaga läbi jagama. \\[\\frac{(1531 \\times 8926) + (993 \\times 140) + ... + (1113 \\times 534)}{8926 + 140 + ... + 534} = 1351\\] Märksa lähemal tõelisele Eesti keskmisele sissetulekule. Päris sama summat ei saanud me peamiselt seetõttu, et võtsime kaalumise aluseks rahvaarvu, mitte töötavate inimese arvu. Lisaks on tõenäoline, et sissetulekute jaotused erinevad maakondade lõikes, mis tähendab, et tegelikult ei oleks siinkohal väga õige aritmeetilist keskmist kasutada. 7.2.2 Post-stratifikatsiooni kaalud Post-stratifikatsiooni abil saame parandada valimi representatiivsust peale valimi moodustamist. Näiteks juhul kui meie valim on moodustatud aadresside alusel, siis ei saa me planeerida oma valimit vastajate vanuselisest ja soolisest jaotusest lähtuvalt, kuigi üldkogumi kohta on meil need proportsioonid teada. Teine ja märksa levinum põhjus post-stratifikatsiooni jaoks on mitte-vastamine (non-response). Mõnede gruppide puhul kipub vastamisaktiivsus olema mõnevõrra madalam (näiteks noored mehed). Ka mitte-vastamise puhul ei ole meil valimi disainimise juures palju teha (võime muidugi suurendada noorte meeste valimisse sattumise tõenäosust, kuid me ei saa kindlustada kogu valimi vastavust üldkogumi proportsioonidele) ja selle ulatust näeme me vaid peale küsitluse läbiviimist. Kui me siis hiljem avastame, et kuigi mehi peaks olema üldkogumis 50%, on neid valimisse sattunud 40%, siis tähendab see seda, et meie valim ei ole soo lõikes representatiivne. Naised on üleesindatud ja mehed alaesindatud. Sellisel juhul on võimalik välja arvutada post-stratifikatsiooni kaalud. Kaalude moodustamisel peame naisi vähemaks kaaluma \\(0.5 \\div 0.6 \\approx 0.8333\\) ja mehi rohkemaks kaaluma \\(0.5 \\div 0.4 = 1.25\\). Kui valimi kaalud on eelnevalt olemas, siis modifitseerime valimi kaale korrutades iga meessoost respondendi kaalud läbi \\(1.25\\)-ga ja iga naissoost respondendi kaalud läbi \\(0.83\\)-ga. Post-stratifikatsiooni kasutatakse tavaliselt valimi representatiivsuse tõestmiseks erinevate sotsiaaldemograafiliste tunnuste lõikes (sugu, vanus, haridus jne). Kuid kaalude moodustamiseks peame teadma kõikide post-stratifikatsioonitunnuste ristlõigete osakaale. Ehk siis kui tahame post-stratifitseerida hariduse ja soo lõikes, siis peame teadma kõrgharitud meeste osakaalu, kõrgharitud naiste osakaalu, keskharitud meeste osakaalu jne. Tihti taolist ristõikelist üldkogumi jaotust me ei tea. Lisaks võivad nii moodustatud grupid minna väga väikeseks ja tekib oht, et mõnda ristlõiget meil valimis ei olegi. Näiteks kui lisaksime haridusele ja soole veel ka vanuse (näiteks vanused 15-75), siis oleks meil juba \\(2\\times3\\times60 = 360\\) gruppi. Arvestades tavapärast valimimahtu (ca 1000 vaatlust) on ülimalt tõenäoline, et osasid post-stratifikatsiooni ristlõikesid meie valimisse lihtsalt ei sattunud. 7.2.2.1 Raking ja calibration Miks me ei saa post-stratifikatsioonikaale iga sotsiaaldemograafilise tunnuse lõikes eraldi välja arvutada ja seejärel need erinevad kaalud läbi korrutada? Kui me arvutaksime kõigepealt välja soo kaalud, siis kaalude rakendamisel oleks meie valim soo lõikes representatiivne. Kui me seejärel arvutaks välja hariduse kaalud ja rakendaks ka need, siis oleks meie valim hariduse lõikes representatiivne, kuid soo proportsioonid oleksid jälle paigast ära. Ja kui me siis kasutaks veel ka vanuse kaale, oleks meie valim representatiivne vanuse lõikes, kuid mitte enam soo ega hariduse lõikes. Peaksime nüüd uuesti arvutama soo kaalud, siis uuesti hariduse kaalud ja siis uuesti vanuse kaalud. Ja seejärel jälle otsast peale. Niikaua, kuni oleme leidnud mingisuguse ekviliibriumi, kus ühegi tunnuse kaal enam ei muutu. Taolist iteratiivset protsessi nimetatakse rakinguks (ka iterative proportional fitting). Alternatiivne viis erinevate tunnuste lõikes valimi proportsioonide korrigeerimiseks on kalibreerimine. Kalibreerimise puhul kasutakse valimi üldkogumiga proportsiooni viimiseks regressioonimudelit, mille abil arvutatakse kalibratsioonikaalud, millega siis korrigeeritakse valimi kaale. 7.2.3 Mitte-vastamise kaalud (non-response weights) Kuigi post-stratifitseerimisega on võimalik neid gruppe, kus vastamismäär oli madalam kui valimiraam ette nägi, üles kaaluda, on tihti otstarbekas pöörata tähelepanu ka otseselt mitte-vastanutele. Kui post-stratifikatsiooni või rakingu/kalibreerimisega üritatakse tasanda eelkõige valimiraami ja üldkogumi erinevusi (juhuvalim, tänu sellele, et see on juhuslik, ei taga alati, et valim vastaks üldkogumi proportsioonidele) terves valimis, siis mitte-vastamise kaalude abil üritatakse tagada, et reaalne valim vastaks teoreetilisele valimile. See tähendab, et üles kaalutakse ainult neid gruppe, kus vastamismäär oli madalam ja gruppe, kus vastamismäär oli 100%, kaale ei muudeta. Tulles tagasi eelneva soolise erinevuse näite juurde: oletame, et meie valimi suuruseks oli 1000 inimest, kellest 50% olid valimi järgi naised ja 50% mehed (valimis vastavalt 500 ja 500), kuid peale küsitlust avastame, et algsest 1000-st respondendist õnnestus küsitleda 500-t naist ja 400-t meest. Ehk siis naiste vastamismäär oli 100% ja meeste vastamismäär 80%. Kuna naiste valimi osaga on kõik korras, ei taha me nende puhul midagi muuta. Küll aga tahame suurendada meeste kaale nii, et meeste valimi osa vastaks algsele valimiraamile. See tähendab, et me peaksime suurendama meeste kaale \\(500\\div400 = 1.25\\) võrra. Iga valimisse sattunud meest arvstame seega 1.25 kordselt. Teine võimalus mitte-vastamise kaalude arvutamiseks on kasutada logistilist regressiooni, kus valimiraamist lähtuvad sotsiaaldemograafilised tunnused on aluseks hinnangule vastamise ja mitte-vastamise kohta. 7.2.4 Kaaludest üldiselt Ise kaale arvutades peaksime jälgima, et meie arvutatud kaalud liiga suureks ei läheks. See juhtub siis, kui mõni grupp mille lõikes me kaale arvutame, on valimis väga alaesindatud. Näiteks kui tahame tagada, et meie valim vastaks üldkogumile töötuse tunnuse lõikes, kuid valimisse on sattunud töötuid vaid 1%, samas kui üldkogumis on töötuid näiteks 10%. Me peaksime kõik olemasolevad töötud kümnekordselt üles kaaluma. Seda on aga ilmselgelt liiga palju. Me ei taha mõne töötu pealt tehtud järeldusi, mis võivad olla suhteliselt juhuslikud, laiendada küllaltki suurele osale populatsioonist. Seetõttu tuleks pärast kaalude moodustamist alati kontrollida, et need ikka mõistlikkuse piiresse jääks (see mõistlikkuse piir oleneb kontekstist) ja vajadusel suuremaid kaalusid väiksemaks teha (weight trimming). Kui andmestikus on mitu kaalude tunnust, näiteks valimi kaalud ja mitte-vastamise kaalud, siis saame need ühendada üheks kaalu tunnuseks. Selleks peame iga vaatluse jaoks tema kaalud läbi korrutama. Ehk siis kui konkreetse vaatluse jaoks on valimi kaalud väärtusega 0.8 ja mitte-vastamise kaalud 1.2, siis vaatluse lõplikeks kaaludeks kujuneb \\(0.8 \\times1.2 = 0.96\\). 7.3 Valimidisain Olukorras, kus kõigil üldkogumi liikmetel ei ole võrdset võimalust valimisse sattuda (kuid kõigil on see võimalus mingi tõenäosusega siiski olemas) või valimi valikuühik ei ole sama mis analüüsiühik (näiteks kui valim on moodustatud leibkondade põhjal aga analüüsime lebkondades olevaid isikuid), on meil tegemist mingi valimidisaini alusel moodustatud valimiga. Valimidisainiga defineeritakse iga analüüsiühiku erinevad valimisse sattumise tõenäosused. Kui me valimi aluseks olevat disaini andmete anlüüsimisel arvesse ei võta, võime analüüsi tulemusel teha väga valesid järeldusi. Seda nii punkthinnangute (nagu eelnevas näites) kui ka standardvigade osas. Laias laastus võib erinevad valimidisainid jagada kahte suuremasse gruppi: stratifitseeritud valimid ja klastervalimid. Üldjuhul kasutatakse reaalselt nende kahe tüübi kombinatsioone või variatsioone. Konkreetse uuringu juures kasutatav valimidisain on tavaliselt kirjeldatud uuringu dokumentatsioonis (kui see nii ei ole, siis tasub alati uuringu läbiviijalt seda küsida). Tihti on selle seletuse juures ära toodud ka juhised edasiseks analüüsiks. Seega esimeseks sammuks mingi uuringu kasutamisel peaks alati olema uuringu dokumentatsiooniga tutvumine. Kuid miks üldse kasutatakse mingeid keerulisi valimidisaine ja ei piirduta tavaliste juhuvalimitega, mida oleks standardmeetoditega lihtne analüüsida? Esimene ja tihti määravaim põhjus on küsitlusega kaasnev kulu. Keerulisemad valimidisainid võimaldavad kontsentreerida küsitluste läbiviimist, lihtsustades seeläbi küsitlusega kaasnevat logistikat. Teine, ja tegelikult mõnevõrra olulisem põhjus on teatud keerulisemate valimidisainidega kaasnev tulemuste kvaliteedi tõus. Me saame valimidisainiga sihtida konkreetseid gruppe või tõsta uuringu üldist täpsusastet. 7.3.1 Stratifitseeritud juhuvalim Üldkogum jagatakse teineteist välistavatesse gruppidesse (stratad) ja igas grupis viiakse läbi juhuvalik. Stratifitseeritud valimiga on võimalik tagada, et stratifitseeriva tunnuse lõikes on valim ühtlaselt jaotunud ning vähem varieeruv. Seetõttu on ka selle põhjal tehtavad hinnangu täpsemad (standardvead on väiksemad). Tihti tahame teha järeldusi mõne küllaltki marginaalse grupi kohta. Et need järeldused oleksid statistiliselt täpsed, on meil vaja tagada, et see grupp oleks esindatud suuremalt kui see juhuvalikuga võimalik on (mida suurem on uuritava grupi valim, seda täpsemaid hinnanguid me selle kohta teha saame). Näiteks kui tahaksime üle-Eestilises uuringus käsitleda mõnda spetsiifilist küsimust Hiiumaa kohta, siis 1000 inimesega juhuvalimiga satuks meie valimisse 7 hiidldast (Hiiumaa rahvaarv moodustab 0.7% Eesti rahvastikust). Seda on aga liiga vähe et me saaksime Hiiumaa kohta midagi olulist järeldada. Seega peaksime hiidlaste valimisse sattumise tõenäosust mõnevõrra suurendama. Lisaks võimaldab stratifitseeritud valim mõningal määral lihtsustada andmekogumise administreerimist (Näiteks igas maakonnas eraldi uuringu läbiviija) või isegi kasutada stratades erinevaid valimi moodustamise meetodeid. Stratifitseeritud valim eeldab, et strtifitseerimise aluseks olev tunnus oleks valimi moodustamisel iga üldkogumi liikme jaoks teada. See võib teatud juhtudel aga küllaltki problemaatiline olla. Näiteks kui me tahame stratifitseerida maakonna alusel ja üldkogumi moodustamisel lähtuda rahvastikuregistrist, siis on küllaltki tõenäoline, et rahvastikuregistris märgitud elukoha maakond (kui see seal üldse märgitud on) erineb reaalsest elukoha maakonnast. 7.3.2 Klastervalim Klastervalimi puhul käsitletakse analüüsiühikuid mingite klastrite liikmetena ning valimi moodustamisel ei valita mitte analüüsiühikuid otse vaid neist moodustunud klstereid. Juhuvaliku alusel valitakse klastrid ning kõikide valimisse sattunud klastrite liikmeid küsitletakse. Kõige lihtsama näitena võib siinkohal tuua koolivõrgu. Eestis on ca 530 üldhariduskooli. Kui me tahame teha küsitlust, mis oleks representatiivne kõigi Eesti õpetajate suhtes, siis juhuvalimi korral peaksime olema valmis, et kõikidest koolidest satub meie valimisse mõni õpetaja (kuna kõigil õpetajatel on võrdne tõenäosus valimisse sattuda). Kõik koolid läbi käia ja seal paari õpetajat küsitleda oleks logistilisel küllaltki tülikas ning mitte eriti aja- ja kuluefektiivne. Lihtsam, kiirem ja odavam oleks valimi alusena käsitleda koole, teha koolide juhuvalim ning valimisse sattunud koolides küsitleda kõiki õpetajaid. Sellisel juhul oleks meie üldkogum ikkagi kogu Eesti õpetajaskond (kuna kõikidel koolidel ja seeläbi ka kõikidel õpetajatel õpetajatel oli võimalus valimisse sattuda) ja kui me taoliselt moodustatud valimidisaini hiljem analüüsi käigus arvestame, siis saaksime tehtud järeldused üldistada ka kõikidele Eesti õpetajatele. Võib-olla ei ole isegi mõtet kõiki valimisse sattunud koolide õpetajaid intervjueerida. Me saaksime tegelikult ka igas koolis teha eraldi teise tasandi juhuvalimi ja küsitleda ainult valimisse sattunud õpetajaid. Seega esmalt valime juhuvalimi alusel koolid ja seejärel juhuvalimi alusel õpetajad neis koolides. Taolist valimidisani nimetatakse mitmetasandiliseks klastervalimiks. Neid tasandeid võib olla ka rohkem kui kaks. Näiteks esimene juhuvalik on koolide tasandil, teine klasside tasandil ja kolmas õpilaste tasandil. Esimese tasandi valimiühikuid nimetatakse primary sampling units või PSU (koolid), teise tasandi ühikuid secondary sampling units või SSU (klassid) jne. Võrreldes juhu- või stratifitseeritud valimiga klastervalim sama valimimahu juures üldiselt vähendab hinnangute täpsust, kuna inimesed klastrite sees kipuvad olema sarnasemad kui klastrite vahel. Kuid samas võimaldab klastervalimi kuluefektiivsus koostada suuremaid valimeid, mis omakorda suurendavad täpsust. Seega kokkuvõttes võib mõnevõrra suurema mahuga klastervalimiga saada väiksema mahuga juhuvalimiga võrreldava täpsusastmega hinnagud väiksema raha eest. 7.4 Tulemuste valimidisaini suhtes korrigeerimine Punkthinnanguid saame üldjuhul korrigeerida valimi kaaludega. Näiteks kaalutud keskmisi saame ka käsitsi välja arvutada, nagu me eelnevalt tegime, või kasutada selleks vastavaid funktsioone (Ri baasfunktsioon weighted.mean() võtab sisendiks kaalumata keskmiste vektori ja kaalude vektori). Standardvigade korrigeerimine asi mõnevõrra keerulisem ja nende puhul tuleks kasutada spetsiifilisemaid lähenemisi. Eelnevalt vaatasime tavalise juhuvalimi standardvigade arvutamise loogikat (standardviga on valimijaotuse standardhälve, näidates kui suur on meie hinnangu keskmine viga kui me võtaksime samast popultaioonist lõputult valimeid). Keerulisemate valimidisainide puhul juhuvalimi loogika enam ei toimi, kuna vaatlused ei ole omavahel sõltumatud ja/või valiku ühikuks ei ole analüüsiühik. Sellises olukorras on meil tegelikult päris mitmeid võimalusi, kuidas standardvigasid, ehk siis valimijaotuse varieeruvust, hinnata. Peamisteks kasutatavateks meetoditeks on: Taylori seeriate meetod (Taylor series linearisation) Bootstrapi replikatsiooni meetod Jackknife replikatsiooni meetod BRR (balanced repeated replication) Kui Taylori seeriate meetodi puhul arvutatakse standardvead analüütiliselt, siis ülejäänud replikatsioonipõhiste meetodite puhul empiiriliselt. Üldiselt ei pea nende meetodite hingeelu nende kasutamiseks väga põhjalikult tundma (vaatame siiski replikatsioonimeetodeid, kuna nende loogika on küllaltki lihtne). 7.4.1 Replikatsioonikaalud (replicate weights) Keerulisemate valimidisainide ja mitte-lineaarsete meetoodite puhul ei ole alati võimalik standardvigasid analüütiliste meetoditega arvutada. Sel juhul on standardvigade leidmisks võimalik kasutada erinevad replikatsioonimeetodid, nagu bootstrap või jackknife (reaalsuses küll tavaliselt mõnda nende variatsioonidest). Paljude suuremate uuringute puhul on andmestikuga kaasas replikatsioonikaalud, mis võimaldavad replikatsioonimeetodeid kasutada. Näiteks OECD uuringutes nagu PISA või PIIAC. Replikatsioonimeetodeid võib ja saab kasutada ka tavalise juhuvalimi korral. Ajalooliselt on nende laiem kasutus jäänud arvutusvõimsuste taha, kuid tänapäeval, kui see enam probleem ei ole, on need järjest rohkem hakanud tavapäraseid analüütilisi standardvigade arvutamise meetodeid asendama. Kõikide replikatsioonimeetodite üldprintsiip ja loogika on lihtne. Olemasolevast valimist võetakse palju alamvalimeid (replikatsioone), ehk siis valimit käsitletakse üldpopultaioonina, millest võetkse omakorda valimid. Kõikide alamvalimite puhul arvutatakse huvipakkuv statistik (näiteks mingi tunnuse keskmine). Alamvalimite statistikutest moodustub (pseudo)valimijaotus, mille standardhälve ongi standardviga. Seega kui tavaline standardvea arvutamise metoodika lähtub küll potentsiaalsest eeldusest, et valimijaotuse aluseks olevaid valimeid on lõputult, kuid tuletab standardvea analüütiliselt (\\(se = \\frac{sd}{\\sqrt{n}}\\)), siis replikatsioonimeetodid tuletavad olemasolevast valimist suurel hulgal alamvalimeid, mille alusel moodustavad valimijaotuse ja tuletavad standardvea empiiriliselt. Repikatsioonimeetodid erinevad üksteisest selle poolest kuidas nad neid alavalimeid moodustavad. Bootstrap meetodiga võetakse üldvalimist juhuslikkuse alusel sama palju vaatlusi kui seal algselt oli, kuid kasutatakse nn tagasipaneku meetodit. See tähendab, et mõni algse valimi vaatlus võib alamvalimisse sattuda mitu korda ja mõni üldse mitte. Kui palju alamvalimeid võetakse, on analüütiku otsustada (mida rohkem, seda parem, kuid võiks olla vähemalt 100). Jackknife meetodiga võetakse alamvalimeid nii palju kui algses valimis vaatlusi oli. Kuid iga alamvalimi puhul jäetakse mingi reegli alusel üks vaatlustest välja. Ülejäänud vaatlused korrutatakse läbi koefitsiendiga, mis korrigeerib alamvalimi suuruse võrdseks algse valimiga. Näiteks kui algses valimis oli 10 inimest, siis võetakse 10 alamvalimit, milles kõigis on 9 inimest ja Kõik alavalimid korrutatakse läbi 1.1-ga (\\(\\frac{10}{9}\\)) Replikatsioonikaalud annavad meile info kuidas valimite replikeerimine peaks toimuma. 7.5 Küsitlusandmed Ris Ris on küsitlusandmete analüüsiks spetsiaalne pakett survey, mis võimaldab kasutada erinevaid analüüsimeetodeid arvestades samal ajal ka valimidisainist tulenevate eripäradega. Kuna praktiliselt kõik suuremad küsitlused kasutavad mingeid keerukamaid valimidisaine, siis peame nende andmete analüüsimisel alati ka vastavate disainidega arvestama ega saa kasutada tavapäraseid meetodeid. Kõigepealt installige (kui te seda juba teinud ei ole) survey pakett ja lugege see sisse. Nagu alati, siis installima peab paketi ainult ühe korra, kuid igaks sessiooniks tuleb see uuesti sisse lugeda. survey paketiga on kaasas ka näidisanmdestikud. Lugege ka need sisse (´data(api)´). api andmestikuga on mõõdetud akadeemilise võimekuse indeksit kõikides Kalifornia koolides. Andmestiku analüüsiühikuks on kool ehk siis andmestik koondab koolitsandi infot. #install.packages(&#39;survey&#39;) library(survey) data(api) survey paketis peab kõigepealt defineerima valimidisaini, milles kirjeldatakse kõiki valimi moodustamise eripärasid, kaale jne. Disaini defineerimise läbi koondatakse kogu valimidisainist lähtuv info ühte andmeojekti. Disaini deineerimiseks on funktsioon svydesign(): 7.5.1 Kaaludega tavalise juhuvalimi defineerimine Valimidisain tavalise juhuvalimi puhul kui meile on ainult valimi kaalud ja/või mingid mingid kaalud. Isegi tavalise juhuvalimi puhul peaksime ikkagi kasutama vähemalt valimi kaale (need lubavad meil hinnata näiteks populatsiooni suurust). Lisaks valimi kaaludele või valimi kaalude asemel võivad andmestikus olla ka mitte-vastamise kaalud või poststratifikatsiooni kaalud (või oleme need ise välja arvutanud), mille olemasolu või vajadus ei sõltu sellest, kas tegemist on tavalise juhuvalimiga või keerulisema disainiga. Kui andmestikus on mitu kaalude tunnust, mida meil on vaja kasutada, siis peaksime need omavahel läbi korrutades üheks tunnuseks koondama. svydesign() funktsiooni argumentidena peame defineerima: ids argumendiga defineeritakse valimiühikud (PSU). Need on klastri idd klastervalimi puhul, ehk siis tunnus, mille lõikes klastervalimi valik toimus. Kui valik toimus analüüsitasandil ilma klastriteta, siis tuleks märkida ids = ~1 weights tähistab kaalu tunnust. Tunnuse nime ette tuleb kindlasti panna tilde märk (või viidata tunnusele otse apisrs$pw) data on meie algne küsitlusandmestik # Kasutame andmestikku apisrs # kaalude tunnus on &#39;pw&#39; des_jv = svydesign(ids = ~1, weights = ~pw, data = apisrs) Väiksemate üldkogumite korral, kui me teame üldkogumi suurust, saab kasutada lõpliku populatsiooni korrektsiooni (finite population correction ehk fpc), mis võimaldab kasutada mõnevõrra väiksemaid standardvigasid. Kui üldkogum on väike, siis on ka valim üldkogumile pigem sarnasem, võrreldes juhuga kui üldkogum on väga suur. Seega on valimis ka vähem määramatust ning standardvead väiksemad. fpc defineeritakse parameetriga: fpc mille kaudu peab deineerima tunnuse, mis sisaldab iga vaatluse kohta üldkogumi suurust. Tavalise juhuvalimi puhul on see suurus kõikide vaatuste jaoks sama. des_jv = svydesign(ids = ~1, weights = ~pw, fpc = ~fpc, data = apisrs) Küsitluse disainist ülevaate saamiseks saame kasutada summary() funktsiooni: summary(des_jv) ## Independent Sampling design ## svydesign(ids = ~1, weights = ~pw, fpc = ~fpc, data = apisrs) ## Probabilities: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.03229 0.03229 0.03229 0.03229 0.03229 0.03229 ## Population size (PSUs): 6194 ## Data variables: ## [1] &quot;cds&quot; &quot;stype&quot; &quot;name&quot; &quot;sname&quot; &quot;snum&quot; &quot;dname&quot; ## [7] &quot;dnum&quot; &quot;cname&quot; &quot;cnum&quot; &quot;flag&quot; &quot;pcttest&quot; &quot;api00&quot; ## [13] &quot;api99&quot; &quot;target&quot; &quot;growth&quot; &quot;sch.wide&quot; &quot;comp.imp&quot; &quot;both&quot; ## [19] &quot;awards&quot; &quot;meals&quot; &quot;ell&quot; &quot;yr.rnd&quot; &quot;mobility&quot; &quot;acs.k3&quot; ## [25] &quot;acs.46&quot; &quot;acs.core&quot; &quot;pct.resp&quot; &quot;not.hsg&quot; &quot;hsg&quot; &quot;some.col&quot; ## [31] &quot;col.grad&quot; &quot;grad.sch&quot; &quot;avg.ed&quot; &quot;full&quot; &quot;emer&quot; &quot;enroll&quot; ## [37] &quot;api.stu&quot; &quot;pw&quot; &quot;fpc&quot; 7.5.2 Stratifitseeritud valimi defineerimine Stratifitseeritud valimi puhul tuleb meil defineerida stratifitseeriv tunnus. Andmestikus apistrat on selleks stype (koolitüüp) tunnus, mille lõikes koolid on stratifitseeritud. strata argumendiga defineeritakse valimit stratifitseeriv tunnus fpc argumendiga saab määrata iga strata suuruse (kui see on olemas, siis ole vaja valimi (disaini) kaale eraldi märkida, survey arvutab vaatluste valimisse sattumise tõenäosused ja kaalud ise välja) des_strat = svydesign(ids = ~1, strata = ~stype, weights = ~pw, fpc = ~fpc, data = apistrat) 7.5.3 Ühetasandilise klastervalimi defineerimine Kasutame apiclus1 andmestikku, milles on koolid klasterdatud piirkonna (dnum) järgi. Ehk siis piirkonnad on esimese tasandi valikuks. Valimisse sattunud piirkondade kõik koolid on valimisse kaasatud. ids argumeniga defineeritakse PSU ehk klastri id (dnum) weights argumendiga määratakse valimi kaalud. Jällegi, kui tegemist on ainult valimi kaaludega ja fpc on defineeritud, siis ei pea kaale määrama (survey arvutab need ise välja) fpc valimi suurus (klastrite koguarv) des_clus &lt;- svydesign(ids=~dnum, weights=~pw, data=apiclus1, fpc=~fpc) 7.5.4 Mitmetasandilise klastervalimi defineerimine Andmestikuks on apiclus2, kus esimese tasandina (PSU) on valitud kooli piirkonnad ja teise tasandina (SSU) koolid. Ehk kõigepealt on tehtud piirkondade valim ja seejärel igas valimisse sattunud piirkonnas omakorda koolide valim. ids argumeniga on defineeritud PSU ja SSU idd fpc argumendiga on defineeritud PSU ja SSU üldkogumite suurused (vastavalt piirkondade koguarv ja koolide koguarv piirkonnas) des_clus2 &lt;- svydesign(ids=~dnum+snum, fpc=~fpc1+fpc2, data=apiclus2) 7.5.5 Replikatsioonikaaludega valimi defineerimine Replikatsioonikaalude tegemiseks on survey funktsioon as.svrepdesign(), mis võtab sisendiks olemasoleva ilma replikatsioonikaaludeta disainiobjekti ja annab väljundiks kaaludega disainiobjekti. design argumendiga defineeritakse survey disain, millele tahetakse replikatsioonikaale arvutada type argumendiga defineeritakse replikatsioonimeetodi tüüp. Variandid on auto, JK1, JKn, BRR, bootstrap, subbootstrap,mrbbootstrap,Fay. Täpsemalt on ndende kohta võimalik lugeda funktsiooni abifailist: ?as.svrepdesign # Defineerime kõigepealt survey disaini des_clus &lt;- svydesign(ids=~dnum, weights=~pw, data=apiclus1) # Arvutame kaalud rw1&lt;-as.svrepdesign(design = des_clus, type = &#39;JK1&#39;) Replikatsioonikaalude kasutamiseks, juhul kui me ise neid kaale ei arvuta ja need on meie andmestikkus juba olemas, peame defineerima jällegi vastava survey disainiobjekti. Näidisandmestikes ühtegi replikatsioonikaaludega andmestikku ei ole. Kuid saame selle vähese vaevaga eelneva näite replikatsioonikaalude objekti abil teha: indeks &lt;- data.frame(indeks = rw1$repweights$index) kaalud &lt;- as.data.frame(rw1$repweights$weights) names(kaalud) &lt;- paste0(&#39;rep_w_&#39;, 1:15) kaalud$indeks &lt;- 1:15 kaalud &lt;- left_join(indeks, kaalud, by = &#39;indeks&#39;) apiclus_rep = cbind(apiclus1, kaalud) Kui kaaludega andmestik on olemas, siis saame replikatsioonikaalude disaini defineerida funktsiooniga svrepdesign(). Funktsioonis on kindlasti vaja defineerida järgmised argumendid: repweights argumendiga defineeritakse replikatsioonikaalud. Neid kaale on tavaliselt küllaltki palju. Seega on otstarbekas need enne andmestikust välja võtta. Näiteks dplyr paketi select() funktsiooniga. Kuna tavaliselt on kaalud mingi kindla reegli alusel nimetatud, siis on mugav kasutada starts_with() funktsiooni, mis võimaldab valida kõik ühtse nimeosaga tunnused. type argumendiga määratakse replikatsioonikaalude tüüp. Võimalikud tüübid on BRR,Fay,JK1, JKn,bootstrap,ACS,successive-difference,JK2 ja other. See, millist tüüpi on vaja kasutada, peaks olema kirjas küsitluse dokumentatsioonis. Näiteks PISA uuringus on kasutatud Fay meetodit. data argumendiga defineeritakse küsitluse andmestik. combined.weights argument ütleb, kas valimi kaalud on juba liidetud replikatsioonikaaludele. Tvaliselt see nii ongi (ja combined.weights vaikimisi väärtus on T), kuid meie isetehtud andmestikus ei ole. library(dplyr) kaalud &lt;- apiclus_rep %&gt;% select(starts_with(&#39;rep_w_&#39;)) des_rw &lt;- svrepdesign(repweights = kaalud, weights = ~pw, type = &#39;JK1&#39;, data = apiclus_rep, combined.weights = F) 7.6 Valimiandmete analüüs Kui valimidisain on defineeritud, siis saab seda kasutada edasistes analüüsides. Selleks on terve hulk survey funktsioone, mis kõik algavd svy eeliitega. Tasub meeles pidada, et survey diainiobjektid sobivad sisendina ainult survey funktsioonidele (mõne erandiga). Näiteks tavalise lineaarse regressiooni lm() funktsiooni kasutada ei saa (küll aga survey glm() funktsiooni, mis võimaldab meil samuti lineaarset regressiooni jooksutada). Silmas tasub pidada ka seda, et enamke funktsioonide puhul on vaja kasutada tilde märki. Üldkogumi kogusumma: # Mitu õpilast õpib Kalifornia koolides (enroll tunnus) svytotal(~enroll, design = des_clus) ## total SE ## enroll 3404940 941611 Tunnuse keskmine: # Akadeemilise võimekuse indeksi (tunnus api00) keskmine svymean(~api00, design = des_clus) ## mean SE ## api00 644.17 23.779 Kui tahame keskmisele usalduspiire, siis saame kasutada funktsiooni confint(): mod &lt;- svymean(~api00, design = des_clus) confint(mod) ## 2.5 % 97.5 % ## api00 597.5634 690.7754 Võime ka korraga mitme tunnuse keskmist hinnata: svymean(~api00+api99, design = des_clus) ## mean SE ## api00 644.17 23.779 ## api99 606.98 24.469 Saame ka kategoriaalsete tunnuste proportsioone hinnata: svymean(~stype, design = des_clus) ## mean SE ## stypeE 0.786885 0.0468 ## stypeH 0.076503 0.0271 ## stypeM 0.136612 0.0299 Kvantiilid: # vajalikud kvantiilid defineerime vektorina svyquantile(~api00, design = des_clus, quantiles = c(.25,.5,.75)) ## $api00 ## quantile ci.2.5 ci.97.5 se ## 0.25 552 491 628 31.93791 ## 0.5 652 559 715 36.36725 ## 0.75 719 696 777 18.88300 ## ## attr(,&quot;hasci&quot;) ## [1] TRUE ## attr(,&quot;class&quot;) ## [1] &quot;newsvyquantile&quot; Gruppide kaupa hinnagud: svyby(~api00, by = ~stype, design = des_clus, FUN = svymean) ## stype api00 se ## E E 648.8681 22.58731 ## H H 618.5714 38.40263 ## M M 631.4400 31.92737 Saame hinnata ka mitut tunnust mitme grupi lõikes: svyby(~api00+api99, by = ~stype+sch.wide, design = des_clus, FUN = svymean) ## stype sch.wide api00 api99 se.api00 se.api99 ## E.No E No 596.3333 601.6667 43.92749 47.75128 ## H.No H No 659.3333 662.0000 27.27433 29.52400 ## M.No M No 606.3750 611.3750 41.53039 41.53240 ## E.Yes E Yes 653.6439 608.3485 20.52153 21.74141 ## H.Yes H Yes 607.4545 577.6364 44.14423 46.96892 ## M.Yes M Yes 643.2353 607.2941 42.55219 42.95820 Kui on vaja usalduspiire: svyby(~api00+api99, by = ~stype+sch.wide, design = des_clus, FUN = svymean, vartype = &#39;ci&#39;) ## stype sch.wide api00 api99 ci_l.api00 ci_l.api99 ci_u.api00 ## E.No E No 596.3333 601.6667 510.2370 508.0759 682.4296 ## H.No H No 659.3333 662.0000 605.8766 604.1340 712.7900 ## M.No M No 606.3750 611.3750 524.9769 529.9730 687.7731 ## E.Yes E Yes 653.6439 608.3485 613.4225 565.7361 693.8654 ## H.Yes H Yes 607.4545 577.6364 520.9334 485.5790 693.9756 ## M.Yes M Yes 643.2353 607.2941 559.8345 523.0976 726.6361 ## ci_u.api99 ## E.No 695.2575 ## H.No 719.8660 ## M.No 692.7770 ## E.Yes 650.9609 ## H.Yes 669.6938 ## M.Yes 691.4906 Regressioonimudelite jaoks on funktsioon svyglm(), mis on väga sarnane glm() funktsioonile. Tavaline lineaarne regressioon: # Lineaarse regressiooni jaoks defineerime argumendi family = gaussian(). # See on tegelikult ka vaikimis väärtus # ja me ei pea seda ilmtingimata märkima mod &lt;- svyglm(api00 ~ ell + meals + mobility, design = des_clus, family = gaussian()) summary(mod) ## ## Call: ## svyglm(formula = api00 ~ ell + meals + mobility, design = des_clus, ## family = gaussian()) ## ## Survey design: ## svydesign(ids = ~dnum, weights = ~pw, data = apiclus1) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 819.2791 21.6051 37.921 5.18e-13 *** ## ell -0.5167 0.3273 -1.579 0.143 ## meals -3.1232 0.2809 -11.119 2.54e-07 *** ## mobility -0.1689 0.4494 -0.376 0.714 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 3157.85) ## ## Number of Fisher Scoring iterations: 2 Logistiline regressioon: # family = binomial() # tunnus awards on kategooriatega &quot;Yes&quot; ja &quot;No&quot;. # I(awards == &#39;Yes&#39;) käsu abil muudame kõik Yes&#39;id TRUE&#39;ks # ja ülejäänud väärtused FALSE&#39;ik mod &lt;- svyglm(I(awards == &#39;Yes&#39;) ~ ell + meals + mobility, design = des_clus, family = binomial()) summary(mod) ## ## Call: ## svyglm(formula = I(awards == &quot;Yes&quot;) ~ ell + meals + mobility, ## design = des_clus, family = binomial()) ## ## Survey design: ## svydesign(ids = ~dnum, weights = ~pw, data = apiclus1) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.425949 0.351961 1.210 0.2516 ## ell 0.041120 0.015248 2.697 0.0208 * ## meals -0.014643 0.007044 -2.079 0.0618 . ## mobility 0.007534 0.013571 0.555 0.5899 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 0.9907103) ## ## Number of Fisher Scoring iterations: 4 7.7 Plausible values Plausible values või eesti keeles siis ehk usutavad väärtused ei ole tegelikult otseselt küsitlusuuringute teema selles mõttes, et need ei ole seotud valimidisainiga. Küll aga kasutatakse neid küsitlusuuringutes (näiteks PIAACi ja PISA uuringutes), seega on asjakohane neid siin ka põgusalt käsitleda. PIAACi uuringu eesmärgiks on mõõta täiskasvanute oskusi (lugemis-, arvutamis- ja probleemilahendusoskused). Kui me aga vaatame PIAACi andmefaili, siis näeme seal ei ole iga mõõdetava oskuse kohta ühte tunnust, vaid tervelt 10 erinevat tunnust. Need 10 tunnust iga oskuse kohta on nende oskuste usutavad väärtused. Need väärtused on tuletatud iga inimese jaoks lähtuvalt selle inimese vastustest oskusi mõõtvatele küsimustele. Iga inimese jaoks kontrueeritakse tema oskuse tõenäosusjaotus (selleks kasutatakse IRT (Item Response Theory) meetodeid). Seejärel võetakse sellest jaotusest 10 juhuslikku väärtus, mis ongi selle inimese usutavad väärtused. Kui me tahame nüüd teada, et kas meeste või naiste lugemisoskus on keskmiselt parem, siis mida me tegema peaksime? Me ei tohiks kindlasti arvutada iga inimese jaoks tema usutavate väärtuste keskmist. Samuti ei tohiks me kasutada ainult ühte usutavat väärtust. Mõlemal juhul oleks meie järeldused valed. Et saada korrektseid tulemusi, peaksime võrdlema esimese usutava väärtuse tulemusi meeste ja naiste vahel, seejärel võrdlema teise usutava väärtuse tulemusi meeste ja naiste vahel ja nii kümme korda. Seejärel peaksime arvutama kõikide kümne meeste ja naiste erinevuse keskmise, mis olekski korrektne meeste ja naiste erinevus. Tundub päris aeganõudev ja tüütu. Õnneks on ka lihtsam viis - kasutada jällegi Ri survey paketti (koos mitools paketiga): # Kõigepealt vajalikud paketid. # Kui teil neid installitud ei ole, siis tehke seda library(haven) library(survey) library(mitools) library(stringr) # tõmbame OECD lehelt Piaaci Eesti andmestiku (SPSS faili) # Selle koodiga tõmbame nad kodukataloogi alamkataloogi &quot;./andmed&quot; # Võite alati ka otse OECD lehele minna ja andmestiku sealt tõmmata dir.create(&quot;data&quot;) download.file(&quot;https://webfs.oecd.org/piaac/puf-data/SPSS/prgestp1.sav&quot;, &quot;./data/prgestp1.sav&quot;, method=&quot;auto&quot;, mode=&quot;wb&quot;) # Loeme andmed sisse dat &lt;- as.data.frame(read_spss(&#39;./data/prgestp1.sav&#39;)) # PIAACI andmestikus on kasutatud replikatsioonikaale # Otsime kaalude nimed välja # Kasutame selleks stringr paketi funktsiooni str_subset # Kõigepealt valimi kaal mw &lt;- str_subset(names(dat), &quot;^SPFWT0$&quot;) # Seejärel replikatsioonikaalud rw &lt;- str_subset(names(dat), &quot;^SPFWT[1-9][0-9]*$&quot;) # Otsime välja ka oskuste usutavate väärtusete tunnused lit &lt;- str_subset(names(dat), &quot;^PVLIT[0-9]+&quot;) num &lt;- str_subset(names(dat), &quot;^PVNUM[0-9]+&quot;) prob &lt;- str_subset(names(dat), &quot;^PVPSL[0-9]+&quot;) # Loome survey disaini des_piaac &lt;- svrepdesign(repweights=dat[,rw], weights=dat[,mw], type=&quot;JK2&quot;, data = dat) f &lt;- as.formula(paste0(&#39;lit~&#39;, paste(lit, collapse = &#39;+&#39;))) results_list&lt;-withPV( mapping = f, data = des_piaac, action = quote(svyby(~lit, by = ~GENDER_R, design = des_piaac, FUN = svymean, na.rm = T))) summary(MIcombine(results_list)) ## Multiple imputation results: ## withPV.svyrep.design(mapping = f, data = des_piaac, action = quote(svyby(~lit, ## by = ~GENDER_R, design = des_piaac, FUN = svymean, na.rm = T))) ## MIcombine.default(results_list) ## results se (lower upper) missInfo ## 1 275.0582 1.0832068 272.8910 277.2254 41 % ## 2 276.6413 0.8109968 275.0258 278.2569 36 % # Hindame regressioonimudeliga soo mõju lugemisoskusele # Tulemus peaks olema sama, mis eelmises analüüsis f &lt;- as.formula(paste0(&#39;lit~&#39;, paste(lit, collapse = &#39;+&#39;))) results_list&lt;-withPV( mapping = f, data = des_piaac, action = quote(svyglm(lit~as.factor(GENDER_R), design = des_piaac, family = gaussian()))) summary(MIcombine(results_list)) ## Multiple imputation results: ## withPV.svyrep.design(mapping = f, data = des_piaac, action = quote(svyglm(lit ~ ## as.factor(GENDER_R), design = des_piaac, family = gaussian()))) ## MIcombine.default(results_list) ## results se (lower upper) missInfo ## (Intercept) 275.058207 1.083207 272.8909806 277.225433 41 % ## as.factor(GENDER_R)2 1.583098 1.243243 -0.8602807 4.026478 15 % "],["struktuurivõrrandite-mudelid.html", "Peatükk 8 Struktuurivõrrandite mudelid 8.1 Rajaanalüüs 8.2 Faktoranalüüs 8.3 Sruktuurivõrrandite mudelid 8.4 Struktuurivõrrandite mudelid Ris", " Peatükk 8 Struktuurivõrrandite mudelid Kui me räägime sotsiaalteadustes mingitest kausaalsetest mõjudest, siis peame arvestama, et väga harva (pigem mitte kunagi) õnnestub meil hinnata mingi muutuja puhast mõju teisele muutujale. Taoline olukord eeldaks, et muutujate vaheline korrelatsioon oleks 1: ühe tunnuse muutus peaks olema täielikult sõltuvuses teise tunnuse muutusest ja vastupidi. Sotsiaalsete süsteemide mitmekesisus ning neis toimivate psüühikate idiosünkraatiline loomus ei luba meil praktiliselt kunagi eeldada taolist puhast põhjuslikkust. Kõik meie uuritavad nähtused on alati mõjutatud tohutust hulgast erinevatest teguritest ja toimijatest. Regressioonimudeli abil on meil võimalik mõjude hindamisel võtta arvesse erinevaid taustategureid ja uuritavat mõju nende suhtes kontrollida või isegi analüüsida erinevate tegurite koosmõjusid, kuid (vähemalt praktikas) ei õnnestu meil mitte kunagi arvesse võtta kõiki võimalikke mõjusid ning kirjeldada 100% regressioonimudeli sõltuva tunnuse variatsioonist. Mudeli \\(R^2\\), mis kirjeldab mudeli seletatud variatsiooni, ei ole kunagi 1. Me räägime heast mudelist juba siis, kui selle \\(R^2\\) on 0.2 või 0.3, ehk me seletame sõltuva tunnuse variatsioonist 20% või 30%. See aga tähendab, et 80% või 70% tunnuse muutumise põhjustest jääb meile teadmata26. Mingi osa sellest seletamata jäävast variatsioonist on põhjustatud teguritest, mida me ei ole osanud mõõta. Selles osas peame peame järgmisel korral lihtsalt paremini üritama ja kaasma mudelisse võimalikult palju relevantseid tunnuseid. Teine osa on aga selline variatsioon, mida me ei saagi ammendavalt seletada. See tuleneb uuritavate eripärastest isiklikest kogemustest ja hetketujust, vastamise kontekstist või muust sarnasest. Taolist selgitamata variatsiooni nimetatakse mõõtmisveaks (measurement error). Tavalise regressiooni puhul ei ole meil väga palju võimalusi mõõtmisviga vähendada või seda kuidagi korrigeerida. Siinkohal tuleb meile aga appi struktuurivõrrandite modelleerimine (structural equation modelling ehk lühidalt SEM). SEMi abil on võimalik modelleerida meid huvitavat nähtust ilma mõõtmisveata (või vähemalt seda oluliselt vähendades). Siin on küll üks aga. Nähtus, mida me uurime, peab sellisel juhul olema mõõdetud mitme tunnusega. Kui see aga nii on, siis on SEMi abil võimalik tuvastada selle nähtuse nn puhas hinnang (true score). Kui erinevad tunnused mõõdavad ühte nähtust, siis saame nende erinevate tunnuste ühise variatsiooni abil tuvastada sellest nähtusest tuleneva variatsiooni ja mõõtmisvea kõrvale jätta. Mõõtmisveaga arvestamine ei ole muidugi ainukene põhjus SEM-i kasutamiseks. Tihti on uuritav nähtus lihtsalt nii mitme tahuline, et seda ei saagi ühe tunnuse abil mõõta. Või on tegemist mingi mentaalse konstruktiga, millele otse küsides ligi ei pääse ja mis eeldabki erinevate nurkade alt eri küsimusi. Samuti saame SEM-i abil uurida väga komplekseid sotsiaalseid või psüühilisi süsteeme ja erinevate mõjude mustreid. Struktuurivõrrandite modelleerimine ei ole üks konkreetne meetod, vaid laiem modelleerimisraamistik, mis ühendab endas erinevaid konkreetsemaid meetodeid. Oluliseimateks neist on faktoranalüüs (factor analysis) ja rajaanalüüs (path analysis), mis mõlemad on omakorda edasiarendused tavalisest regressioonist (seega oleks hea, kui enne nende kasutamist oleks regressioonist mingi arusaam). Vaatame lähemalt, mida need kaks SEM-i alusmeedodid endast kujutavad. 8.1 Rajaanalüüs Regressioonanalüüsiga saame hinnata millised tunnused ja kui palju mõjutavad mingit sõltuvat tunnust. Meie huvi objektiks on sõltuvale tuunusele suunatud otsesed mõjud. See on aga paratamatult mõnevõrra lihtsustatud pilt märksa keerulisemast mõjude virvarrist. Rajaanalüüs võimaldab meil sotsiaalse elu paratamatut kompleksust mõnevõrra eksplitsiitsemalt välja tuua ja analüüsida. Täpsemalt võimaldab see meil hinnata lisaks otsestele mõjudele ka kaudseid mõjusid ning seeläbi luua terviklikum pilt mingite protsesside taga olevatest kausaalsetest süsteemidest. Me saame analüüsida erinevate tegurite omavaheliste mõjude mustrit ning testida mingite mõjuahelate olemasolu. Kõige lihtsam on sellest aru saada joonise näitel: Joonis 8.1: Rajaanalüüsi diagramm Joonisel 8.1 on kujutatud väga lihtne rajaanalüüsi mudeli diagramm (puhtalt hüpoteetiline). Meil on hüpotees, et vanemate haridustase mõjutab inimese sissetulekut (eeldame, et isa ja ema haridustase on korreleeritud sissetulekuga). Kuid see mõju ei ole otsene, vaid vahendatud läbi isiku hariduse. Vanemate haridus mõjutab inimese haridustaset, mis omakorda mõjutab sissetulekut. Sisuliselt on meil tegemist kahe regressioonimudeliga, mis on rajaanalüüsiks kokku pandud: 1) mudel, kus sõltumatuteks tunnusteks on isa ja ema haridus ning sõltuvaks tunnuseks haridus; 2) mudel, kus sõltumatuks tunnuseks on haridus ja sõltuvaks tunnuseks sissetulek. Sõltumatuid tunnuseid nimetatakse rajaanalüüsi kontekstis eksogeenseteks tunnusteks. Need on tunnused, mis ei ole mitte ühegi teise tunnuse poolt seletatud (ükski nool ei lähe nende poole). Antud mudelis on eksogeenseteks tunnusteks isa ja ema haridus. Sõltuvaid tunnuseid kutsustakse rajaanalüüsis endogeenseteks tunnusteks. Endogeensed tunnused on seletatud mingite teiste tunnuste poolt (vähemalt üks nool läheb selle tunnuse poole), kuid võivad ka ise olla sõltumatuks tunnuseks mingile muule tunnusele. Antud mudeli on endogeensed tunnused haridus ja sissetulek. 8.1.1 Mõjud Rajaanalüüsi kontekstis analüüsime ja eristame erinevat tüüpi mõjusid. Kõige laiemalt võttes võib eristada otseseid mõjusid ja kaudseid mõjusid. Lisaks võib eristada kõrvalmõjusid, ühismõjusid ja vastastikmõjusid (korrelatsioonid). Otsene mõju (direct effect) on mõju, mida me üldjuhul taga ajamegi (joonis 8.2). See toimib siis kui mingi tegur mõjutab otseselt mingit nähtust. Näiteks temperatuur mõjutab otseselt elavhõbeda mahtu. Tavline regressioon hindab otsesest mõjust - palju muutub \\(y\\) kui \\(x\\) muutub ühe ühiku võrra. Joonisel 8.2 kujutatud vaimse võimekuse mõju kõrgkooli sissesaamise tõenäosusele võiks ju teoreetiliselt olla otsene. Kuid kas see ka nii on? Kõrgkooli sissesaamine eeldab lisaks vaimsele võimekusele ka teatud õppeedukuse kriteeriumite täitmist või näiteks soosivat kodust keskkonda jne. Joonis 8.2: Otsene mõju Kaudset mõju või ka vahendatud mõju (indirect effect) eeldame siis kui mingi tunnuse mõju ei ole enam otsene, vaid mõne muu muutuja poolt vahendatud (joonis 8.3). Gümnaasiumi tase võiks ju olla prediktoriks õpilase kõrgkooli sissesaamise tõenäosuse hindamiseks, kuid tegelikult peaksime eeldama, et gümnaasiumi tase otseselt ei määra ülikooli sissesaamise tõenäosust, vaid pigem loob keskkonnda selle tõenäosuse kujunemiseks. Joonis 8.3: Kaudne mõju Kõrvalmõjuga (spurious effect) on tegemist siis kui mingite tunnuste vahel on küll seos, kuid see seos on tegelikult põhjustatud mingist muust tunnusest, mis mõjutab korraga kõiki seotuid tunnuseid (joonis 8.4). Näiteks gümnaasiumi tase on tõenäoliselt tugevas positiivses korrelatsionis kõrgkooli sissesaamise tõenäosusega. Kuid võime spekuleerida, et need mõlemad tunnused on mõjutatud hoopis õpilaste õppeedukusest, mis mõjutab nii gümnaasiumi taset kui kõrgkooli sisseastumiseksamite sooritust. Seega gümnaasiumi taseme ja kõrgkooli sisseastumiseksamite sooritus vaheline korrelatsioon on põhjustatud õppeedukusest. Joonis 8.4: Kõrvalmõju Ühismõju (joint effect) on midagi kaudse mõju ja kõrvalmõju vahepealset (joonis 8.5). Meil on kaks vastasmõjus (korreleeritud) tunnust - Sotsiaalmajanduslik staatus ja sotsiaalne kapital, milles esimene mõjutab edasiõppimise tõenäosust ja teine gümnaasiumi taset (a la eliitkool või mitte). Kuna sotsiaalmajanduslik staatus ja sotsiaalne kapital on korreleeritud, siis ühe muutudes muutub ka teine ning seega muutuvad ka nii edasiõppimise tõenäosus kui ka gümnaasiumi tase. Kui me jätaksime sotsiaalmajanduliku staatuse ja sotsiaalse kapitali mängust välja, siis peaksime järeldama, et edasiõppimise tõenäosus ja gümnaasiumi tase on seotud (kuigi tegelikult ei ole). Ühismõjuga peame arvestame alati kui on tegemist korrelatsiooniga. Joonis 8.5: Ühismõju 8.1.2 Rajaanalüüsi loogika Vaatame järgmisena ühte natukene kompleksemat rajaanalüüsi mudelit (joonis 8.6). Joonis 8.6: Rajaanalüüsi diagramm Kas oskate selle diagrammi põhjal määrata otsesed, kaudsed, kõrval- ja ühismõjud? otsed mõjud: \\(b_1\\), \\(b_2\\), \\(b_3\\), \\(b_4\\) kaudsed mõjud: \\(b_1b_4\\), \\(b_2b_4\\) kõrvalmõjud: \\(b_2b_3\\) ühismõjud: \\(b_1 \\phi b_3\\), \\(\\phi b_1\\), \\(\\phi b_3\\), \\(\\phi b_2\\), \\(\\phi b_1 b_4\\), \\(\\phi b_2 b_4\\) Millised on eksogeensed ja endogeensed tunnused? eksogeensed: Sissetulek ja Haridus endogeensed: Staatus ja Maailmavaade Kuidas sellist rajaanalüüsi mudelit hinnata? Nagu eelnevalt oli jutuks, on rajaanalüüsi puhul tegemist regressioonanalüüsi edasiarendusega, kus ühte mudelisse on pandud mitu regressioonivõrrandit. Seega saame mõjude suurused kätte, kui defineerime kõik rajaanalüüsis määratletud regressioonivõrrandid. Selleks peame otsima üles kõik endogeensed (sõltuvad) tunnused ning defineerima igale neist regressioonivõrrandi, mille sõltumatuteks tunnusteks on kõik neid otseselt mõjutavad tunnused. Antud juhul tuleks meil defineerida kaks regressioonivõrrandit (sest endogeenseid tunnuseid on kaks): \\[Staatus = b_1Sissetulek + b_2 Haridus + \\epsilon\\] \\[Maailmavaade = b_3 Haridus + b_4 Staatus + \\epsilon\\] Kui me need regressioonivõrrandid Ri abil ära hindaks, siis saadud regressioonikoefitsiendid (standardiseeritud koefitsiendid) oleksidki vastavad rajaanalüüsis defineeritud otseste mõjude suurused. Kaudsed, kõrval- ja ühismõjud saame kui korrutame neid moodustavad otsed mõjud. Näiteks kaudne mõju Sissetuleku ja Maailmavaate vahel on \\(b_1 \\times b_4\\). Kõrvalmõju Staatuse ja Maailmavaate vahel on \\(b_2 \\times b_3\\). Nagu näha, siis ühismõju \\(\\phi\\) (korrelatsiooni) Sissetuleku ja Hariduse vahel me ei defineerinud, kuna sellel mõjul ei ole otsest suunda, mida regressioonivõrrand eeldab. Aga loomulikult saame vajadusel selle korrelatsiooni välja arvutada. Samuti ei ole mudelites vabaliikmeid (\\(b_0\\)) kuna meie eesmärk on antud hetkel keskenduda ainult mõjudele. Vabaliikmed on aga vajalikud eelkõige keskmiste arvutamiseks. Tasub meeles pidada, et rajaanalüüs ei võimalda kinnitada või määrata kausaalsuse olemasolu. See on ikkagi ainult meetod, mille abil saame kontrollida oma teooriast tulenevaid hüpoteetilisi mõjusid. Kausaalsuse eeldused saavad põhineda ainult teoreetilistel eeldustel, mitte meetodil. 8.1.3 Korrelatsiooni dekompositsioon Siiani oleme käsitlenud rajaanalüüsi eraldiseisva meetodina (nagu seda tihti ka kasutatakse). Nüüd aga liigume natuke lähemale rajaanalüüsi kasutamisele SEMi raamistikus. SEM põhineb korrelatsioonide (või kovariatsioonide) analüüsil. Tuleb välja, et me saame oma rajaanalüüsi mudelit väljendada struktuurivõrranditena, millega on võimalik reprodutseerida algsed tunnustevahelised korrelatsioonid. Iga kahe tunnuse vahelise korrelatsiooni (tähistatud kui \\(r\\)) saame lahutada neljaks meile juba tuttavaks komponendiks: \\[r = \\text{otsene mõju} + \\text{kaudne mõju} + \\text{kõrvalmõju} + \\text{ühismõju}\\] Seda omadust nimetatakse dekompositsiooni reegliks ja see võimaldab meil SEM-i raamistikus siduda andmed (korrelatsioonimaatriksi) hinnatava SEM-i mudeli parameetritega. Seega, kui meil on defineeritud kõikide mudelis olevate tunnuste vahelised teoreetilised mõjud27, siis saame nende tunnuste korrelatsioonimaatriksi alusel arvutada nende mõjude suurused. Näidismudelis oli 4 tunnust, mis tähendab, et nende vahel on 6 korrelatsiooni (joonis 8.7). Joonis 8.7: Rajaanalüüsi diagramm Need korrelatsioonid on lähtuvalt mudelis defineeritud mõjudest dekomponeeritavad järgmiselt: \\(r(Sissetulek, Haridus) = \\text{ühismõju} = \\phi \\hspace{9.5cm}\\) \\(r(Sissetulek, Staatus) = \\text{otsene mõju} + \\text{ühismõju} = b_1 + \\phi b_2 \\hspace{5cm}\\) \\(r(Sissetulek, Maailmavaade) = \\text{kaudne mõju} + \\text{ühismõju} = b_1b_4 + \\phi b_3 + \\phi b_2b_4 \\hspace{1cm}\\) \\(r(Haridus, Staatus) = \\text{otsene mõju} + \\text{ühismõju} = b_2 + \\phi b_1 \\hspace{6cm}\\) \\(r(Sissetulek, Staatus) = \\text{otsene mõju} + \\text{kaudne mõju} + \\text{ühismõju} = b_3 + b_2b_4 + \\phi b_1b_4\\) \\(r(Staatus, Maailmavaade) = \\text{otsene mõju} + \\text{kõrvalmõju} + \\text{ühismõju} = b_4 + b_2b_3 + b_1 \\phi b_3\\) Kui me need võrrandid lahendame (kus tundmatuteks on \\(b_1\\), \\(b_2\\), \\(b_3\\), \\(b_4\\)), siis saamegi lähutvalt korrelatsioonimaatriksist tuletada kõikide mõjude koefitsiendid. Ei hakka seda siin tegema, kuid keskkoolist (või oli see isegi põhikoolis) peaks meeles olema, et see on võimalik. 8.1.4 Rajaanalüüsi eeldused Nagu kõikidel meetoditl, on ka rajaanalüüsil rida eeldusi: - Kuna rajaanalüüs koosneb lineaarse regressiooni mudelitest, siis kehtivad neile ka regressiooni eeldused. Endogeensed (sõltuvad) tunnused peaksid olema enamvähem normaaljaotusega, tunnustevahelised seosed peaksid olema lineaarsed (tavalises rajaanalüüsis ei saa kasutada ka näiteks polünoome), regressiooni jäägid (residuals) ei tohiks olla korreleeritud eksogeensete (sõltumatute) tunnustega. - Kausaalsus peab olema ühesuunaline. Seda omadust nimetatakse ka rekursiivsuseks ning see tähendab, et kausaalsuse alguspunkt ja lõppunkt ei tohi olla samad (me ei saa modeleerida kana ja muna omavahelisi mõjusid: kana &gt; muna &gt; kana &gt; muna &gt; ). - Mudelis ei tohi olla multikollineaarsust. See tähendab, et tunnused, mille vahelist suhet ei ole mudeliga defineeritud ei tohiks olla tugevalt korreleeritud (kui nad on, siis peaksime vastavad seosed ka mudelis defineerima). Need eeldused kehtivad nn tavalise rajaanalüüsi kohta. On erinevaid spetsiifilisemaid (ja keerulisemaid) mudeleid, mille puhul nendest eeldustest saab kõrvale viilida. 8.2 Faktoranalüüs Faktoranalüüsi all mõeldakse üldjuhul (tegelikult küll järjest vähem) eksploratiivset faktoranalüüsi (explorative factor analysis ehk EFA)28. SEMi raamistikus peame aga faktoranalüüsi all silmas kinnitavat faktoranalüüsi (confirmatory factor analysis ehk CFA). Nii EFA kui CFA eesmärgiks on uurida mingeid hüpoteetilisi kontseptsioone nagu näiteks kultuuriline kapital, eluga rahulolu või sotsiaalmajanduslik staatus. Need on kontseptsioonid, mida me ei saa otse, ühe küsimusega mõõta, sest nad on niivõrd mitmetahulised ja komplekssed ning tihti ka subjektiivsed või kontekstsuaalsed. Küll saame aga mõõta nende kontseptsioonide erinevaid tahke. Näiteks kui tahame uurida eluga rahulou, siis võime küsida inimestelt nende rahulolu tööga, isikliku eluga, ühiskondliku staatusega jne. Faktoranalüüsi (nii EFA kui CFA) eesmärgiks ongi nendest erinevatest kontseptsiooni tahkudest tuletada seda kontseptsiooni kirjeldav latentne tunnus. Neid erinevaid küsimusi nimetatakse faktorindikaatoriteks ja tuletatud latentset tunnust faktoriks. Siinkohal tasub tähele panna indikaatorite ja faktorite kausaalse seose järjestust. Mis põhjustab mida? Kuigi indikaatorid on meil nö varem käes ja nende abil me tuletame faktorid, siis tegelik kausaalne järjestus on vastupidine. Mingi hüpoteetiline latentne konstrukt mõjutab indikaatoreid. Miks me ei saa lihtsalt välja arvutada nende erinevate tunnuste keskmist ning võtta seda kui uuritava kontstrukti koondtunnust? Siin saab välja tuua kaks põhjust. Esiteks, kuna üldjuhul on need meie uuritavad kontstruktid (sotsiaal-) psühholoogilised ja mõõdetud küsitluste abil, siis on need väga variatiivsed ja sisaldavd paratamatult teatud mõõtmisviga. Ehk siis meie koondtunnus võib mingitel juhtudel olla väga vale. CFA ja ka EFA võimaldavad mõõtmisviga (vähemalt teatud määrani) elimineerida. Teiseks võib juhtuda, et me oleme küll omast arust välja mõelnud väga geniaalsed küsimused mingi konstrukti mõõtmiseks, kuid tuleb välja, et respondentide arvates ei ole tegemist üldse mingi üheselt määratletava kontstruktiga. Et näiteks raholulu tööga ei ole üldse seotud rahuloluga isikliku eluga ja need on täiesti erinevad mentaalsed kontseptsioonid. Kui aga mingit meie hüpoteetilist konstrukti mõõtma mõeldud tunnuste vahel ei ole korrelatsiooni, siis see tähendab, et meie koondtunnus ei mõõdaks üldse midagi mõistlikku. Vähemalt mitte seda, mida me plaanisime mõõta. Nii CFA kui EFA lähtuvad latentsete tunnuste tuletamisel mõõdetud tunnuste vahelisest korrelatsioonist (või enamikel juhtudel tegelikult kovariatasioonist29) ning hea (st nii statistilistele kvaliteedikriteeriumitele vastava kui ka kontseptuaalselt aktsepteeritava) faktori eelduseks on indikaatorite vaheliste seoste olemasolu. Nende peamine olemus seisnebki indikaatorite ühise variatsiooni leidmises ning seeläbi nende suhete struktuuri (kovariatsiooni struktuuri) kirjeldamises. Ja kui eluga rahulolu tõesti jagunebki kaheks teineteisega mitteseotud rahulolu tüübiks, siis on need tüübid faktoranalüüsiga tuvastatavad ja eristatavad. Ka võib juhtuda, et tunnused, millega me rahulolu mõõdame, panustavad erineval määral üldisesse rahulolusse. Faktoranalüüsiga on võimalik arvestada erinevate tunnuste erinevate mõjude suurusega. Tavalise üldkeskmisega see võimalik ei ole. Siiani oleme rääkinud CFA-st ja EFA-st paralleelselt. Mis neid siis eristab? Nagu nimigi ütleb, on EFA puhtalt kirjeldav meetod. Selle abil saab mingite tunnuste kovariatsioonimaatriksi alusel leida neid tunnuseid ühendavaid ja eristavaid faktoreid. Piltlikult öeldes, me anname EFA-le mingi hunniku tunnuseid ja saame vastu neist moodustuvad faktorid30. CFA puhul tuleb meil aga konkreetne faktorstruktuur eelnevalt defineerida ning CFA võimaldab meil vaid kontrollida kas meie struktuur on adekvaatne ja ka tegelikkuses kehtiv. Kui EFA puhul laaduvad kõik tunnused kõikidesse faktoritesse, siis CFA puhul laadub üks tunnus üldjuhul ainult ühte faktorisse (kuigi faktoreid võib mudelis muidugi mitu olla). Ehk siis igal tunnusel on oma kindel eelnevalt defineeritud faktor. Tundub, nagu oleks EFA märksa mõistlikum meetod, mida kasutada - teeb ise kõik töö ära ja meil jääb üle vaid tulemused välja kirjutada. Kuid kui lähemalt mõtlema hakata, siis on CFA tegelikult teadusliku meetodiga tunduvalt kooskõlalisem. Teadlastena on meil teooriad, millest lähtuvalt püstitame testitavad hüpoteesid. CFA võimaldabki meil neid teoreetilisi hüpoteese testida ning neist laiemapõhjalisemaid järeldusi teha. Samas kui EFA abil saame vaid kirjeldada mingit konkreetset andmestikku ja selles leiduvaid seoseid, mis võivad väga vabalt olla tingitud konkreetse andmestiku eripäradest ja olla puhtalt juhuslikud. 8.2.1 Kinnitav faktoranalüüs CFA selgitamiseks ja kommunikeerimiseks on kõige mõistlikum kasutada diagramme. Joonisel 8.8 on kujutatud kolme faktoriline CFA mudel. Joonis 8.8: Kinnitava faktoranalüüsi mudel Indikaatortunnused on \\(x1\\)-\\(x9\\) ja nad on kujutatud kastide sees. Kastid annavad märku, et tegemist on vaadeldud/mõõdetud tunnustega. Indikaatorid on seletatud kolme latentse faktori poolt: \\(Faktor_1\\), \\(Faktor_2\\) ja \\(Faktor_3\\). Faktorid on ringide sees, mis annab märku, et tegemist on mudeli poolt hinnatud (estimated) tunnustega. Faktorite mõjude suurused ehk faktorlaadungid on tähistatud kui \\(\\lambda_1\\) - \\(\\lambda_9\\)31. Need on sisuliselt regressioonikordajad, mis kirjeldavad kui palju indikaator muutub, kui faktor muutub ühe ühiku võrra. Iga indikaatori juures on vea määrad ehk jäägid \\(\\epsilon_1\\)-\\(\\epsilon_9\\), mis koondavad informatsiooni, mida me faktoritega selgitada ei suuda, ehk siis iga indikaatori unikaalset variatsiooni. Mudeli seisukohast on mudeliga mitteseletatav variatsioon viga. Jäägid on jällegi ringide sees, andes märku, et need on mudeli poolt hinnatud ja mitte otseselt mõõdetud. Faktorite vahel on kahesuunalised nooled, mis tähistavad faktoritevahelist korrelatsiooni. Faktorid võivad põhimõtteliselt olla korreleeritud, kuigi me üldjuhul tahaksime, et see korrelatsioon oleks väike, st faktorid oleksid võimnalikult eripärased. Indikaatorite ja vigade vahel korrelatsioone aga olla ei tohiks. Kogu indikaatorite vaheline korrelatsioon peab olema selgitatud faktorite kaudu. Tasub jälgida noolte suundi. Nooled liiguvad faktorite ja jääkide poolt indikaatorite poole. Põhimõtteliselt on siin tegemist üheksa regressioonimudeliga, kus faktorid on sõltumatud tunnused, indikaatorid sõltuvad tunnused ning faktorlaadungid regressioonikoefitsiendid: \\[x_1 = \\lambda_1 Faktor_1 + \\epsilon_1\\] \\[x_2 = \\lambda_2 Faktor_1 + \\epsilon_2\\] \\[\\cdots\\] \\[x_9 = \\lambda_9 Faktor_3 + \\epsilon_9\\] 8.2.2 Kinnitava faktoranalüüsi loogika CFA aluseks on korrelatsioonimaatriks (tavaliselt pigem küll kovariatsioonimaatriks). Kuidas me nende korrelatsioonide abil faktorid, faktorlaadungid ja kõik muu kätte saame? Loogika on siin sarnane rajanalüüsi puhul täheldatule. Me kasutame dekompositsiooni reeglit, et lahutada regressioonid mõju komponentideks. Mõjude suunad ja struktuur on faktormudelis eelnevalt defineeritud. Seejärel saame jälle moodustada struktuurivõrrandid. Kui need lahendada siis saamegi kätte mõjude suurused (faktorlaadungid). Vaatame ühefaktorilist mudelit, mis on moodustatud kolme indikaatori põhjal (joonis 8.9). Joonis 8.9: Faktormudeli moodustamine Kolm indikaatorit annavad meile kolm korrelatsioonikoefitsienti (tabel 8.1). .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-f4bc3a76{}.cl-f4add3b4{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-f4add3b5{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-f4add3b6{font-family:'Arial';font-size:6.6pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;top:3.3pt;}.cl-f4ade26e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-f4ae579e{width:29.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae579f{width:35.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae57a0{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae57a1{width:29.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae57a2{width:35.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae57a3{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae57a4{width:29.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae57a5{width:35.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae57a6{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae57a7{width:29.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae57a8{width:35.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f4ae7eae{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Tabel 8.1: Korrelatsioonimaatriks y1y2y3y11y20.21y30.30.241 Millised mõjud meil mudelis on? Lisaks kolmele otsele mõjule (faktori ja indikaatorite vahel) on meil kolm kaudset mõju. Kuna mudelis indikaatorite vahel korrelatsioone defineeritud ei ole, siis peavad kõik indikaatorite vahelised korrelatsioonid olema vahendatud faktori poolt: \\[r(y_1, y_2) = \\text{kaudne mõju} = \\lambda_1 \\lambda_2\\] \\[r(y_2, y_3) = \\text{kaudne mõju} = \\lambda_2 \\lambda_3\\] \\[r(y_1, y_3) = \\text{kaudne mõju} = \\lambda_1 \\lambda_3\\] Kuna reaalsed indikaatorite vahelised korrelatsioonid on meil teada, siis saame need struktuurivõrrandid lahendada. Kolme võrrandiga ja kolme tundmatuga võrrand on küllaltki lihtne, seega teeme lahenduskäigu kiirelt läbi. Meil on struktuurivõrrandid: \\[r(y_1, y_2) = \\lambda_1 \\lambda_2 = 0.2\\] \\[r(y_2, y_3) = \\lambda_2 \\lambda_3 = 0.24\\] \\[r(y_1, y_3) = \\lambda_1 \\lambda_3 = 0.3\\] Mis on \\(\\lambda_2\\) väärtus? \\[\\frac{\\lambda_1 \\lambda_2}{\\lambda_1 \\lambda_3} = \\frac{0.2}{0.3} \\Rightarrow \\lambda_2 = \\frac{0.2 \\lambda_3}{0.3}\\] Ja kuna \\[\\lambda_2 \\lambda_3 = 0.24\\] siis saame asendada \\(\\lambda_2\\)-e eelmise võrrandiga: \\[0.24 = \\lambda_3(\\frac{0.2 \\lambda_3}{0.3})\\] Nüüd on meil ühe tundmatuga võrrand, mille saame lahendada: \\[\\lambda_3 = \\sqrt{0.24(\\frac{0.3}{0.2})} = 0.6\\] Teised faktorlaadungid saame kätte juba lihtsalt: \\[\\lambda_2 = \\frac{0.24}{\\lambda_3} = \\frac{0.24}{0.6} = 0.4\\] \\[\\lambda_1 = \\frac{0.3}{\\lambda_3} = \\frac{0.3}{0.6} = 0.5\\] Ja nii lihtne see ongi. Me suudame tunnustevaheliste korrelatsioonide abil leida faktorlaadungid, mis ühendavad neid tunnuseid mingi latentse tunnusega. Puudu on veel jäägid \\(\\epsilon_1\\) - \\(\\epsilon_3\\). Korrelatsioonikoefitsiendi puhul on meil tegemist ruutjuurega seletatud variatsioonist. Mõlemad korrelatsiooni osapooled selgitavad teise osapoole variatiivsusest \\(r^2\\) %-i. Ja kuna faktorlaadungid (standardiseeritud kujul) on lihtsalt korrelatsioonid faktori ja indikaatorite vahel32, siis saame nende abil leida nii faktori poolt seletatud kui ka seletamata indikaatorite variatiivsuse. \\[\\epsilon_1 = 1- \\lambda_1^2 = 1- 0.5^2 = 0.75\\] \\[\\epsilon_2 = 1- \\lambda_2^2 = 1- 0.4^2 = 0.84\\] \\[\\epsilon_3 = 1- \\lambda_3^2 = 1- 0.6^2 = 0.64\\] Ja lõplik, meie päris enda välja arvutatud faktormudeli diagramm33: Joonis 8.10: Faktormudeli moodustamine 8.2.3 Vabadusastmed Faktorlaadungite leidmiseks pidime kasutama kõiki kolme korrelatsioonikoefitsienti. Oleks korrelatsioone olnud üks vähem, siis me ei oleks saanud võrrandeid lahendada. Mis aga juhtuks kui mudelis oleks veel üks indikaator? Sellisel juhul oleks korrelatsioonimaatriksis 6 koefitsienti (\\(4 \\times 4 \\div2 - 4\\)) ning peaksime hindama 4 faktorlaadungit. Seega saaksime juurde 3 korrelatsiooni, kuid peaksime hindama vaid ühe lisaparameetri (tabel 8.2). .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-f69d8b74{}.cl-f6919d46{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-f6919d47{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-f6919d48{font-family:'Arial';font-size:6.6pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;top:3.3pt;}.cl-f691eb20{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-f6923954{width:29.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f6923955{width:35.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f6923956{width:27.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f6923957{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f6923958{width:29.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f6923959{width:27.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f692395a{width:35.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f692395b{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f692395c{width:27.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f692395d{width:29.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f692395e{width:35.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f692606e{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f692606f{width:29.1pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f6926070{width:41.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f6926071{width:27.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f6926072{width:35.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Tabel 8.2: Korrelatsioonimaatriks y1y2y3y4Xy11y20.21y30.30.241y40.30.20.41X0.50.40.6?1 Neljanda faktorlaadungi saame välja arvutada ühe lisandunud korrelatsiooni abil: \\[\\lambda_4 = \\frac{r(y_1, y_4)}{\\lambda_1} = \\frac{0.3}{0.5}\\] Ehk siis mudelis on nüüd kuus ühikut infot, millest kõigi parameetrite hindamiseks on vaja vaid nelja ühikut. Neid üle jäänud infoühikuid nimetatakse vabadusastemeteks (degrees of freedom või df). Kui vabadusastemid on rohkem kui 0, siis ütleme, et meie mudel on üleidentifitseeritud. Kui vabadusastmeid on täpselt 0, siis on mudel identifitseeritud. Ja kui vabadusastemid on vähem kui 0, ehk siis meil on vähem infoühikuid kui parameetreid, siis mudel ei ole identifitseeritud. Sellisel juhul me ei saa tuletada andmetest unikaalseid parameetreid ja ei saa mudelit hinnata. Näiteks ei ole võimalik hinnata kahe indikaatoriga mudelit. Sellisel juhul oleks korrelatsioone üks, kuid parameetreid kaks. 8.2.4 Mudeli sobivus Tahame üldjuhul alati, et mudel oleks üleidentifitseeritud. Kui meil on infoühikuid üle, siis saame neid kasutada mudeli kvaliteedi hindamiseks. Täpsemalt saame hinnata, kuivõrd hästi mudel sobitub meie andmetega. Antud juhul jäi meil üle kaks korrelatsiooni. Lisaks selle, et meil on need algsed korrelatsioonid teada, saame need tuletada ka juba välja arvutatud faktorlaadungitest: \\[r(y_2, y_4) = 0.2 \\text{ ja } r(y_2, y_4) = \\lambda_2 \\lambda_4\\] \\[r(y_3, y_4) = 0.4 \\text{ ja } r(y_3, y_4) = \\lambda_3 \\lambda_4\\] Kui need mõlemad väärtused, algne korrelatsioon ja tuletatud korrelatsioon, on samad, siis saame järeldada, et meie defineeritud struktuur kirjeldab täpselt andmetes olevaid seoseid. Kui need aga ei kattu, siis oleme mingil määral oma mudeli valesti defineerinud. Arvutame algsete korrelatsioonide ja tuletatud korreltasioonide vahed: \\[r(y_2, y_4) - \\lambda_2 \\lambda_4 = 0.2 - 0.4\\times0.6 = -0.04\\] \\[r(y_3, y_4) - \\lambda_3 \\lambda_4 = 0.4 - 0.6\\times0.6 = 0.04\\] eed erinevused on mudeli jäägid (residuals). Saame nad arvutada kõikide korreltasioonikordajate jaoks (sellisel juhul kasutame parameetrite arvutamiseks lihtsalt teisi korrelatsioone). Kui need jäägid on suured, siis see tähendab, et meie defineeritud mudel ei vasta andmetele (ja on valesti defineeritud). See tähendab, et meie mudeli parameetrid võivad olla valed, nende standardvead võivad olla valed ja seega ka mudelist tehtavad järeldused võivad olla valed. Korrelatsioonimaatriksit (või kovariatsioonimaatriksit), mis moodustub nendest mudeli põhjal tuletatud korrelatsioonidest, nimetatakse mudelipõhiseks maatriksiks (model-implied matrix või fitted matrix). Algsete korrelatsioonide ning tuletatud korrelatsioonide vahe maatriksit nimetatakse jääkide maatriksiks (residual matrix). Kui mudel sobitub andmetega väga hästi, siis ei tähenda see automaatselt, et meil on tegemist hea mudeliga. Võib juhtuda, et hästi sobituvuva mudeli puhul ei ole meie faktorlaadungid olulised (ehk siis indikaatorid ei ole omavahel seotud) või on mudel teoreetiliselt mittesobiv. 8.2.5 Veel mudeli identifikatsioonist Me oleme näidetena käsitlenud korrelatsioonimaatriksitel põhinevaid mudeleid, kuid tegelikult kasutatakse enamikul juhtudel mudeli alustena kovariatsioonimaatrikseid. Lisaks saab mudelitesse lisada ka indikaatorite keskmisi (neid käsitletakse sel juhul analoogselt regressioonimudelite vabaliikmetega). Mõningad keerulisemad mudelid eeldavad kogu andmestiku olemasolu. Tegelikult see sisendi kuju ei olegi väga tähtis, sest reaalsuses anname me SEM-i programmidesse sisendina nii ehk naa kogu andmestiku ning programm keerab selle ise vajalikku vormi. Kuid keerulisemate sisenditega kaasneb ka muutus mudelite identifitseerimise loogikas. Kui sisendiks on kovariatsioonimaatriks, siis saame kaasa ka iga indikaatori dispersiooni. Ehk meil on kasutada rohkem infot, kuid samas peame hindama ka rohkem parameetreid. Kui mudelis on ka keskmised (vabaliikmed), siis peame hindama iga indikaatori vabaliikme ning ka faktortunnuse keskmise. Ehk kui meil on keskmistega mudel kolme indikaatoriga, siis meil oleks vaja nelja keskmist, kuid on ainult kolm. Probleem on ka faktori dispersiooniga. Kuna faktor on latentne tunnus siis ei ole sellel mingit konkreetset skaalat ning meil ei ole mingit referentspunkti, millest lähtuda. Võtame järgmise näite. Meil on 5 tunnusega ühefaktoriline mudel. Kasutada on 20 infoühikut: 5 keskmist 5 dispersiooni 10 kovariatsiooni Faktormudelis (keskmistega mudelis) peab olema defineeritud 17 parameetrit: 5 faktorlaadungit 5 jääkide variatsiooni 5 vabaliiget 1 faktori keskmine 1 faktori dispersioon Kuigi meil on 3 vabadusastet, pole mudel ometi identifitseeritud. Puudu on üks dispersioon ja üks keskmine. Mida siis teha? Variante on tegelikult mitu. Kui tegemist on ilma keskmisteta mudeliga, siis peame valima ühe kahest variandist: Fikseerime iga faktori puhul ühe faktorlaadungi \\(1\\)-ks. Sellega anname faktori skaalale referentspunkti, mis võimaldab määrata dispersiooni. Fikseerime faktori dispersiooni \\(1\\)-ks (oma näites kasutasime vaikimisi seda varianti) Kui meil on tegemist keskmistega mudeliga, siis peame lisaks kasutama veel ühte kahest piirangust: Fikseerime ühe vabaliikme \\(0\\)-ks Fikseerime faktori keskmise \\(0\\)-ks SEM-i programmid üldjuhul kasutavad neid piiranguid vajaduse korral automaatselt. Seega me ei peaks väga palju selle identifitseerimise pärast pead valutama. Tasub lihtsalt sellest asjast teadlik olla. Üldjuhul on automaatselt kasutatavateks piiranguteks ühe faktorlaadungi \\(1\\)-na fikseerimine ning keskmistega mudeli korral faktori keskmise fikseerimine. 8.3 Sruktuurivõrrandite mudelid Lõpuks oleme jõudnud ka põhiteema, SEM-i, juurde. Kuid õnneks siin enam väga millestki rääkida ei olegi. SEM-iks nimetataksegi mudelit, milles on kinnitav faktoranalüüs ja rajaanalüüs kokku pandud. See tähendab mudelit, kus on mingid latentsed tunnused ning analüüsitakse latentsete tunnuste vahelisi või latentsete tunnuste ja eksogeensete tunnuste vahelisi mõjusid. SEM-i mudel jaguneb mõõtmismudeliks (faktoranalüüs) ja struktuurimudeliks (rajaanalüüs või ka tavaline regressioon) (joonis 8.11). Joonis 8.11: Struktuurivõrrandite mudeli diagramm Joonisel 8.11 on kujutatud küllaltki lihtne SEM-i mudel. Mõõtmismudelina on defineeritud kaks kolme indikaatoriga faktorit. Struktuurimudelis on defineeritud Faktor 1-e mõju Faktor 2-le (\\(\\beta_1\\)). Lisaks on struktuurimudelis eksogeenne tunnus \\(x\\), mis mõjutab nii Faktor 1-te (\\(\\beta_2\\)) kui ka Faktor 2-te (\\(\\beta_3\\)) 8.4 Struktuurivõrrandite mudelid Ris R-is on SEM-i jaoks mitmeid pakette. Neist peamised on lavaan, sem ja openMX. Neist kõige võimekam (minu subjektiivse arvamuse järgi) ja tõenäoliselt ka populaarseim on lavaan. Tegemist on väga intensiivselt arendatava paketiga, mida näitab ka asjaolu, et kuigi pakett on ligi 10 aastat vana, siis on see ikka veel beeta-versioon staatuses. Ehk siis uusi võimalusi tuleb pidevalt peale ning pakett ei ole siiani valmis saanud. lavaan-i kodulehelt on võimalik paketi kohta täpsemalt lugeda. Seal on ka hulgaliselt tutoriale ja muid materjale. lavaan on oma süntaksilt, väljundilt ja ka tehniliselt küllaltki sarnande MPlus-i programmile. Mplus on päris kindlasti kõige parem, arenenum ja suurimate võimalustega SEM-i tarkvara, kuid paraku mitte vabavara, nagu R ja lavaan. Kuid kui on vaja mingeid keerulisemaid või spetsiifilisemaid mudeleid teha, siis tasub sinnapoole vaadata. Jällegi, MPlus-i kodulehelt saab lisaks programmile seonduvale lugeda ka palju muud põnevat SEM-i kohta. Kõigepealt loeme sisse vajalikud paketid. Ja nagu alati, kui mõnda paketti ei ole eelnevalt installitud, siis tuleb seda teha (install.packages() funktsiooniga, kus paketi nimi peab olema jutumärkides). library(lavaan) library(semPlot) library(dplyr) 8.4.1 lavaani süntaks lavaan-is tuleb mudeli võrrand defineerida formula formaadis tekstilise objektina. See tähendab, et kogu mudeli definitsioon peaks olema jutumärkide sees (ja Rstudio peaks selle roheliseks värvima). Tavaline regressioon on defineeritav identselt tavalisele regressioonile lm funktsioonis, kus sõltuv tunnus on eraldatud sõltumatutsest tildega ja sõltumatud tunnused üksteisest plussmärgiga (koosmõjude puhul * või : märgiga): mod &lt;- &#39;y ~ x1 + x2&#39; mod &lt;- &#39;y ~ x1 * x2&#39; # koosmõjudega mudel SEMis tuleb meil pea alati defineerida mitu võrrandit. Sellisel juhul peavad nad kõik olema ühe tekstilise objekti sees ning erinevatel ridadel: mod &lt;- &#39;y1 ~ x1 + x2 y2 ~ x2 + x3 + x4 y1 ~ y2&#39; Faktorid tuleb defineerida =~ märgiga: mod &lt;- &#39;f1 =~ y1 + y2 + y3 f2 =~ y4 + y5 + y6&#39; Variatsioonid ja kovariatsioonid saab vajadusel defineerida kahe tildega: mod &lt;- &#39; y1 ~~ y2 f1 ~~ f2 &#39; Andmestikuna kasutame lavaan-iga kaasa olevat näidisandmestikkus PoliticalDemocracy. Andmestik sisaldab demokraatia ja majanduse mõõdikuid erinevate riikide kohta. Täpsemalt saab andmestiku kohta lugeda selle abifailist help(PoliticalDemocracy) # paneme andmed mugavama nime alla dt &lt;- PoliticalDemocracy # muudame ka mõned tunnuste nimed arusaadavamaks dt &lt;- dt %&gt;% rename(free_press_60 = y1, fair_elect_60 = y3, fair_elect_65 = y7, gnp_60 = x1) 8.4.2 Regressioon Kuigi lavaan on mõeldud eelkõige SEM-i jaoks, siis saab sellega väga edukalt ka tavalist regressiooni jooksutada. Mis on ka iseenesest mõistetav, arvestades asjaolu, et SEM ju suures osas ongi laiendatud regressioonanalüüs. Vaatame kuidas on valimisvabadus seotud pressivabadusega. Selleks peame kõigepealt defineerima mudeli ning seejärel seda jooksutama funktsiooniga sem(). Tulemusi summary() funktsiooniga (nagu tavalise regressiooni puhulgi). # Defineeerime mudeli mod &lt;- &#39;fair_elect_60 ~ free_press_60&#39; # Jooksutame mudelit fit &lt;- sem(mod, data = dt) summary(fit) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 2 ## ## Number of observations 75 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## fair_elect_60 ~ ## free_press_60 0.849 0.106 8.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .fair_elect_60 5.731 0.936 6.124 0.000 Saame kätte regressioonikoefitsiendi, selle p väärtuse ja sõltuva tunnuse disprsiooni. Kuid puudu on vabaliige. Kuna SEM on üldjoontes kovariatsiooni struktuuride analüüsimise raamistik, siis vaikimisi vabaliiget mudelisse ei kaasta. Üldiselt SEMi mudelite puhul see meid niiväga ka ei huvita. Me tahame näha pigem mõjusid ja seoseid. Kuid vajadusel saame vabaliikme lisada meanstructure = T argumendiga. Veel on puudu \\(R^2\\). Selle saame samuti juurde panna rsquare=T argumendiga. Ka F-testi meile ei näidata. Ja seda ka ei saa, kuna hindamine toimub maximum likelihood meetodiga. Samamoodi ei saa me F-testi glm() funktsiooniga. Paneme mainitud argumendid juurde ja jooksutame koodi uuesti: # defineeerime mudeli mod &lt;- &#39;fair_elect_60 ~ free_press_60&#39; # Jooksutame mudelit fit &lt;- sem(mod, data = dt, meanstructure = T) summary(fit, rsquare=T) ## lavaan 0.6-10 ended normally after 15 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Number of observations 75 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## fair_elect_60 ~ ## free_press_60 0.849 0.106 8.000 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .fair_elect_60 1.924 0.642 2.996 0.003 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .fair_elect_60 5.731 0.936 6.124 0.000 ## ## R-Square: ## Estimate ## fair_elect_60 0.460 Nüüd on olemas nii \\(R^2\\) kui vabaliige. Vaatame, kas tulemused on sarnased lm() funktsioonile. summary(lm(fair_elect_60 ~ free_press_60, data = dt)) ## ## Call: ## lm(formula = fair_elect_60 ~ free_press_60, data = dt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.7629 -1.0838 0.2313 1.7092 3.9162 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9245 0.6511 2.956 0.0042 ** ## free_press_60 0.8488 0.1075 7.893 2.24e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.426 on 73 degrees of freedom ## Multiple R-squared: 0.4604, Adjusted R-squared: 0.453 ## F-statistic: 62.29 on 1 and 73 DF, p-value: 2.24e-11 Suhteliselt sarnased. Mõningane erinevus tulenebki erinevatest hindamismeetoditest ja võimalik et mingitest ümardamistest vms. Tavalise regresiooniga saame ka RSE (Residual standard error) ehk jääkide standardvea. See näitab regressiooni jääkide, mudeli (ehk sõltuva tunnuse) seletamata jäänud variatsiooni keskmist suurust. Kuigi lavaan-i väljundis seda numbrit ei ole, siis see väärtus on seal ikkagi olemas. Nimelt sõltuva tunnuse dispersiooni näol. Disepersioon on standardhälve ruudus, seega: sqrt(5.731) ## [1] 2.393951 8.4.3 Rajaanalüüs Defineerime rajaanalüüsi mudeli, milles vaatame kuidas valimisvabadus 1965 aastal on mõjutatud valimisvabadusest 1960 aastal, pressivabadusest 1960 aastal ja rahvamajanduse kogutoodangust (GNP) 1960 aastal. Eeldame et valimisvabadus 1960 aastal on omakorda mõjutatud ajakirjandusvabadusest 1960 aastal pressivabadus 1960 aastal on mõjutatud rahvamajanduse kogutoodangust (GNP). See ei ole nüüd mingi väga sisuline hüpotees. Kuid võiks ju eeldada, et vabad valimised on eelduseks järgmistele vabadele valimistele, pressivabadus on eelduseks nii praegustele kui järgmistele valimistele ning rikkamad rikkamates (arenenumates) riikides on nii valimiste kui ajakirjandusega paremad lood. Keerulisemaid mudeleid on kirjalikult suhteliselt keeruline defineerida. Siin aitab see, kui need eelnevalt näiteks üles joonistada. Nii on kõik silme ees ning lihtsam neid ka kirja panna. mod &lt;- &#39;fair_elect_65 ~ fair_elect_60 + free_press_60 + gnp_60 fair_elect_60 ~ free_press_60 free_press_60 ~ gnp_60&#39; fit &lt;- sem(mod, data = dt) # Kasutame standardiseeritud lahendit summary(fit, standardized=T) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 8 ## ## Number of observations 75 ## ## Model Test User Model: ## ## Test statistic 0.756 ## Degrees of freedom 1 ## P-value (Chi-square) 0.385 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fair_elect_65 ~ ## fair_elect_60 0.341 0.107 3.181 0.001 0.341 0.341 ## free_press_60 0.495 0.140 3.532 0.000 0.495 0.396 ## gnp_60 0.568 0.381 1.490 0.136 0.568 0.127 ## fair_elect_60 ~ ## free_press_60 0.849 0.106 8.000 0.000 0.849 0.679 ## free_press_60 ~ ## gnp_60 1.367 0.382 3.580 0.000 1.367 0.382 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .fair_elect_65 4.936 0.806 6.124 0.000 4.936 0.466 ## .fair_elect_60 5.731 0.936 6.124 0.000 5.731 0.540 ## .free_press_60 5.796 0.947 6.124 0.000 5.796 0.854 Standardiseeritud lahend on tulbas Std.all. Näeme, et meie hüpoteesid pidasid suures osas paika. Kõik mõjud on positiivsed ja olulised (p &gt; 0.5). Erandiks on rahvamajanduse kogutoodangu mõju valimisvabadusele 65 aastal. Seega oleks otstarbekas see mõju mudelist välja jätta. Lihtsam on rajaanalüüsist aru saada rajadiagrammi põhjal. Funktsioon semPath() võimaldam mudeli alusel selle diagrammi mugavlt välja joonistada- See ei tee just kõige ilusamaid jooniseid, avaldamiseks päris ei sobi, kuid ülevaate saab selle kaudu kätte. semPaths(fit, node.width = 2, edge.label.cex = 1, what = &quot;paths&quot;, whatLabels = &#39;stand&#39;) Jätame GNP mõju valimisvabadusele 65 aastal välja ja jooksutame mudeli uuesti (salvestame selle ka teise mudeliobjekti): mod &lt;- &#39;fair_elect_65 ~ fair_elect_60 + free_press_60 fair_elect_60 ~ free_press_60 free_press_60 ~ gnp_60&#39; fit1 &lt;- sem(mod, data = dt) # Kasutame standardiseeritud lahendit summary(fit1, standardized=T) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 7 ## ## Number of observations 75 ## ## Model Test User Model: ## ## Test statistic 2.922 ## Degrees of freedom 2 ## P-value (Chi-square) 0.232 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fair_elect_65 ~ ## fair_elect_60 0.357 0.109 3.283 0.001 0.357 0.356 ## free_press_60 0.542 0.136 3.985 0.000 0.542 0.433 ## fair_elect_60 ~ ## free_press_60 0.849 0.106 8.000 0.000 0.849 0.679 ## free_press_60 ~ ## gnp_60 1.367 0.382 3.580 0.000 1.367 0.382 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .fair_elect_65 5.080 0.830 6.124 0.000 5.080 0.477 ## .fair_elect_60 5.731 0.936 6.124 0.000 5.731 0.540 ## .free_press_60 5.796 0.947 6.124 0.000 5.796 0.854 Kasutame anova() funktsiooni, et hinnata, kas saime parema mudeli: anova(fit, fit1) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit 1 1037.0 1055.5 0.7562 ## fit1 2 1037.2 1053.4 2.9217 2.1655 1 0.1411 p väärtus on &gt; 0.05, mis tähendab, et keerulisem mudel (fit) ei erine oluliselt lihtsamast mudelist (fit1). Seega jääme lihtsama mudeli juurde (kui keeruline ja lihtne mudel seletavad ühepalju variatiivsust, siis eelistame alati lihtsamat). 8.4.4 Mudeli väljund lavaan-i mudeli objektis on teglikult peidus märksa enam infot kui summary() funktsioon vakimisi välja annab. Erinevatele testidele ja sobivusindeksitele saab ligi kui summary() funktsioonis kasutada argumenti fit.measures=T summary(fit1, fit.measures=T) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 7 ## ## Number of observations 75 ## ## Model Test User Model: ## ## Test statistic 2.922 ## Degrees of freedom 2 ## P-value (Chi-square) 0.232 ## ## Model Test Baseline Model: ## ## Test statistic 116.586 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.992 ## Tucker-Lewis Index (TLI) 0.975 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -511.574 ## Loglikelihood unrestricted model (H1) -510.113 ## ## Akaike (AIC) 1037.148 ## Bayesian (BIC) 1053.370 ## Sample-size adjusted Bayesian (BIC) 1031.308 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.078 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.256 ## P-value RMSEA &lt;= 0.05 0.294 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.047 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## fair_elect_65 ~ ## fair_elect_60 0.357 0.109 3.283 0.001 ## free_press_60 0.542 0.136 3.985 0.000 ## fair_elect_60 ~ ## free_press_60 0.849 0.106 8.000 0.000 ## free_press_60 ~ ## gnp_60 1.367 0.382 3.580 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .fair_elect_65 5.080 0.830 6.124 0.000 ## .fair_elect_60 5.731 0.936 6.124 0.000 ## .free_press_60 5.796 0.947 6.124 0.000 Nüüd tundub, et seda infot on jälle liiga palju. Kuid käime väljundi lõik lõigu haaval läbi ja vaatame, mis selles kasulikku on. ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 7 ## ## Number of observations 75 Esmalt antakse meile teada mõned tehnilised detailid. Esimesest reast peaksime teada saama, et mudeli hindamisel ei olnud probleeme. Järgmistest ridades, et hindamisel kasutati maximum likelihood meetodit (saaksime kasutada kui meetodeid, kuid ML on vast kõige levinum). Meil on 75 vaatlust ning mudeli vabadusastmeid on 7. ## Model Test User Model: ## ## Test statistic 2.922 ## Degrees of freedom 2 ## P-value (Chi-square) 0.232 Model Test User Model on hii ruut test, mis võrdleb mudeli sobitatud kovariatsioonimaatriksit (implised covariance matrix) algse andmetel põhineva kovariatsioonimaatriksiga. Nullhüotees on siin see, et maatriksid ei ole erinevad, ehk siis see, mida me tegelikult tahamegi - reprodutseerida oma mudeli struktuuri abil andmestikus leiduvaid seoseid. Seega meile sobiks, kui p väärtus oleks siin &gt; 0.05. Probleem on aga see, et enamike mudelite puhul, kus meil on palju vaatlusi, on see test suhteliselt kasutu. Hii-ruut test on suurte andmestike puhul lihtsalt natuke liiga tundlik ning annab meile ka suhteliselt hästi sobituvate maatriksite puhil negatiivse tulemuse (p &lt; 0.05). Seetõttu, kui valim oleks suurem (üle 400), siis ei peaks sellele testile ülemäära palju tähelepanu pöörama. ## Model Test Baseline Model: ## ## Test statistic 116.586 ## Degrees of freedom 6 ## P-value 0.000 Model Test Baseline Model testiga võrreldakse nn sõltumatuse mudelit või nullmudelit (kus tunnuste vahel ei ole kovariatsioone) meie defineeritud mudeliga. Kui p väärtus on väike, siis meie mudel erineb oluliselt nullmudelist, mis tähendab, et meie mudelis on olulisi seoseid. ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.992 ## Tucker-Lewis Index (TLI) 0.975 CFI ja TLI on sobivusindeksid. Nad võiksid olla suuremad kui 0.95 (mõne allika järgi ka suuremad kui 0.9). ## Loglikelihood user model (H0) -511.574 ## Loglikelihood unrestricted model (H1) -510.113 ## ## Akaike (AIC) 1037.148 ## Bayesian (BIC) 1053.370 ## Sample-size adjusted Bayesian (BIC) 1031.308 AIC ja BIC on informatsioonikriteeriumid. Need ei ole iseseisvalt eriti informatiivsed (vaatamata nimele), kuid on kasulikud erinevate mudelite võrdlemisel. Väiksem väärtus on parem. ## Root Mean Square Error of Approximation: ## ## RMSEA 0.078 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.256 ## P-value RMSEA &lt;= 0.05 0.294 RMSEA võrdleb defineeritud mudelit ja küllastunud mudelit. See võiks olla väiksem kui 0.05. Mõnede allikate järgi on ka kuni 0.08 veel enamvähem. ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.047 SRMR on tuletatud erinevusest defineeritud mudeli ja küllastunud mudeli (saturated model) kovariatsioonimaatriksite vahel. Väiksem kui 0.08 on hea. Kõige tavapärasemad raporteeritavad sobivusnäitajad on CFI, TLI, RMSEA ja SRMR. Loomulikult me tahaksime, et kõik sobivuskriteeriumid oleksid täidetud, kuid tihti leoitakse ka mudeliga, mille mõni kriteerium on natuke üle või alla piirmäära. Seda seetõttu, et ükski neist piirmääradest ei ole absoluutne. Erinevad allikad viitavad erinevatele määradele ning seega on need kriteeriumid küllaltki ebamäärased. Kui aga kõik või enamik neist siitavad ebasobivale mudelile, siis tasub muidugi olla oma mudeli suhtes kriitiline. 8.4.5 Mudeli intspekteerimine Isegi siis, kuisobivusindeksid mahtusid meie piirmäärade sisse, peab alati mudelit lähemalt intspekteerima. Sobivusnäitajad annavad märku mudeli üldisest sobivusest (global fit). Lisaks sellele on nn kohalik sobivus (local fit), mis käsitleb erinevate konkreetsete parameetrite andmetega sobivust. Selleks saame esmalt vaadata ja võrrelda mudeli sobitatud kovariatsioonimaatriksit (implised covariance matrix) algse andmetel põhineva kovariatsioonimaatriksiga. Mõistlik oleks vaadata nende standardiseeritud variante ehk korrelatsioonimaatrikseid. Algse korrelatsioonimaatriksi saame funktsiooniga lavCor(): lavCor(fit1) ## fr__65 fr_l_60 fr_p_60 gnp_60 ## fair_elect_65 1.000 ## fair_elect_60 0.650 1.000 ## free_press_60 0.674 0.679 1.000 ## gnp_60 0.389 0.327 0.382 1.000 Sobitatud korrelatsioonimaatriksi funktsiooniga inspect(): inspect(fit1, what=&quot;cor.all&quot;) ## fr__65 fr_l_60 fr_p_60 gnp_60 ## fair_elect_65 1.000 ## fair_elect_60 0.650 1.000 ## free_press_60 0.674 0.679 1.000 ## gnp_60 0.258 0.259 0.382 1.000 Saame võõrelda nende erinevusi, lahutame ühest teise (kuna tegemist on maatriksitega, siis saame seda lihtsalt teha): lavCor(fit1) - inspect(fit1, what=&quot;cor.all&quot;) ## fr__65 fr_l_60 fr_p_60 gnp_60 ## fair_elect_65 0.000 ## fair_elect_60 0.000 0.000 ## free_press_60 0.000 0.000 0.000 ## gnp_60 0.131 0.068 0.000 0.000 Veelgi lihtsam moodus oleks kasutada residuals()funktsiooni (kui tahame korrelatsioonide erinevust, siis argumen type = 'cor') residuals(fit1, type = &#39;cor&#39;) ## $type ## [1] &quot;cor.bollen&quot; ## ## $cov ## fr__65 fr_l_60 fr_p_60 gnp_60 ## fair_elect_65 0.000 ## fair_elect_60 0.000 0.000 ## free_press_60 0.000 0.000 0.000 ## gnp_60 0.131 0.068 0.000 0.000 Jääkide maatriksist peaks otsima suuremaid väärtusi, mis annavad tunnistust, et mingi tunnuste vahelised seosed ei ole meie defineeritud mudelis vajalikul määral esindatud. Lähtuvalt sellest saame me oma mudelisse seoseid lisada. Peame muidugi arvestama nende teoreetilise adekvaatsusega ning ka statistilise olulisusega. Jätsime enne GNP ja valimisvabaduse seose mudelist välja kuna see ei olnud statistiliselt oluline. Jääkide maatriks näitab, et mingi seos nende vahel siiski on. Antud juhul mitte piisav, et see otsese mõjuna mudelisse kaasata. Kuid võime näiteks kaaluda mõnda kaudset mõju. Lisaks jääkide maatriksile saame kohalikku sobivust analüüsida modifikatsiooniindeksite põhjal: modindices(fit1) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 10 fair_elect_65 ~~ free_press_60 2.113 -2.384 -2.384 -0.439 -0.439 ## 11 fair_elect_60 ~~ free_press_60 0.752 -1.511 -1.511 -0.262 -0.262 ## 12 fair_elect_65 ~ gnp_60 2.113 0.562 0.562 0.125 0.172 ## 14 fair_elect_60 ~ gnp_60 0.752 0.356 0.356 0.080 0.109 ## 15 free_press_60 ~ fair_elect_65 2.778 -0.503 -0.503 -0.630 -0.630 ## 16 free_press_60 ~ fair_elect_60 0.752 -0.264 -0.264 -0.330 -0.330 ## 17 gnp_60 ~ fair_elect_65 2.778 0.063 0.063 0.282 0.282 ## 18 gnp_60 ~ fair_elect_60 0.752 0.033 0.033 0.148 0.148 Siin tasun vaadata veergu mi, mis näitab kui palju meie mudel läheks paremaks kui mõne seose mudelisse lisaks või ära jätaks (~ näitab regressioonseost, ~~ näitab kovariatsiooni). 8.4.6 Kinnitav faktoranalüüs Kinnitava faktoranalüüsi jaoks on lavaan-is funktsioon cfa() (see on tegelikult suures osas identne sem() funktsiooniga, mõned vaikeväärtused on lihtsalt muudetud). Mudeli loomine ja jooksutamine on analoogne rajaanalüüsiga. Ka sobivusindeksid, testid ja hilisema mudeli intspekteerimise funktsioonid on samad. Kasutame näidisena samat demoktia indikaatorite andmestikku ja moodustame kolm faktorit (majandusnäitajate faktor, demokraatia 1960 faktor ja demokraatia 1965 faktor) # Tõmbame andmestiku uuesti sisse, kuid seekord ei muuda nimesid (hoiame ruumi kokku) dt &lt;- PoliticalDemocracy # Defineerime mudeli mod &lt;- &#39; maj60 =~ x1 + x2 + x3 dem60 =~ y1 + y2 + y3 + y4 dem65 =~ y5 + y6 + y7 + y8 &#39; fit &lt;- cfa(mod, data = dt) summary(fit, fit.measures = TRUE, standardized = T) ## lavaan 0.6-10 ended normally after 47 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 25 ## ## Number of observations 75 ## ## Model Test User Model: ## ## Test statistic 72.462 ## Degrees of freedom 41 ## P-value (Chi-square) 0.002 ## ## Model Test Baseline Model: ## ## Test statistic 730.654 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.953 ## Tucker-Lewis Index (TLI) 0.938 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1564.959 ## Loglikelihood unrestricted model (H1) -1528.728 ## ## Akaike (AIC) 3179.918 ## Bayesian (BIC) 3237.855 ## Sample-size adjusted Bayesian (BIC) 3159.062 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.101 ## 90 Percent confidence interval - lower 0.061 ## 90 Percent confidence interval - upper 0.139 ## P-value RMSEA &lt;= 0.05 0.021 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.055 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## maj60 =~ ## x1 1.000 0.669 0.920 ## x2 2.182 0.139 15.714 0.000 1.461 0.973 ## x3 1.819 0.152 11.956 0.000 1.218 0.872 ## dem60 =~ ## y1 1.000 2.201 0.845 ## y2 1.354 0.175 7.755 0.000 2.980 0.760 ## y3 1.044 0.150 6.961 0.000 2.298 0.705 ## y4 1.300 0.138 9.412 0.000 2.860 0.860 ## dem65 =~ ## y5 1.000 2.084 0.803 ## y6 1.258 0.164 7.651 0.000 2.623 0.783 ## y7 1.282 0.158 8.137 0.000 2.673 0.819 ## y8 1.310 0.154 8.529 0.000 2.730 0.847 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## maj60 ~~ ## dem60 0.660 0.206 3.202 0.001 0.448 0.448 ## dem65 0.774 0.208 3.715 0.000 0.555 0.555 ## dem60 ~~ ## dem65 4.487 0.911 4.924 0.000 0.978 0.978 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x1 0.082 0.020 4.180 0.000 0.082 0.154 ## .x2 0.118 0.070 1.689 0.091 0.118 0.053 ## .x3 0.467 0.090 5.174 0.000 0.467 0.240 ## .y1 1.942 0.395 4.910 0.000 1.942 0.286 ## .y2 6.490 1.185 5.479 0.000 6.490 0.422 ## .y3 5.340 0.943 5.662 0.000 5.340 0.503 ## .y4 2.887 0.610 4.731 0.000 2.887 0.261 ## .y5 2.390 0.447 5.351 0.000 2.390 0.355 ## .y6 4.343 0.796 5.456 0.000 4.343 0.387 ## .y7 3.510 0.668 5.252 0.000 3.510 0.329 ## .y8 2.940 0.586 5.019 0.000 2.940 0.283 ## maj60 0.448 0.087 5.169 0.000 1.000 1.000 ## dem60 4.845 1.088 4.453 0.000 1.000 1.000 ## dem65 4.345 1.051 4.134 0.000 1.000 1.000 Mida me väljundist näeme? Vaatame kõigepealt parameetreid ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## maj60 =~ ## x1 1.000 ## x2 2.182 0.139 15.714 0.000 ## x3 1.819 0.152 11.956 0.000 ## dem60 =~ ## y1 1.000 ## y2 1.354 0.175 7.755 0.000 ## y3 1.044 0.150 6.961 0.000 ## y4 1.300 0.138 9.412 0.000 ## dem65 =~ ## y5 1.000 ## y6 1.258 0.164 7.651 0.000 ## y7 1.282 0.158 8.137 0.000 ## y8 1.310 0.154 8.529 0.000 Estimate veerg annab meile faktorlaadungid (standardiseerimata). Kõikide faktorite esimene laadung on fikseeritud \\(1\\)-ks (mudeli identifikatsiooni pärast). Kõik laadungid on positiivsed ja statistiliselt olulised (p &lt; 0.05). Problemaatiline võib olla \\(x2\\) laadung, mis oluliselt suurem kui \\(x1\\) laadung. Me üldiselt tahaksime, et kõik laadungid oleksid enamvähem ühesuurused, st kõik indikaatorid panustaksid faktorisse ühepalju. ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## maj60 ~~ ## dem60 0.660 0.206 3.202 0.001 ## dem65 0.774 0.208 3.715 0.000 ## dem60 ~~ ## dem65 4.487 0.911 4.924 0.000 Latentsete tunnuste vahelised covariatsioonid on kõik positiivsed ja olulised. dem60 ja dem65 on tugevalt seotud, mis on muidugi loogiline aga samas võib mudeli sobivuse seisukohast olla problemaatiline. ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.082 0.020 4.180 0.000 ## .x2 0.118 0.070 1.689 0.091 ## .x3 0.467 0.090 5.174 0.000 ## .y1 1.942 0.395 4.910 0.000 ## .y2 6.490 1.185 5.479 0.000 ## .y3 5.340 0.943 5.662 0.000 ## .y4 2.887 0.610 4.731 0.000 ## .y5 2.390 0.447 5.351 0.000 ## .y6 4.343 0.796 5.456 0.000 ## .y7 3.510 0.668 5.252 0.000 ## .y8 2.940 0.586 5.019 0.000 ## maj60 0.448 0.087 5.169 0.000 ## dem60 4.845 1.088 4.453 0.000 ## dem65 4.345 1.051 4.134 0.000 Ja lõpuks dispersioonid, mis indikaatorite puhul näitavad iga indikaatori nn unikaalset varieeruvust (mudeli seisukohast viga). Ehk seda varieeruvust, mida me faktoritega selgitada ei suuda. Me tahame, et need oleksid võimalikult väikesed (\\(y2\\) ja \\(y3\\) jäägid tunduvad suhteliselt suuremad). Faktorite dispersioonid näitavad faktorite varieeruvust. Saame kõike seda infot vaadata ka diagrammi kujul: semPaths(fit, node.width = 1, edge.label.cex = 1, what = &quot;paths&quot;, whatLabels = &#39;est&#39;) Kui vaatame sobivusindekseid, siis hii-ruut test &lt; 0.05 (halb), CFI &gt; 0.95 (hea), TLI &lt; 0.95 (halb), RMSEA &gt; 0.05 (halb) ja SRMR &lt; 0.08 (hea). Ehk siis suhteliselt ebamäärased tulemused (nagu nad tavaliselt ka on). Kuid tundub, et peame üritama oma mudelit natukene paremaks teha. Vaatame mudeli jääke. residuals(fit, type = &#39;cor&#39;) ## $type ## [1] &quot;cor.bollen&quot; ## ## $cov ## x1 x2 x3 y1 y2 y3 y4 y5 y6 y7 y8 ## x1 0.000 ## x2 -0.001 0.000 ## x3 -0.003 0.002 0.000 ## y1 0.034 -0.047 -0.083 0.000 ## y2 -0.099 -0.082 -0.086 -0.038 0.000 ## y3 0.037 0.005 -0.050 0.083 -0.085 0.000 ## y4 0.114 0.068 0.054 -0.033 0.066 0.002 0.000 ## y5 0.155 0.089 0.043 0.075 -0.054 0.022 -0.024 0.000 ## y6 -0.054 -0.068 -0.047 0.003 0.123 -0.113 0.000 -0.064 0.000 ## y7 -0.029 -0.040 -0.044 -0.002 -0.028 0.085 -0.008 0.020 -0.032 0.000 ## y8 0.032 -0.002 -0.039 -0.034 -0.024 -0.054 0.025 -0.050 0.090 0.018 0.000 Silma torkavad jäägid, mis tulenevad \\(x1\\) ja \\(y5\\), \\(x1\\) ja \\(y4\\), \\(y2\\) ja \\(y6\\) ning \\(y3\\) ja \\(y6\\) korrelatsioonidest. Vaatame ka modifikatsiooniindekseid: # Kasutame minimum.value = 5 parameetrit, et filtreerida välja kõige olulisemad modifikatsiooniindeksid modindices(fit, minimum.value = 5) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 79 y1 ~~ y3 5.204 1.024 1.024 0.318 0.318 ## 81 y1 ~~ y5 8.183 0.884 0.884 0.410 0.410 ## 88 y2 ~~ y6 9.279 2.129 2.129 0.401 0.401 ## 93 y3 ~~ y6 6.574 -1.590 -1.590 -0.330 -0.330 ## 104 y6 ~~ y8 8.668 1.513 1.513 0.423 0.423 Saame siit sisuliselt sama sisendi, mida ka jääkide maatriksi abil täheldasime (~~ märk annab märku, et mudel läheks paremaks kui antud tunnuste vahel oleks defineeritud kovariatsioon). Mida siis teha, kui me näeme, et mudel läheks oluliselt paremaks kui me kaasaks mudelise mõned indikaatorite vahelised korrelatsioonid (teooria osast peaks meeles olema, et korrektsete faktorlaadungite saamiseks ei tohiks indikaatorid korreleeritud olla)? Üldiselt peab ütlema, et mudeli sobivuse nimel seda tihti ka tehakse (isegi väga tihti). Aga igal juhul, kui seda teha, siis peaks olema ka mingi teoreetiline põhjus, mis mõne tunnuse vaheline seos ei ole määratletud ainult faktoriga. On muidugi ka võimalus mõni problemaatiline tunnus mudelist välja visata, aga ka see peaks olema teoreetiliselt põhjendatud. Muudame oma mudelit nii, et defineerime \\(y2\\) ja \\(y6\\) vahelise kovariatsiooni: # Defineerime mudeli mod &lt;- &#39; maj60 =~ x1 + x2 + x3 dem60 =~ y1 + y2 + y3 + y4 dem65 =~ y5 + y6 + y7 + y8 y2 ~~ y6 &#39; fit1 &lt;- cfa(mod, data = dt) summary(fit1, fit.measures = TRUE, standardized = T) ## lavaan 0.6-10 ended normally after 53 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 26 ## ## Number of observations 75 ## ## Model Test User Model: ## ## Test statistic 62.958 ## Degrees of freedom 40 ## P-value (Chi-square) 0.012 ## ## Model Test Baseline Model: ## ## Test statistic 730.654 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.966 ## Tucker-Lewis Index (TLI) 0.953 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1560.207 ## Loglikelihood unrestricted model (H1) -1528.728 ## ## Akaike (AIC) 3172.415 ## Bayesian (BIC) 3232.670 ## Sample-size adjusted Bayesian (BIC) 3150.725 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.087 ## 90 Percent confidence interval - lower 0.042 ## 90 Percent confidence interval - upper 0.127 ## P-value RMSEA &lt;= 0.05 0.079 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.051 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## maj60 =~ ## x1 1.000 0.670 0.920 ## x2 2.179 0.138 15.734 0.000 1.460 0.973 ## x3 1.818 0.152 11.976 0.000 1.218 0.872 ## dem60 =~ ## y1 1.000 2.216 0.851 ## y2 1.313 0.175 7.488 0.000 2.909 0.742 ## y3 1.061 0.148 7.189 0.000 2.351 0.721 ## y4 1.293 0.137 9.443 0.000 2.865 0.861 ## dem65 =~ ## y5 1.000 2.100 0.809 ## y6 1.233 0.164 7.510 0.000 2.590 0.770 ## y7 1.283 0.155 8.268 0.000 2.695 0.826 ## y8 1.302 0.152 8.594 0.000 2.735 0.848 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y2 ~~ ## .y6 2.164 0.791 2.734 0.006 2.164 0.383 ## maj60 ~~ ## dem60 0.672 0.208 3.234 0.001 0.453 0.453 ## dem65 0.790 0.210 3.756 0.000 0.561 0.561 ## dem60 ~~ ## dem65 4.482 0.911 4.920 0.000 0.963 0.963 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x1 0.081 0.019 4.163 0.000 0.081 0.153 ## .x2 0.121 0.070 1.732 0.083 0.121 0.054 ## .x3 0.467 0.090 5.172 0.000 0.467 0.239 ## .y1 1.876 0.396 4.734 0.000 1.876 0.276 ## .y2 6.921 1.261 5.487 0.000 6.921 0.450 ## .y3 5.093 0.915 5.564 0.000 5.093 0.480 ## .y4 2.862 0.623 4.595 0.000 2.862 0.259 ## .y5 2.324 0.444 5.237 0.000 2.324 0.345 ## .y6 4.607 0.846 5.447 0.000 4.607 0.407 ## .y7 3.390 0.663 5.116 0.000 3.390 0.318 ## .y8 2.911 0.594 4.904 0.000 2.911 0.280 ## maj60 0.449 0.087 5.176 0.000 1.000 1.000 ## dem60 4.911 1.094 4.488 0.000 1.000 1.000 ## dem65 4.411 1.058 4.171 0.000 1.000 1.000 Tundub, et saime mõnevõrra paremate indeksitega mudeli. Kontrollime seda ka anova() funktsiooniga. anova(fit, fit1) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit1 40 3172.4 3232.7 62.958 ## fit 41 3179.9 3237.9 72.462 9.5033 1 0.002051 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Mudelite võrdluse hii-ruut test ütleb, et peaksime siiski eelistama teist mudelit. Mudelid on oluliselt erinevad, kusjuures teine (vähemate parameetritega) mudel on on oluliselt parem (hii-ruut väärtus on madalam). Ka AIC ja BIC väärtused annavad meile sarnase indikatsiooni (väiksem väärtus on parem). Kuna aga kõik sobivusindeksid ei vastanud meie setud piirmääradele, siis peaksime veel mõned kovariatsioonid mudelisse lisama. # Defineerime mudeli mod &lt;- &#39; maj60 =~ x1 + x2 + x3 dem60 =~ y1 + y2 + y3 + y4 dem65 =~ y5 + y6 + y7 + y8 y2 ~~ y6 y1 ~~ y5 y6 ~~ y8 &#39; fit1 &lt;- cfa(mod, data = dt) summary(fit1, fit.measures = TRUE, standardized = T) ## lavaan 0.6-10 ended normally after 62 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 28 ## ## Number of observations 75 ## ## Model Test User Model: ## ## Test statistic 46.259 ## Degrees of freedom 38 ## P-value (Chi-square) 0.168 ## ## Model Test Baseline Model: ## ## Test statistic 730.654 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.988 ## Tucker-Lewis Index (TLI) 0.982 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1551.858 ## Loglikelihood unrestricted model (H1) -1528.728 ## ## Akaike (AIC) 3159.715 ## Bayesian (BIC) 3224.605 ## Sample-size adjusted Bayesian (BIC) 3136.356 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.054 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.102 ## P-value RMSEA &lt;= 0.05 0.427 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.047 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## maj60 =~ ## x1 1.000 0.670 0.920 ## x2 2.180 0.138 15.750 0.000 1.460 0.973 ## x3 1.818 0.152 11.964 0.000 1.218 0.872 ## dem60 =~ ## y1 1.000 2.168 0.828 ## y2 1.350 0.184 7.337 0.000 2.927 0.748 ## y3 1.085 0.156 6.967 0.000 2.352 0.722 ## y4 1.340 0.147 9.126 0.000 2.905 0.873 ## dem65 =~ ## y5 1.000 2.073 0.797 ## y6 1.191 0.173 6.892 0.000 2.470 0.736 ## y7 1.300 0.162 8.021 0.000 2.696 0.826 ## y8 1.296 0.160 8.120 0.000 2.687 0.834 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y2 ~~ ## .y6 2.221 0.742 2.995 0.003 2.221 0.377 ## .y1 ~~ ## .y5 0.787 0.355 2.219 0.027 0.787 0.341 ## .y6 ~~ ## .y8 1.499 0.566 2.650 0.008 1.499 0.371 ## maj60 ~~ ## dem60 0.646 0.204 3.172 0.002 0.445 0.445 ## dem65 0.807 0.212 3.804 0.000 0.581 0.581 ## dem60 ~~ ## dem65 4.355 0.961 4.531 0.000 0.969 0.969 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x1 0.082 0.019 4.186 0.000 0.082 0.154 ## .x2 0.120 0.070 1.717 0.086 0.120 0.053 ## .x3 0.467 0.090 5.179 0.000 0.467 0.240 ## .y1 2.151 0.441 4.873 0.000 2.151 0.314 ## .y2 6.727 1.241 5.419 0.000 6.727 0.440 ## .y3 5.090 0.921 5.524 0.000 5.090 0.479 ## .y4 2.632 0.613 4.293 0.000 2.632 0.238 ## .y5 2.467 0.476 5.180 0.000 2.467 0.365 ## .y6 5.161 0.922 5.597 0.000 5.161 0.458 ## .y7 3.389 0.682 4.973 0.000 3.389 0.318 ## .y8 3.171 0.650 4.881 0.000 3.171 0.305 ## maj60 0.448 0.087 5.173 0.000 1.000 1.000 ## dem60 4.702 1.095 4.296 0.000 1.000 1.000 ## dem65 4.298 1.059 4.060 0.000 1.000 1.000 Tundub, et nüüd on meie mudel märka sobilikum. Kõik sobivusindeksid, välja arvatud RMSEA (kuigi ka see peaaegu), mahuvad meie seatud piiridesse. 8.4.7 SEM Meil on nüüdseks olemas sobilik faktormudel (ehk mõõtmismudel). Järgmisena vaatame, kuidas faktorite vahelisi seoseid defineerida (tegelikult on ka faktormudelis faktorid omavahel korreleeritud, kuid meid huvitab kausaalsete mõjude testimine). Põhimõtteliselt kasutame faktortunnuseid sõltumatute ja sõltuvate tunnustena regressioonanalüüsis. Vaatame kuidas dem65 on mõjutatud dem60 ja maj60 faktoritest. Lisaks eeldame, et dem60 on samuti mõjutatud maj60-st. Selleks tuleb meil lisaks eelnevale faktormudelile defineerida nende vahelised regressioonid. Ja kasutame nüüd sem() funktsiooni. mod &lt;- &#39; maj60 =~ x1 + x2 + x3 dem60 =~ y1 + y2 + y3 + y4 dem65 =~ y5 + y6 + y7 + y8 y2 ~~ y6 y1 ~~ y5 y6 ~~ y8 dem60 ~ maj60 dem65 ~ maj60 + dem60 &#39; fit &lt;- sem(mod, data = dt) summary(fit, fit.measures = TRUE) ## lavaan 0.6-10 ended normally after 53 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 28 ## ## Number of observations 75 ## ## Model Test User Model: ## ## Test statistic 46.259 ## Degrees of freedom 38 ## P-value (Chi-square) 0.168 ## ## Model Test Baseline Model: ## ## Test statistic 730.654 ## Degrees of freedom 55 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.988 ## Tucker-Lewis Index (TLI) 0.982 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1551.858 ## Loglikelihood unrestricted model (H1) -1528.728 ## ## Akaike (AIC) 3159.715 ## Bayesian (BIC) 3224.605 ## Sample-size adjusted Bayesian (BIC) 3136.356 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.054 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.102 ## P-value RMSEA &lt;= 0.05 0.427 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.047 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## maj60 =~ ## x1 1.000 ## x2 2.180 0.138 15.750 0.000 ## x3 1.818 0.152 11.964 0.000 ## dem60 =~ ## y1 1.000 ## y2 1.350 0.184 7.337 0.000 ## y3 1.085 0.156 6.967 0.000 ## y4 1.340 0.147 9.126 0.000 ## dem65 =~ ## y5 1.000 ## y6 1.191 0.173 6.892 0.000 ## y7 1.300 0.162 8.021 0.000 ## y8 1.296 0.160 8.120 0.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## dem60 ~ ## maj60 1.441 0.388 3.714 0.000 ## dem65 ~ ## maj60 0.579 0.213 2.713 0.007 ## dem60 0.847 0.098 8.675 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y2 ~~ ## .y6 2.221 0.742 2.995 0.003 ## .y1 ~~ ## .y5 0.787 0.355 2.219 0.027 ## .y6 ~~ ## .y8 1.499 0.566 2.650 0.008 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.082 0.019 4.186 0.000 ## .x2 0.120 0.070 1.717 0.086 ## .x3 0.467 0.090 5.179 0.000 ## .y1 2.151 0.441 4.873 0.000 ## .y2 6.727 1.241 5.419 0.000 ## .y3 5.090 0.921 5.524 0.000 ## .y4 2.632 0.613 4.293 0.000 ## .y5 2.467 0.476 5.180 0.000 ## .y6 5.161 0.922 5.597 0.000 ## .y7 3.389 0.682 4.973 0.000 ## .y8 3.171 0.650 4.881 0.000 ## maj60 0.448 0.087 5.173 0.000 ## .dem60 3.771 0.897 4.204 0.000 ## .dem65 0.144 0.197 0.727 0.467 Sobivusindeksid võrreldes eelmise mudeliga ei muutu. See on sellepärast, et kõik seosed, mis me defineerisime regressioonseostena, olid faktormudelis hinnatud korrelatsioonidena. Suures osas on väljund sama, mis faktoranalüüsi puhul. Juurde tuli vaid regressiooniosa. ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## dem60 ~ ## maj60 1.441 0.388 3.714 0.000 0.445 0.445 ## dem65 ~ ## maj60 0.579 0.213 2.713 0.007 0.187 0.187 ## dem60 0.847 0.098 8.675 0.000 0.886 0.886 Näeme, et kõik regressioonid on statistiliselt olulised. Mida kõrgem on maj60 seda kõrgem on ka dem60 ning mida kõrgemad on maj60 ja dem60, seda kõrgem on dem65. Koefitsientide suurused ei ole reeglina väga hästi tõlgendatavad. Meid pigem huvitab nende suund ja statistiline olulisus. Saame oma mudelit jällegi ka graafiliselt vaadata. semPaths(fit, node.width = 1, edge.label.cex = 1, what = &quot;paths&quot;, whatLabels = &#39;est&#39;) 8.4.8 Mudeli spetsifikatsioonid Päris palju asju toimub lavaan-is nn kapoti all. Mingid argumendid on vaikeväärtustena, mingid parameetrid on fikseeritud jne. Et kõigest sellest natuke aimu saada, on võimalik vaadata kuidas lavaan meie defineeritud mudeli täpselt spetsifitseeris. inspect() funktsiooniga saab kätte kõik mudeli poolt hinnatud parameetrid. inspect(fit) ## $lambda ## maj60 dem60 dem65 ## x1 0 0 0 ## x2 1 0 0 ## x3 2 0 0 ## y1 0 0 0 ## y2 0 3 0 ## y3 0 4 0 ## y4 0 5 0 ## y5 0 0 0 ## y6 0 0 6 ## y7 0 0 7 ## y8 0 0 8 ## ## $theta ## x1 x2 x3 y1 y2 y3 y4 y5 y6 y7 y8 ## x1 15 ## x2 0 16 ## x3 0 0 17 ## y1 0 0 0 18 ## y2 0 0 0 0 19 ## y3 0 0 0 0 0 20 ## y4 0 0 0 0 0 0 21 ## y5 0 0 0 10 0 0 0 22 ## y6 0 0 0 0 9 0 0 0 23 ## y7 0 0 0 0 0 0 0 0 0 24 ## y8 0 0 0 0 0 0 0 0 11 0 25 ## ## $psi ## maj60 dem60 dem65 ## maj60 26 ## dem60 0 27 ## dem65 0 0 28 ## ## $beta ## maj60 dem60 dem65 ## maj60 0 0 0 ## dem60 12 0 0 ## dem65 13 14 0 Numbritena on üles loetletud kõik hinnatud parameetrid. $lambda on laadungite maatriks. On näha millised tunnused laaduvad millisesse faktorisse. $theta on kovariatsioonimaatriks. Diagonaalis on kõikide indikaatorite hinnatud dispersioonid. Kõik indikaatorite kovariatsioonid on 0 (välja arvatud need, mis me ise defineerisime), ehk neid ei ole hinnatud ja eeldatakse, et need on \\(0\\) $psi on latentsete tunnuste kovariatsioonimaatriks $beta on regressioonikoefitsientide maatriks Saame näha ka hinnatud parameetreid inspect(fit, &quot;est&quot;) ## $lambda ## maj60 dem60 dem65 ## x1 1.000 0.000 0.000 ## x2 2.180 0.000 0.000 ## x3 1.818 0.000 0.000 ## y1 0.000 1.000 0.000 ## y2 0.000 1.350 0.000 ## y3 0.000 1.085 0.000 ## y4 0.000 1.340 0.000 ## y5 0.000 0.000 1.000 ## y6 0.000 0.000 1.191 ## y7 0.000 0.000 1.300 ## y8 0.000 0.000 1.296 ## ## $theta ## x1 x2 x3 y1 y2 y3 y4 y5 y6 y7 y8 ## x1 0.082 ## x2 0.000 0.120 ## x3 0.000 0.000 0.467 ## y1 0.000 0.000 0.000 2.151 ## y2 0.000 0.000 0.000 0.000 6.727 ## y3 0.000 0.000 0.000 0.000 0.000 5.090 ## y4 0.000 0.000 0.000 0.000 0.000 0.000 2.632 ## y5 0.000 0.000 0.000 0.787 0.000 0.000 0.000 2.467 ## y6 0.000 0.000 0.000 0.000 2.221 0.000 0.000 0.000 5.161 ## y7 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 3.389 ## y8 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.499 0.000 3.171 ## ## $psi ## maj60 dem60 dem65 ## maj60 0.448 ## dem60 0.000 3.771 ## dem65 0.000 0.000 0.144 ## ## $beta ## maj60 dem60 dem65 ## maj60 0.000 0.000 0 ## dem60 1.441 0.000 0 ## dem65 0.579 0.847 0 See muidugi ei tähenda, et me peaksime regressioonimudeli, mille \\(R^2\\) on 0.2, prügikasti viskama. Üldjuhul ei olegi vaja meil tarvs kogu sõltuva tunnuse variatsiooni kirjeldada. Tahame lihtsalt teada kas mingi mõju eksisteerib või mitte. Näiteks kui leiame, et pikaaegsete töötute koolitamine tõstab nende hõivesse sisenemise tõenäosust 10% võrra, siis meid ei huvita, et seda hõiesse sisenemist mõjutavad veel mustmiljon muud tegurit. Meil on olemas kinnitus, et koolitustest on kasu ja meil on mõtet seda alustada võijätkata. võib juhtuda, et me defineerime ka näiteks ainult otsese mõju. Sellisel juhul on teised mõjud \\(0\\) ja \\(r = \\text{otsene mõju} + 0 + 0 + 0\\) Eksploaratiivne faktoranalüüs aetakse omakorda tihti segamini peakomponentide meetodiga (principal component analysis ehk PCA). Need on oma eesmärkide ja kasutatavuse poolest küll sarnased meetodid, kuid tehniliselt siiski küllaltki erinevad Kovariatsioon (covariation) on standardiseerimata korrelatsioon. See kõik on tegelikult muidugi märksa keerulisem. \\(\\lambda\\) hääldatakse kui lambda Samamoodi nagu ühe sõltumatu tunnusega regressioonikoefitsient on korrelatsioon sõltuva ja sõltumatu tunnuse vahel Üks asi on tegelikult puudu. Me ei arvutanud faktori dispersiooni. Kuna meie mudeli sisendiks oli korrelatsioonimaatriks, siis me otseselt ei pidanudki seda arvutama ega seda käsitlema. Korrelatsioonimaatriksist sisendi puhul on selleks dispersiooniks 1. Kuid alati see nii ei ole. Miks faktori dispersioon oluline on ja miillal see 1 ei ole, sellest natukene hiljem. "]]
