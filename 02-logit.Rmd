
# (PART) Üldistatud lineaarsed mudelid {-} 

# Logistiline regressioon

```{r setup, include=FALSE}

knitr::opts_chunk$set(warning = F, message = F)
library(dplyr)
library(ggplot2)
library(wesanderson)
library(gridExtra)
library(tidyr)
library(stargazer)
library(data.table)
cols <- wes_palette(5, name="Cavalcanti1", type = "discrete")[c(5,2,3,4,1)]

```


Logistilise regressiooniga (logit-mudeliga) saame hinnata sõltumatute tunnuste mõju binaarsele sõltuvale tunnusele (töötav/töötu, käis valimas/ei käinud valimas, surnud/ei ole surnud). Teisisõnu, hindame kuidas mingid sõltumatud tunnused mõjutavad mingi sündmuse toimumise tõenäosust (näiteks kas haridus mõjutab hõives olemise tõenäosust või kas sotsiaalmajanduslik staatuse kasvades suureneb inimese tõenäosus valima minna). Seega sõltuvaks tunnuseks ei ole enam algne binaarne tulem, vaid seda iseloomustav tõenäosus (mitte jah/ei, vaid jah tõenäosus): $P(Y=1)=\pi$.  

## Logistiline funktsioon

Tavalise regressioonimudel oli väljendatav kui $\bar{y}=\beta_0+\beta_p x_p$. Miks me ei võiks pidevtunnuselise $y$ keskmist asendada $\pi$-ga: $\bar{\pi}=\beta_0+\beta_k x_k$? Binaarse tuynnuse keskmine (kui tunnus on $0/1$ kujul) ongi ju tõenäosus ja lineaarse regressiooniga me hindamegi ju $Y$ keskmist erinevate $X$ väärtuste korral. Aga sellepärast, et tõenäosus on piiritletud $0$ ja $1$-ga, samas kui lineaarne funktsioon hõlmab kõiki reaalarvulisi väärtusi. Seega on paratamatu, et mingite $x$-i väärtuste puhul on prognoositav $y$ väärtus suurem kui $1$ või väiksem kui $0$. 

```{r glm-1, fig.cap = "Lineaarse regressiooni kasutamine binaarse sõltuva tunnusega",  echo=F, warning=F}
set.seed(669)
x1 = rnorm(50)           
x2 = rbinom(50, 1, 0.5)
z = 1 + 3*x1        
pr = 1/(1+exp(-z))        
y = rbinom(50,1,pr) 

df = data.frame(y=y,x1=x1,x2=as.factor(x2))
df$y_f <- fitted(glm(y~x1,data=df,family="binomial"))
df$y_l <- fitted(lm(y~x1,data=df))
ggplot(df, aes(x1, y))+
  geom_point(shape=1)+
  #geom_line(aes(y=y_f), color=cols[1])+
  geom_line(aes(y=y_l), color=cols[2], size = 1)+
  theme_minimal()+
  labs(y = 'Sõltuv tunnus Y (tõenäosus)', x = "Sõltumatu tunnus X")+
  scale_y_continuous(breaks = c(0,0.5, 1))
```

Lisaks tekivad probleemid jääkide struktuuriga (tavaline regressioon eeldab normaaljaotust) ja jääkide dispersiooniga (tavaline regressioon eeldab konstantset hajuvust).

Seega, et hinnata tunnuste vahelist suhet adekvaatselt, peaksime tõenäosuse skaala ($0 \dots1$) kuidagi pidevaks skaalaks teisendama ($-\infty \dots \infty$).

### Šansid 
Et saada lahti maksimaalsest väärtusest ($1$), on võimalik kasutada sündmuse toimumise tõenäosuse asemel sündmuse toimumise šanssi (*odds*). Šanssideks nimetatakse sündmuse toimumise ja mittetoimumise suhet:

$$\text{šansid}=\frac{p}{(1-p)}$$
kus $p$ on sündmuse toimumise tõenäosus.  

Valemist võime välja lugeda järgmist:  

1. Šansid on alati positiivsed  
1. Kui šansid on $1$, siis on sündmuse toimumise ja mittetoimumise tõenäosus võrdsed ($p=0.5$).  
1. Kui šansid on suuremad kui $1$, siis on sündmuse toimumise tõenäosus suurem kui mittetoimumise tõenäosus ($p>0.5$) ja vastupidi.  

Näiteks kui sündmuse toimumise tõenäosus on $0.8$, siis on šansid $\frac{0.8}{1-0.8}=\frac{0.8}{0.2}=4$. Seega sündmuse toimumise tõenäosus on $4$ korda suurem kui selle mittetoimumise tõenäosus. Kui sündmuse toimumise tõenäosus on $0.2$, siis on šansid $\frac{0.2}{1-0.2}=\frac{0.2}{0.8}=\frac{1}{4}=0.25$. Sündmuse toimumise tõenäosus on $4$ korda väiksem kui selle mittetoimumise tõenäosus.  

Kulli ja kirja viskamisel on kulli saamise šanss $\frac{0.5}{(1-0.5)}=1$. 

Šanss võtta kaardipakist ruutu on $\frac{0.25}{(1-0.25)}=\frac{1}{3}=0.33$. 

```{r odds, fig.cap = "Šansside ja tõenäosuse suhe",  echo=F, warning=F}
data.frame(p <- runif(1000), odds <- p/(1-p)) %>% 
  ggplot(aes(odds, p))+
  geom_line(color=cols[2], size = 1)+
  scale_y_continuous(breaks = c(0,0.5, 1))+
  geom_hline(yintercept = 0, linetype=2)+
  geom_hline(yintercept = 1, linetype=2)+
  theme_minimal()+
  labs(y="Tõenäosus", x = "Šansid")+
  scale_x_continuous(limits=c(0,100))
```

Šansid saab omakorda teisenda tagasi tõenäosuseks:

$$p=\frac{\text{šansid}}{1+\text{šansid}}$$

### Logit    
Kuid šansside puhul jääb tõenäosuse alumine piir ikkagi alles. Ka šansid on altpoolt piiratud (nad ei saa olla väiksemad kui $0$). Lahenduseks on võtta šansside logaritm, millega eemaldame ka alumise piiri. Saadud väärtust nimetatakse **logit**-iks (*log odds*):  

$$\text{logit}=\log \bigg(\frac{p}{(1-p)}\bigg)$$

```{r glm-3, fig.cap = "Logit-i ja tõenäosuse suhe",  echo=F, warning=F}
data.frame(p <- runif(1000), odds <- p/(1-p), logit=log(odds)) %>% 
  ggplot(aes(logit, p))+
  geom_line(color=cols[2], size = 1)+
  theme_minimal()+
  labs(y="Tõenäosus", x = "Logit")+
  geom_hline(yintercept = 0, linetype=2)+
  geom_hline(yintercept = 1, linetype=2)+
  geom_segment(aes(x = -8, xend = 0, y = 0.5, yend = 0.5), linetype=2)+
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = 0.5), linetype=2)
```


Taolist funktsionaalset seost, millega saame luua seose pidevtunnuse ($x$) ja tõenäosuse ($y$) vahel, nimetatakse **logistiliseks funktsiooniks**:

$$y = f(x) = \frac{\exp(x)}{1 + \exp(x)} = \frac{1}{1 + \exp(-x)}$$

## Logit mudel 

Logistilise funktsiooni pöördfunktsiooniks on **logit funktsioon**, millega saame tõenäosuse "mappida" pidevskaalale (logistilise funktsiooni sisendiks on pidevtunnus ja väljundiks tõenäosus, logit funktsiooni sisendiks on tõenäosus ja väljundiks pidevtunnus):

$$\text{logit}(\pi_i)=\text{log} \left(\dfrac{\pi_i}{1-\pi_i}\right)$$ 

Logistilise regressiooni kontekstis kasutame logit funktsiooni linkfunktsioonina, mille abil muudame hinnatava tõenäosuse pidevskaalaliseks logitiks. Logit väärtusi saame aga hinnata juba tavapärase lineraarse regressioonivõrrandi abil. Seega logistilise regressiooni mudel moodustub järgmiselt:

$$\text{logit}(\pi_i)=\text{log} \left(\dfrac{\pi_i}{1-\pi_i}\right)=\beta_0+\beta_1 x_i$$  
Saame sama mudeli esitada ka otse logistilise funktsiooni kaudu:

$$\pi_i=Pr(Y_i=1|X_i=x_i) = \frac{1}{1+e^{-(\beta_0+\beta_1 x_i)}} =\dfrac{e^{(\beta_0+\beta_1 x_i)}}{1+e^{(\beta_0+\beta_1 x_i)}}$$  

Logistilise regressiooni seos eelneva näite puhul näeb aga välja selline:

```{r glm-2, fig.cap = "Logistiline regressioon võrdluses lineaarse regressiooniga",  echo=F, warning=F}
set.seed(669)
x1 = rnorm(50)           
x2 = rbinom(50, 1, 0.5)
z = 1 + 3*x1        
pr = 1/(1+exp(-z))        
y = rbinom(50,1,pr) 

df = data.frame(y=y,x1=x1,x2=as.factor(x2))
df$y_f <- fitted(glm(y~x1,data=df,family="binomial"))
df$y_l <- fitted(lm(y~x1,data=df))
ggplot(df, aes(x1, y))+
  geom_point(shape=1)+
  geom_line(aes(y=y_f, colour = 'Logistiline regressioon'), size = 1)+
  geom_line(aes(y=y_l, color = 'Lineaarne regressioon'), size = 1, alpha = 0.2)+
  theme_minimal()+
  labs(y = "Tõenäosus", x = "X")+
  scale_y_continuous(breaks = c(0,0.5, 1), limits = c(-0.5,1.5))+
  scale_color_manual(values = c('Lineaarne regressioon' = cols[1], 'Logistiline regressioon' = cols[2]))+
  theme(legend.title = element_blank(),
        legend.position = 'bottom')
```

### Mudeli tõlgendus
Tavalise regresioonimudeliga saime prognoosida $y$ väärtust mingite $x$ väärtuste korral (ja $y$ muutust, kui $x$ muutub ühe ühiku võrra). Sama kehtib ka logistilise regressiooni puhul. Kuid mida me siinjuures täpsemalt prognoosime? Tahaksime kindlasti prognoosida (uuritava sündmuse toimumise) tõenäosust. Kuid kuna me teisendasime tõenäosuse logititeks, siis tegelikult saame prognoosida hoopis logitit. Ja ka ühe ühikuline muutus $x$-is ei peegelda mitte $y$ tõenäosuse muutust, vaid logit($y$)-i muutust. Logiteid ei oska me (vähemalt esialgu) kuidagi tõenäosuslikult tõlgendada. Mida siis teha? 

Lahenduseks on võtta *logit*-i võrrandi mõlemast poolest eksponent, misläbi saame *logit*-i muuta šanssideks:  

$$\exp(logit) = \exp(\beta_0+\beta_1 x_i) \implies \frac{\pi_i}{1-\pi_i}=e^{({\beta_0}+\beta_1 x_i)}$$ 

Sellisel juhul saab $\beta_0$-i tõlgendada kui $y$-i šansse juhul kui $x$ on $0$ ja $\beta_1$-te kui šanssides kordajat (mitu korda $x$-i ühe ühiku muutudes $y$ šansid suurenevad või vähenevad). Seda šansside muutust väljendavat kordajat nimetatakse **šansside suhteks**.

Miks me nüüd räägime $\beta_1$-st kui kordajast (lineaarse regressiooni puhul oli tegemist lisanduva muutusega)? Põhjus on eksponendi võtmises, kuna $e^{({\beta_0}+\beta_1 x_i)} = e^{{\beta_0}}\times e^{\beta_1 x_i}$.   


### Šansside suhe

Vaatame näidet, kus hindame hääletamise tõenäosust ning abielu mõju sellele:

```{r, echo=FALSE}
matrix(c(0.75, 0.54, 0.25, 0.46), ncol = 2, dimnames = list(c('Abielus', 'Vallaline'), c('Hääletab', 'Ei hääleta'))) %>% 
  as.data.frame() %>%
  knitr::kable() %>% 
  kableExtra::column_spec(1, bold = TRUE) %>% 
  kableExtra::kable_minimal(full_width = F)
```


Abielus inimeste puhul on šanss hääletamiseks  $\frac{0.75}{0.25} = \frac{3}{1}= 3$ (st $3$ hääletajat iga mittehääletaja kohta).  

Vallaliste puhul on šanss hääletamiseks  $\frac{0.54}{0.46} = 1.17$ (st $1.17$ hääletajat iga mittehääletaja kohta).  

Meid huvitab kuidas sõltumatu tunnuse muutus sündmuse toimumise šansse mõjutab, ehk kui palju muutuvad šansid kui sõltumatu tunnus muutub ühe ühiku võrra. Seda muutust väljendabki šansside suhe (*odds ratio* ehk OR)

$$OR=\frac{y \text{ šanss juhul kui } x \text{ väärtus muutub ühe ühiku võrra}}{y \text{ šanss juhul kui } x \text{ väärtus jääb samaks}}$$

Abielus olijate ja vallaliste šansside suhe on $\frac{3}{1,17}=2.56$. Ehk siis abielus olijate šanss hääletada on kaks ja pool korda suurem. Abielu tunnuse ühe ühiku muutumisega muutuvad šansid $2.56$ korda ehk suurenevad $156\%$.


## Logistiline regressioon R-is

Võtame R-i näidisandmestiku `Titanic`, mis kirjeldab Titanicul hukkunute ja ellujäänute sugu, vanust ja reisijaklassi. Üritame hinnata kuidas ja kas need tunnused mõjutasid ellujäämist.  

Logistilise regressiooni mudeleid saab Ris hinnata `glm()` funktsiooniga. Selle loogika ja argumendid on sarnased `lm()` funktsiooni omadele. Peamiseks erinevuseks on see, et nüüd peame defineerima ka sõltuva tunnuse jaotuse ja linkfunktsiooni. See käib argumendiga `family`. Logistilise regressiooni jaoks peame defineerima `family = binomial(link = 'logit')` (sõltuva tunnuse jaotus on binoomjaotus ja linkfunktsioon on logit).  
Vaatame kõigepealt soo mõju:

```{r}
# Kasutame paket carData andmestikku TitanicSurvival
titanic <- carData::TitanicSurvival
```

Vaatame andmestiku esimesi ridu:

```{r}
head(titanic)
```

Ellujäämise tunnus (survived) on tekstiline. Peame selle muutma faktortunnuseks või numbriliseks tunnuseks (0/1). Sobib ka loogiline (TRUE/FALSE) tunnuseks, kuna R käsitleb loogilist tunnust tegelikult numbrilisena (TRUE = 1 ja FALSE = 0). Ja Loogilist tunnust on üldiselt lihtsam konstrueerida. 

```{r}
titanic <- titanic %>% 
  mutate(surv = survived == 'yes')

# Või nii:
# titanic$surv <- titanic$survived == "yes"
```

Ja defineerime logistilise regressiooni mudeli. Kasutame `glm()` funktsiooni, mille kasutus on sarnane `lm()` funktsioonile, välja arvatud see, et peame `familiy` argumendiga määratlema kasutatava jaotuse ning linkfunktsiooni. Binaarset tunnust iseloomustab binoomjaotus ning linkfunktsiooniks on *logit*.

```{r}
mudel7 <- glm(surv~sex, data = titanic, family = binomial(link = 'logit'))
summary(mudel7)
```


Väljund on sarnane `lm()` väljundile. Leiame seal regressioonikoefitsiendid, nende standardvead, z-väärtused ja z-testi *p*-väärtuse^[z-test puhul on tegemist t-testi analoogiga, mis ei lähtu mitte t-jaotusest, vaid normaaljaotusest. Tõlgendus on aga sama]. Kuid koefitsiendid on nüüd logititskaalal ja seepärast küllaltki raskesti tõlgendatavad. Saame siiski järeldada, et meeste tõenäosus ellu jääda oli väiksem kui naistel (koefitsient on negatiivne) ja naiste tõenäosus ellu jääda oli suurem kui 0.5 (vabaliige, ehk naiste ellujäämistõenäose *logit*, on positiivne). 

Mõnevõrra lihtsam on tõlgendada šansside suhet. Selleks peame koefitsientidest eksponendi võtma:

```{r}
exp(coef(mudel7))
```

```{r, include = F}
b0 <- round(coef(mudel7)[1], 2)
b1 <- round(coef(mudel7)[2], 2)
odds_naised <- round(exp(coef(mudel7))[1], 2)
or_mehed <- round(exp(coef(mudel7))[2], 2)
odds_mehed <- round(or_mehed*odds_naised, 2)
```

Saame siin kasutada (ja oleks tegelikult igati mõistlik kasuatda) ka paketi `jtools` funktsiooni `summ()` koos argumendiga `exp = T`, mis annab meile ilusasti vormistatud šansside suhete tabeli koos usalduspiiride jms-ga:

```{r, render = 'normal_print'}
library(jtools)
summ(mudel7, exp = T)
```

- *(Intercept)* - vabaliiget tõlgendame kui referentsgrupi (antud juhul naiste) šansse ellu jääda. Seega naise šanss Titanicul ellu jääda oli $`r odds_naised`$, ehk siis iga hukkunud naise kohta jäi ellu $`r odds_naised`$ naist, või vastupidi $1 \div `r odds_naised` = `r round(1/odds_naised, 2)`$, iga ellujäänud naise kohta hukkus $`r 1/odds_naised`$ naist. Saame välja arvutada ka naiste ellujäämise tõenäosuse:

$$\pi=\frac{\text{šansid}}{1+\text{šansid}} = \frac{`r odds_naised`}{1+`r odds_naised`} = `r round(odds_naised/(1+odds_naised), 2)`$$  

- *sexmale* - regressioonikoefitsienti puhul tõlgendame šansside suhet. Ehk mitu korda erinevad meeste ellujäämise šansid naiste ellujäämise šanssidest. Tuleb välja, et ligi $`r or_mehed`$ korda. Seega meeste šansid ellu jääda olid $`r or_mehed` \times `r odds_naised` = `r odds_mehed`$. Iga hukkunud mehe kohta jäi `r odds_mehed` meest ellu. Meeste ellujäämise tõenäosus oli:

$$\pi=\frac{\text{šansid}}{1+\text{šansid}} = \frac{`r odds_mehed`}{1+`r odds_mehed`} = `r round(odds_mehed/(1+odds_mehed), 2)`$$  

Saame selle tõenäosuse ka otse välja arvutada, kui paneme koefitsiendid regressioonivõrrandisse (eelnevalt toodud valemi järgi):

$$\pi=\dfrac{e^{(\beta_0+\beta_1 x_i)}}{1+e^{(\beta_0+\beta_1 x_i)}} = \dfrac{e^{(`r b0`+ (`r b1` \times 1))}}{1+e^{(`r b0`+(`r b1` \times 1))}} = `r round(odds_mehed/(1+odds_mehed), 2)`$$

Me loomulikult ei pea kogu seda asja käsitsi välja arvutama. Logititest saame otse tõenäosused arvutada `plogis()` funktsiooni abil:

```{r}
# Naiste tõenäosus ellu jääda:
coef(mudel7)[1] %>% # Võtame esimese koefitsiendi
  as.numeric() %>% 
  plogis() 

# Meeste tõenäosus ellu jääda:
## nüüd tegeleme logititega, ehk lineaarse seosega. 
## seega liidame, mitte ei korruta
(coef(mudel7)[1] + coef(mudel7)[2]) %>% # esimene ja teine koefitsient
  as.numeric() %>% 
  plogis()
```


### Mitu sõltumatut tunnust ja interaktsioon

Vaatme ka, kuidas muudab ellujäämise tõenäosust lisaks soole vanus. Eeldame ka soo ja vanuse koosmõju (st kontrollime kas erinevas vanuses meeste ja naiste elujäämistõenäosused erinevad). Tsentreerime intrepretatsiooni huvides vanuse tunnuse. Seeläbi saame mõistlikul viisil vabaliiget tõlgendada ning muudame ka soo koefitsiendi sisukaks. Interaktsiooni tõttu on soo mõju eri vanuste puhul erinev ning ilma tsentreerimata näitaks koefitsient soo erinevust ainult 0 vanuse puhul. Tsentreeritud vanuse korral aga soo erinevust keskmise vanuse puhul, mis on mõnevõrra sisukam näitaja.

```{r}
mudel8 <- glm(surv~sex*scale(age, scale = F), data = titanic, family = binomial())
summary(mudel8)
```

```{r, include = F}
b0 <- round(coef(mudel8)[1], 2)
b1 <- round(coef(mudel8)[2], 2)
b2 <- round(coef(mudel8)[3], 2)
b3 <- round(coef(mudel8)[4], 2)

eb0 <- round(exp(b0), 2)
eb1 <- round(exp(b1), 2)
eb2 <- round(exp(b2), 2)
eb3 <- round(exp(b3), 2)
```


Täiskasvanuks olemine mõnevõrra tõstis ellujäämise tõenäosust, kuid seda ainult naiste puhul (soo ja vanuse interaktsioon on negatiivne ning meeste vanuse koefitsient on `r b2` + `r b3` = `r b2 + b3`). Tulemuste tõlgendamisel kasutame jälle `jtools::summ()` abil arvutatud koefitsientide eksponente ehk šansse ja šansside suhteid:

```{r, render = 'normal_print'}
summ(mudel8, exp = T)
```

- *(Intercept)* ehk vabaliige kirjeldab ellujäämise šansse juhul kui sõltumatud tunnused on nullid. Ehk siis antud juhul ellujäämise šansse keskmise vanusega naiste puhul. Nende ellujäämise tõenäosus oli:

$$\pi = \frac{`r eb0`}{1+`r eb0`} = `r round(eb0/(1+eb0), 2)`$$  

- *sexmale* - keskmise vanusega meeste (ainult keskmise vanusega, kuna muude vanuste puhul tuleb mängu koosmõju) ellujäämise šansid olid $`r eb1`$ korda keskmises vanuses naiste šansid (ehk siis $`r (1-eb1)*100`\%$  väiksemad). 

- *scale(age, scale = F)* - naiste (ainult naiste, kuna meeste puhul tuleb mängu koosmõju) šansid ellu jääda tõusid iga lisanudva eluaastaga $`r eb2`$ korda ehk $`r (eb2-1)*100`\%$. 

- *sexmale:scale(age, scale = F)* - meeste vanuse mõju puhul peame appi võtma koosmõju koefitsiendi. Meeste šansid ellu jääda kahanesid iga lisanudva eluaastaga $`r eb2` \times `r eb3` = `r eb2*eb3`$ korda ehk $`r (1-eb2*eb3)*100`\%$.

Näeme, et koosmõju on antud mudeli puhul vägagi sisukas. Meeste puhul vanuse kasv langetas ellujäämise šansse, naiste puhul aga tõstis.



::: {.teie-kord}
Ülesanne!  

* Piaaci andmestikus on tunnus *staatus3*. Võtke see aluseks ja tehke uus loogiline (TRUE/FALSE) tunnus *hoiv*, mis kirjeldaks kas inimene on või ei ole hõivatud.  
* Hinneke logistilise regressiooniga, kas hõivatus on mõjutatud inimese haridusest ja vanusest. 
:::


## Mudeli kvaliteet

Kuidas hinnata mudeli kvaliteeti? Meile ei anta ei jääkide standardviga ega determinatsioonikordajat. Küll on aga väljunis toodud *Null deviance* ja *Residual deviance*. *Deviance* kirjeldab mudeli hälvet ehk seda kui hästi (või õigem oleks öelda kui halvasti) meie mudel andmetega sobitub, ehk kui suur on erinevus meie andmete ja mudeli prognoosi vahel. Mida väiksem on *deviance*, seda väiksem on mudeli viga, ehk seda täpsemini sobitub mudel andmetega. 

- **Null deviance** kirjeldab küllastunud mudeli hälbimust nullmudelist.  
- **Residual deviance** ehk jääkhälbimus kirjeldab defineeritud mudeli hälbimust küllastunud mudelist. 

Nullmudel on ainult vabaliikmega mudel (ehk mudel kus ei ole ühtegi selgitavat tunnust peale $Y$-i keskmise). Küllastunud mudel on selline, kus on sama palju parameetreid kui andmepunkte (ehk mudel millega on kogu $Y$-i varieeruvus ära kirjeldatud).

### Mudeli sobivus andmetega

Mudeli sobivust andmetega (*goodness of fit*) saame hinnata jääkhälbimuse (*Residual deviance*) näitaja abil. Jääkhälbimus näitab kui palju mudeliga hinnatud $Y$ väärtused empiirilistest $Y$ väärtustest erinevad (analoogne asi lineaarse regressiooni puhul oli *residual sum of squares*). Jääkhälbimuse abil saame võrrelda kui palju meie sobitatud mudel erineb küllastunud (*saturated*) mudelist, st mudelist mis sobituks täiel määral andmetega. Jääkhälbimus näitabki sisuliselt meie mudeli ja küllastunud mudeli erinevust.  Mida väiksem on jääkhälbimus, seda paremini meie mudel andmetega "sobib". Seda, kas see on piisavalt väike (mis tähendab, et meie mudeli ja andmete vahel ei ole statistliselt olulist erinevust), saame testida hii-ruut testiga (arvestades mudeli  vabadusasteid (*degrees of freedom*).

```{r}
res_dev <- deviance(mudel8)
res_df <- df.residual(mudel8)
pchisq(res_dev, res_df, lower.tail = F)
```

`pchisq()` funktsiooniga saame testitulemusele ka *p*-väärtuse. Näeme, et see on suurem kui $0.05$, mis tähendab, et meie mudel ei erine küllastunud mudelist olulisel määra ja seega sobitub andmetega (siin tahame, et *p*-väärtus oleks võimalikult suur, st meie mudeli ja küllastunud mudeli vahel ei oleks statistilist erinevust). Reaaleluliste ja suurte andmetega on tegelikult suhteliselt keeruline hästi sobituvat mudelit leida. Seega isegi kui see test näitab, et mudel ei ole sobilik, siis üldjuhul me lihtsalt lepime asjaoluga, et meie mudel ei ole täiuslik ja jätame selle testi tähelepanuta.

Üks kasulik nipp on ka võrrelda jääkhälbimuse väärtust ja tema vabadusastmeid. Kui *Residual deviance* on oluliselt suurem kui tema *degrees of freedom*, siis annab see jällegi tunnistust halvast mudelist.

#### Hosmer-Lemeshow Test

Võime mudeli sobivust andmetega hinnata ka Hosmer-Lemeshow testiga, mis testib kuivõrd hästi meie mudeli prognoositud väärtused vastavad tegelikele väärtustele mingite alamgruppide lõikes (test teeb ise andmetes alamgrupid). Nullhüpoteesiks on see, et mudel sobitub andmetega, seega kui näeme suurt *p*-väärtust (*p* > 0.05), siis järeldame, et mudel on sobiv ja vastupidi.

```{r}
library(ResourceSelection)
hoslem.test(mudel8$y, fitted(mudel8))
```
Antud juhul näeme, et testi *p*-väärtus on suurem kui 0.05, seega mudel sobitub andmetega.

### Mudeli statistiline olulisus

Näeme, et meie sisuka mudeli hälve küllastunud mudelist (*Residual deviance*) on võrreldes nullmudeli erinevusega küllastunud mudelist (*Null deviance*) tunduvalt väiksem^[Peame siin arvestama ka erinevust vabadusastmetes. Kuigi sisuka mudeli hälve on väiksem, on selles ka vähem vabadusastmeid]. See tähendab, et tänu sõltumatutele tunnustele  suudame me sõltuva tunnuse variatsiooni seletada paremini kui ainult $Y$-i keskmise abil. Aga kas mudeli hälve läks väiksemaks piisavalt paju, et me saaksime selle kohta ka statistiliselt olulisi järeldusi teha? Ehk siis kas me saame järeldada, et sõltumatud tunnused seletavad statistiliselt olulisel määral sõltuva tunnuse variatsiooni ja meie mudel on parem kui lihtsalt sõltuva tunnuse keskmine? Saame seda testida  *likelihood ratio* testiga. 

Arvutame esmalt *Null deviance* ja *Residual deviance* erinevuse. Seeläbi saame enda defineeritud mudeli hälbe nullmudelist:

```{r}
dev_vahe <- mudel8$null.deviance - mudel8$deviance
dev_vahe
```

Ja ka vabadusasteme erinevuse:
```{r}
df_vahe <- mudel8$df.null-mudel8$df.residual
df_vahe
```

Hälvete vahe on jaotunud hii-ruut jaotuse alusel, seega saame hii-ruut jaotuse põhjal määrata selle olulisust. Arvutame hälvete vahe olulisustõenäosuse. Kasutame selleks jälle hii-ruut jaotuse funktsiooni `pchisq()`, mis tahab sisendina teatstatisikut (hälvete vahe) ja vabadusastemeid (vabadusastemete vahe). Samuti peame ütlema, et meid huvitab jaotuse parempoolse saba alla jääv tõenäosus.

```{r}
pchisq(dev_vahe, df_vahe, lower.tail = F)
```

Võime kasutada ka `anova()` funktsiooni, kus võrdleme enda mudelt ja ainult vabaliikmega mudelit:

```{r}

# ~1 tähistab vabaliiget
# et mudelites oleks sama palju vaatlusi, siis jätame 
# ka vabaliikmega mudelist kõik vanuse NA-d välja
anova(mudel8,
      glm(surv~1, data = filter(titanic, !is.na(age)), family = binomial),    
      test="Chisq")

```

Või kasutame `lmtest` paketi `lrtest()` funtsiooni:

```{r}
library(lmtest)
lrtest(mudel8)
```

Kõik eelnevad testid näitavad sama asja ja kõikide puhul huvitab meid eelkõige *p* väärtus. Kui see on piisavalt väike (näiteks väiksem kui $0.05$), siis saame järeldada, et meie testitavad mudelid on piisavalt erinevad ehk siis sõltumatute tunnuste lisamine vähendas *deviance*'i olulisel määral. 

Eelnevates näidetes on *p*-väärtus on väga väike, seega meie mudel on võrreldes nullmudeliga oluliselt parem.  

`anova()`-ga saame ka testida kas uue sõltumatu tunnuse lisamine teeb mudeli oluliselt paremaks.

```{r}
# Lisame mudelisse reisijateklassi
# Kasutame update() funktsiooni, millega saame olemasolevat mudelit muuta
anova(mudel8, 
      update(mudel8, ~.+passengerClass),
      test = "Chisq")
```
Näeme, et kui lisame mudelisse ka reisijaklassi, siis muutub muutub oluliselt ($p < 0.05$) paremaks.

### Pseudo-$R^2$

Kui lineaarse regressiooni puhul hindasime mudeli sobivust andmetega determinatsioonikordaja ($R^2$) abil, siis GLM-ide puhul vastavat näitajat ei ole. Küll on aga nn pseudo-$R^2$ statistikud, mida võib analoogsel viisil kasutada (need ei näita küll päris sama asja, kuid tõlgendus on sama). Üheks selliseks on näiteks Mcfadden'i $R^2$. See jääb 0 ja 1 vahele ning mida suurem selle väärtus on, seda parem on mudeli *fit*. Üldiselt ei küündi see näitaja kunagi päris 1-ni ja nii loetakse näiteks väärtust 0.2-0.4 juba päris heaks *fit*-iks.

```{r}
library(pscl)
pR2(mudel8)
```



## Predict

Sageli tahame oma mudeli alusel prognoosida mingitele kindlatele sõltumatute tunnuste väärtustele sõltuva tunnuse hinnanguid. Saame loomulikult need sõltumatute tunnuste väärtused regressioonivõrrandisse sisse panna ja hinnangu käsitsi välja arvutada. Aga on ka mugavam variant. Nimelt `predict()` funktsioon^[`predict()` funktsiooni saab kasutada ka tavalise regressiooni puhul].  

`predict()` vajab sisendiks mudelit ning referentsandmestikku vajalike sõltumatute tunnuste kategooriate kombinatsioonidega (vaikimisi, ilma uue andmestikuta, prognoosib ta mudelipõhised hinnangud igale mudeli andmestiku vaatlusele). Referentsandmestiku saame valmis teha käsitsi või kasutada näiteks `expand.grid()` funktsiooni.

Tahame teada `titanic` andmestiku põhjal 40 aastaste meeste tõenäosust ellu jääda:

```{r}
# Teeme referentsandmestiku
ref_data <- data.frame(sex = "male", age = 40)

# Kasutame predict() funktsiooni ja lisame referentsandmestikule
# pred tunnuse, millesse kirjutame prognoosi
# Kuna tegemist on logit mudeliga, siis defaultis
# prognoosib predict() logiteid. Kui tahame teada
# tõenäosusi, siis peame määrama type = 'response'

ref_data$pred <- predict(mudel8, newdata = ref_data, type = "response")
ref_data
```

Kui tahame prognoosi rohkematele kategooriate kombinatsioonidele, saame kasutada `expand.grid()` funktsiooni:

```{r}
# Teeme kõigepealt uue andmestiku, kus on sees nii mehed kui naised 
# ning vanused 10 aastaste intervallidena
ref_data <- expand.grid(sex = c("male", "female"), 
                        age = seq(from = 0, to = 80, by = 10))

# Lisame andmestikule predictioni
ref_data$pred <- predict(mudel8, newdata = ref_data, type = "response")

ref_data %>% 
  head()
```

Nüüd saame oma tulemused näiteks joonisele panna:

```{r}
ggplot(ref_data, aes(x = age, 
                  y = pred, 
                  color = sex))+
  geom_line(size = 1)+
  geom_point(size = 3)+
  labs(y = "Ellujäämise tõenäosus")+
  scale_y_continuous(labels = scales::percent)+
  scale_color_manual(values = c("#972D15", "#02401B"))+
  theme_minimal()
```


### Broom

Prognoositud väärtused kõikidele meie andmetes olevatele vaatlustele saame mõnevõrra lihtsamalt kätte paketi *broom* abil. *broom*i funktsioon `augment()` loob mudeli objektist andmestiku, milles on lisaks algsetele tunnusetele ka kõikidele vaatlustele prognoositud väärtudsed (*.fitted*), prognoositud väärtuste standardvead (*.se.fit*), jäägid (*.resid*) jne. 

```{r}
library(broom)
# Kasutame broomi funktsiooni augment 
mudel_fit <- augment(mudel8, type.predict = "response")
head(mudel_fit)
```


## Marginaalsed efektid

Marginaalsed efeketid (*marginal effects*) kirjeldavad sõltuva tunnuse muutust kui mingi sõltumatu tunnus muutub ühe ühiku võrra. Seega võimaldavad need logistilise regressiooni puhul kasutada lineaarse regressiooniga analoogset tõlgendamisloogikat. Marginaalsete efektide arvutamiseks on erinevaid viise. Üheks levinuimaks meetodiks on nn *Keskmised marginaalsed efektid* (*Average Marginal Effects* ehk AME).  

Oletame, et tahame Titanicu andmestiku alusel hinnata kui palju muutub inimese ellujäämise tõenäosus sõltuvalt tema soost. Logistilise regressioonimudeli abil saame teada vastava šansside suhte. Meid aga huvitaks tõenäosus. Me saame ka tõenäosuse välja arvutada (näiteks `predict()` funktsiooniga või ka käsitsi), kuid selleks peame määratlema mingi konkreetse grupi, kellele me regressioonivõrrandi abil tõenäosust prognoosime (näiteks saame võrrelda esimese klassi kajutis elvate täiskasvanud meeste ellujäämise tõenäosust esimese klassi kajutis elavate täsikasvanud naiste ellujäämise tõenäosusega). Meid aga huvitaks lihtsalt keskmine tõenäosuse erinevus meeste ja naiste vahel. Kuidas seda saavutada?  

Marginaalsete efektide (täpsemalt selle AME variandi) leidmiseks prognoositakse kõikidele andmestiku vaatlustele mudelipõhine hinnang kahel juhul - esimesel juhul nii, et kõikide vaatluste puhul määratakse nende sooks mees ja teisel juhul nii, et kõikide vaatluste puhul määratakse nende sooks naine. Kõik muud tunnused on mõlemal puhul nii nagu nad algselt olid. Keskmine marginaalne efekt ongi keskmine kahe prognoositud hinnangu vahe.

```{r}
library(margins)

# Defineerime uuesti mudeli, kuna margins() funktsioonile ei meeldi scale()
# võrrandi sees. Marginaalsete efektide puhul ei ole meil nii ehk naa vaja 
# vanust tsentreerida, kuna meid ei huvita algsed koefitsiendid
mudel9 <- glm(surv~sex*age, data = titanic, family = binomial())

# Soo mõju
margins(mudel9, variables = 'sex') %>% 
  summary()

```
Saame järeldada, et meeste tõenäosus ellu jääda oli $`r abs(round(summary(margins(mudel9, variables = 'sex'))[, 2]*100, 0))`$ protsendipunkti madalam kui naistel.

```{r}
# Kuna interaktsioon oli oluline ning meeste ja naiste vanuse mõjud
# olid erisuunalised, siis arvutame vanuse marginaalsed efektid 
# eraldi meeste ja naiste subsettidele 

# Meeste vanuse marginaalne efekt
margins(mudel9, variables = 'age', data = filter(titanic, sex == 'male')) %>% 
  summary()

# Naiste vanuse marginaalne efekt
margins(mudel9, variables = 'age', data = filter(titanic, sex == 'female')) %>% 
  summary()
```




## Prognoosi täpsus

*Confusion matrix*'i (segaduse maatriks?) abiga saame hinnata oma prognoosi täpsust. Võrdleme tegelikke ja hinnatuid väärtusi. Kasutame jälle `predict()` funktsiooni ning prognoosime seekord kõikidele titanic andmestiku vaatlustele mudelipõhised hinnangud. Seejärel võrdleme neid hinnanguid vaatluste tegelike väärtustega:

```{r}
# Anname table() funktsioonile ette kaks loogilist vektorit.
# Kui me predict funktsioonile newdata argumeti ei anna,
# siis võtab ta automaatselt mudeli objektist kogu andmestiku
# ja prognoosib hinnangu igale vaatlusele. Kuna prognoos on 
# tõenäosusskaalal, siis teeme selle loogiliseks vektoriks nii,
# et kõik üle 0.5 tõenäosused oleksd T ja väiksemad F
vaadeldud <- titanic[!is.na(titanic$sex) & !is.na(titanic$age), ]$survived == "yes"
prognoos <- predict(mudel8, type = "response")> 0.5
table(vaadeldud, prognoos)
```

Saadud maatriksist näeme, et prognoosisime oma mudeliga õigesti $523 + 292 = 815$ juhul ning valesti $96+135 = 231$ juhul, ehk siis meie mudeli **täpsus** (*accuracy*) on 
$$\text{accuracy} = \frac{\text{õige}}{\text{õige} + \text{vale}} = \frac{523 + 292}{523 + 292 + 96 + 135} = 0.779 = 78\%$$.  

Maatriksist saame välja lugeda ka prognoosi tundlikkuse (*sensitivity*) ja spetsiifilisuse (*specificity*).

**Tundlikkus** väljendab õigesti prognoositud positiivsete väärtuste osakaalu kõikidest positiivsetest väärtustest
$$\text{sensitivity} = \frac{\text{õige positiivne}}{\text{õige positiivne} + \text{vale negatiivne}} = \frac{292}{(292+135)} = 0.68$$
**Spetsiifilisus** omakorda väljendab õigesti prognoositud negatiivsete väärtuste osakaalu kõikidest negatiivsetest väärtustest

$$\text{specificity} = \frac{\text{õige negatiivne}}{\text{õige negatiivne} + \text{vale positiivne}} = \frac{523}{(523+96)} = 0.84$$

Saame need arvutused teha ka *caret* paketi ja `confusionMatrix()` funktsiooniga.

```{r}
library(caret)
# confusionMatrix vajab sisendiuna faktoreid, 
# positive = TRUE arguimendiga ütleme, et ellujäämine oli positiivne sündmus
confusionMatrix(data = as.factor(prognoos), 
                reference = as.factor(vaadeldud), positive = 'TRUE')
```


Nii mudeli täpsus, tundlikkus, kui ka spetsiifilisus lähtusid eeldusest, et me klassifitseerisime vaatlused positiivseteks või negatiivseteks lähtuvalt sellest kas nende prognoositud tõenäosus oli suurem või väiksem kui $0.5$ (nn *treshold* või *cutoff value*). Mida suurem on see *cutoff*, seda rohkem õigeid positiivseid väärtusi saame prognoosida. Kuid samas, seda vähem saame prognoosida õigeid negatiivseid väärtusi. Ehk siis tundlikkuse ja spetsiifilisuse vahel on pöördvõrdeline seos. Mida suurem on üks, seda väiksem peab teine olema ja vastupidi. Seda seost saame vaadelda ROCi (*receiver operating characteristics*) graafiku abil.

```{r}
library(ROCit)
library(broom)
# Kasutame broomi funktsiooni augment 
mudel_fit <- augment(mudel8, type.predict = "response")
roc_obj <- rocit(score = mudel_fit$.fitted, class=mudel_fit$surv)
plot(roc_obj)

```

Mida suurem on pind graafiku kurvi all, seda parema mudeliga meil tegemist on (seda täpsemini võimaldab mudel prognoosida). Seda kurvi alust pindala suurust kasutataksegi prognoosi täpsuse hindamiseks. Vastavat statistikut kutsustaksegi kurvialuseks pindalaks (AUC ehk *area under the curve*). Mida lähemal AUC $1$-le on, seda parema prognoosivõimega mudeliga meil tegemist on. Kui AUC on 0.5, siis on tegemist puhtalt juhusliku arvamisega.

```{r}
summary(roc_obj)
```

## Mudeli eeldused

Nagu igal mudelil on ka logistilise regressiooni mudelil rida eeldusi, mis peavad olema täidetud, et tulemused oleksid korrektselt tõlgendatavad:  

- Sõltuv tunnus peab olema binaarne  
- Vaatlused on sõltumatud
- Sõltumatute tunnuste vahel ei tohi olla multikollineaarsust (sõltumatud tunnused ei tohi olla liialt korreleeritud)  
- Sõltumatute tunnuse ja sõltuva tunnuse logiti vahelised suhted peavad olema lineaarsed


## Cross-validation

Ristvalideerimine on meetod mudeli hindamiseks, kus andmestik jaotatakse kordamööda treening- ja testosadeks, et saada võimalikult objektiivne hinnang mudeli võimekusele uut teavet üldistada. Treeningandmete peal õpib mudel seda, milline seos on sõltumatute tunnuste ja sõltuva tunnuse vahel. Testandmete peal kontrollime, kas mudel oskab ka uute andmete korral neid seoseid korrektselt prognoosida.

Logistilise regressiooni kontekstis tähendab see, et me jagame oma andmed näiteks viide või kümnesse võrdse suurusega ossa (k-osa ehk *k-fold*), treenime igas voorus mudelit n-1 osal ja testime väljajäänud ühel osal, seejuures roteerides testosa üle kõikide osade. Selline lähenemine aitab vältida ülesobitamist (*overfitting*), sest mudel peab igal iteratsioonil „hakkama saama“ seninägemata andmetega. Igas voorus kogume kokku mingi soorituse mõõdiku, näiteks ROC kõvera aluse pindala (AUC) või klassifitseerimistäpsuse, ja lõpuks keskmistame need väärtused, et saada üldine hinnang mudeli võimekusele.

- Valime k (nt 5 või 10)
- Segame (shuffle) andmed juhuslikult, et voldid oleks võimalikult homogeensed
- Jaotame andmed k ossa
- Iteratsioonid üle osade (1 kuni k):
  + Treeningandmestikuks kõik osad välja arvatud k osa ja testandmestikuks k osa
  + Sobitame logistilise regressiooni treeningandmestikul
  + Prognoosime mudelit testandmestiku peal
  + Salvestame soorituse mõõdiku(d)
- Keskmistame kõik mõõdikud lõplikuks hindeks

R-ikeskkonnas on tüüpiline viis logistilise regressiooni ristvalideerimiseks kasutada "caret" paketti, kus defineeritakse treeningkontrolli seaded ja antakse ette, mitu korda ja millise meetodiga ristvalideerimine läbida.

```{r}
library(caret)

# andmete jagamise raamistik
train_ctrl <- trainControl(method = "cv", # kasutame cross-validation meetodit
                           number = 5, # andmed jaotatakse viieks võrdseks tükiks
                           classProbs = TRUE, # mudel tekitab ka klasside tõenäosused, mis on vajalikud ROC ja AUC arvutamiseks
                           summaryFunction = twoClassSummary # annab meile ROC väärtuse, tundlikkuse ja spetsiifilisuse
                           )

# "train" funktsioon eeldab kahe tasemega faktorit, jätame välja ka kõik puuduolevad väärtused
df <- titanic %>% 
  select(survived, age, sex) %>% 
  mutate(survived = factor(survived), age = scale(age, scale = F)) %>% 
  na.omit()

# mudeli jooksutamine
model_cv <- train(survived~ sex * age, # regressioonivõrrand
                  data = df, # andmestik
                  method = "glm", # meetod on glm
                  family = "binomial", 
                  trControl = train_ctrl,
                  metric = "ROC") # võrdleme mudeli tulemusi AUC alusel


model_cv

```

Saadud väljundis näeme, kuidas mudel igas voorus sooritas ja milline on keskmine ROC koos hinnangulise standardveaga. Mida kõrgem on lõplik keskmine AUC, seda paremini suudab mudel õigeid klassifikatsioone teha. Lisaks AUC-le võib kasutada ka muid mõõdikuid, näiteks täpsust (accuracy), tundlikkust (sensitivity) või F-skoori, olenevalt konkreetsest rakendusjuhtumist.

Ristvalideerimisest tulenev keskmine mõõdik on üldiselt parem kui terve andmestiku korraga saadud hinnang, sest võimaldab tagada, et mudel ei oleks kindlat tüüpi vaatlustele liigselt sobitunud ning suudab seetõttu paremini üldistada. Igas voorus kasutab mudel varem treenimiseks mittevalitud osa testimiseks, mis imiteerib reaalse elu  pidevalt uuenevaid andmeid. Nii näeme, kui suured on tegelikud vead ja kui palju võib mudeli sooritus kõikuda sõltuvalt sellest, milline andmestiku alamhulk parasjagu treeninguks või testimiseks valitakse.
